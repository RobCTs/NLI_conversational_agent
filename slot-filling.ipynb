{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T18:36:52.613509Z","iopub.status.busy":"2023-11-21T18:36:52.613108Z","iopub.status.idle":"2023-11-21T18:36:53.426461Z","shell.execute_reply":"2023-11-21T18:36:53.425591Z","shell.execute_reply.started":"2023-11-21T18:36:52.613476Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\camil\\anaconda3\\envs\\my-torch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["from datasets import load_dataset\n","import torch\n","import torch.nn as nn\n","from transformers import BertTokenizerFast\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","# Check if GPU is available and set the device accordingly\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Load the Train, Validation and Test data\n","\n","dataset = load_dataset(\"multi_woz_v22\")\n","train_data = dataset['train']\n","val_data = dataset['validation']\n","test_data = dataset['test']"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T18:36:53.428336Z","iopub.status.busy":"2023-11-21T18:36:53.428058Z","iopub.status.idle":"2023-11-21T18:37:15.734609Z","shell.execute_reply":"2023-11-21T18:37:15.733602Z","shell.execute_reply.started":"2023-11-21T18:36:53.428311Z"},"trusted":true},"outputs":[],"source":["def filterDomains(data):\n","    \"\"\"\n","    Filters a list of dictionaries by only including entries with services\n","    either \"restaurant\" or \"hotel\" and having only one service.\n","\n","    Parameters:\n","    - data: list of dictionaries containing a \"services\" key, which is a list of services.\n","\n","    Returns:\n","    - List of filtered dictionaries.\n","    \"\"\"\n","    return [entry for entry in data if set(entry[\"services\"]).issubset({\"restaurant\", \"hotel\"})]\n","\n","# Only keep dialogues related to Restaurants or Hotels.\n","\n","train_data_filtered = filterDomains(train_data)\n","val_data_filtered = filterDomains(val_data)\n","test_data_filtered = filterDomains(test_data)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T18:37:15.736088Z","iopub.status.busy":"2023-11-21T18:37:15.735820Z","iopub.status.idle":"2023-11-21T18:37:15.741404Z","shell.execute_reply":"2023-11-21T18:37:15.740523Z","shell.execute_reply.started":"2023-11-21T18:37:15.736065Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'dialogue_id': 'PMUL4398.json', 'services': ['restaurant', 'hotel'], 'turns': {'turn_id': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11'], 'speaker': [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1], 'utterance': ['i need a place to dine in the center thats expensive', 'I have several options for you; do you prefer African, Asian, or British food?', 'Any sort of food would be fine, as long as it is a bit expensive. Could I get the phone number for your recommendation?', 'There is an Afrian place named Bedouin in the centre. How does that sound?', 'Sounds good, could I get that phone number? Also, could you recommend me an expensive hotel?', \"Bedouin's phone is 01223367660. As far as hotels go, I recommend the University Arms Hotel in the center of town.\", 'Yes. Can you book it for me?', 'Sure, when would you like that reservation?', 'i want to book it for 2 people and 2 nights starting from saturday.', 'Your booking was successful. Your reference number is FRGZWQL2 . May I help you further?', 'That is all I need to know. Thanks, good bye.', 'Thank you so much for Cambridge TownInfo centre. Have a great day!'], 'frames': [{'service': ['restaurant', 'hotel'], 'state': [{'active_intent': 'find_restaurant', 'requested_slots': [], 'slots_values': {'slots_values_name': ['restaurant-area', 'restaurant-pricerange'], 'slots_values_list': [['centre'], ['expensive']]}}, {'active_intent': 'find_hotel', 'requested_slots': [], 'slots_values': {'slots_values_name': [], 'slots_values_list': []}}], 'slots': [{'slot': [], 'value': [], 'start': [], 'exclusive_end': [], 'copy_from': [], 'copy_from_value': []}, {'slot': [], 'value': [], 'start': [], 'exclusive_end': [], 'copy_from': [], 'copy_from_value': []}]}, {'service': [], 'state': [], 'slots': []}, {'service': ['restaurant', 'hotel'], 'state': [{'active_intent': 'find_restaurant', 'requested_slots': ['restaurant-food'], 'slots_values': {'slots_values_name': ['restaurant-area', 'restaurant-pricerange'], 'slots_values_list': [['centre'], ['expensive']]}}, {'active_intent': 'find_hotel', 'requested_slots': [], 'slots_values': {'slots_values_name': [], 'slots_values_list': []}}], 'slots': [{'slot': [], 'value': [], 'start': [], 'exclusive_end': [], 'copy_from': [], 'copy_from_value': []}, {'slot': [], 'value': [], 'start': [], 'exclusive_end': [], 'copy_from': [], 'copy_from_value': []}]}, {'service': [], 'state': [], 'slots': []}, {'service': ['restaurant', 'hotel'], 'state': [{'active_intent': 'find_restaurant', 'requested_slots': ['restaurant-phone'], 'slots_values': {'slots_values_name': ['restaurant-area', 'restaurant-name', 'restaurant-pricerange'], 'slots_values_list': [['centre'], ['bedouin'], ['expensive']]}}, {'active_intent': 'find_hotel', 'requested_slots': [], 'slots_values': {'slots_values_name': ['hotel-pricerange', 'hotel-type'], 'slots_values_list': [['expensive'], ['hotel']]}}], 'slots': [{'slot': [], 'value': [], 'start': [], 'exclusive_end': [], 'copy_from': [], 'copy_from_value': []}, {'slot': [], 'value': [], 'start': [], 'exclusive_end': [], 'copy_from': [], 'copy_from_value': []}]}, {'service': [], 'state': [], 'slots': []}, {'service': ['hotel'], 'state': [{'active_intent': 'find_hotel', 'requested_slots': [], 'slots_values': {'slots_values_name': ['hotel-name', 'hotel-pricerange', 'hotel-type'], 'slots_values_list': [['university arms hotel'], ['expensive'], ['hotel']]}}], 'slots': [{'slot': [], 'value': [], 'start': [], 'exclusive_end': [], 'copy_from': [], 'copy_from_value': []}]}, {'service': [], 'state': [], 'slots': []}, {'service': ['hotel'], 'state': [{'active_intent': 'book_hotel', 'requested_slots': [], 'slots_values': {'slots_values_name': ['hotel-bookday', 'hotel-bookpeople', 'hotel-bookstay', 'hotel-name', 'hotel-pricerange', 'hotel-type'], 'slots_values_list': [['saturday'], ['2'], ['2'], ['university arms hotel'], ['expensive'], ['hotel']]}}], 'slots': [{'slot': [], 'value': [], 'start': [], 'exclusive_end': [], 'copy_from': [], 'copy_from_value': []}]}, {'service': [], 'state': [], 'slots': []}, {'service': [], 'state': [], 'slots': []}, {'service': [], 'state': [], 'slots': []}], 'dialogue_acts': [{'dialog_act': {'act_type': ['Restaurant-Inform'], 'act_slots': [{'slot_name': ['area', 'pricerange'], 'slot_value': ['centre', 'expensive']}]}, 'span_info': {'act_type': ['Restaurant-Inform', 'Restaurant-Inform'], 'act_slot_name': ['area', 'pricerange'], 'act_slot_value': ['centre', 'expensive'], 'span_start': [30, 43], 'span_end': [36, 52]}}, {'dialog_act': {'act_type': ['Restaurant-Inform', 'Restaurant-Select'], 'act_slots': [{'slot_name': ['choice'], 'slot_value': ['several']}, {'slot_name': ['food', 'food', 'food'], 'slot_value': ['African', 'Asian', 'British']}]}, 'span_info': {'act_type': ['Restaurant-Inform', 'Restaurant-Select', 'Restaurant-Select', 'Restaurant-Select'], 'act_slot_name': ['choice', 'food', 'food', 'food'], 'act_slot_value': ['several', 'African', 'Asian', 'British'], 'span_start': [7, 46, 55, 65], 'span_end': [14, 53, 60, 72]}}, {'dialog_act': {'act_type': ['Restaurant-Request'], 'act_slots': [{'slot_name': ['food'], 'slot_value': ['?']}]}, 'span_info': {'act_type': [], 'act_slot_name': [], 'act_slot_value': [], 'span_start': [], 'span_end': []}}, {'dialog_act': {'act_type': ['Restaurant-Inform'], 'act_slots': [{'slot_name': ['area', 'food', 'name'], 'slot_value': ['centre', 'Afrian', 'Bedouin']}]}, 'span_info': {'act_type': ['Restaurant-Inform', 'Restaurant-Inform', 'Restaurant-Inform'], 'act_slot_name': ['food', 'name', 'area'], 'act_slot_value': ['Afrian', 'Bedouin', 'centre'], 'span_start': [12, 31, 46], 'span_end': [18, 38, 52]}}, {'dialog_act': {'act_type': ['Hotel-Inform', 'Restaurant-Request'], 'act_slots': [{'slot_name': ['pricerange', 'type'], 'slot_value': ['expensive', 'hotel']}, {'slot_name': ['phone'], 'slot_value': ['?']}]}, 'span_info': {'act_type': ['Hotel-Inform', 'Hotel-Inform'], 'act_slot_name': ['pricerange', 'type'], 'act_slot_value': ['expensive', 'hotel'], 'span_start': [76, 86], 'span_end': [85, 91]}}, {'dialog_act': {'act_type': ['Hotel-Recommend', 'Restaurant-Inform'], 'act_slots': [{'slot_name': ['area', 'name'], 'slot_value': ['center of town', 'the University Arms Hotel']}, {'slot_name': ['name', 'phone'], 'slot_value': ['Bedouin', '01223367660']}]}, 'span_info': {'act_type': ['Restaurant-Inform', 'Restaurant-Inform', 'Hotel-Recommend', 'Hotel-Recommend'], 'act_slot_name': ['name', 'phone', 'name', 'area'], 'act_slot_value': ['Bedouin', '01223367660', 'the University Arms Hotel', 'center of town'], 'span_start': [0, 19, 65, 98], 'span_end': [7, 30, 90, 112]}}, {'dialog_act': {'act_type': ['Hotel-Inform'], 'act_slots': [{'slot_name': ['none'], 'slot_value': ['none']}]}, 'span_info': {'act_type': [], 'act_slot_name': [], 'act_slot_value': [], 'span_start': [], 'span_end': []}}, {'dialog_act': {'act_type': ['Booking-Request'], 'act_slots': [{'slot_name': ['bookday'], 'slot_value': ['?']}]}, 'span_info': {'act_type': [], 'act_slot_name': [], 'act_slot_value': [], 'span_start': [], 'span_end': []}}, {'dialog_act': {'act_type': ['Hotel-Inform'], 'act_slots': [{'slot_name': ['bookday', 'bookpeople', 'bookstay'], 'slot_value': ['saturday', '2', '2']}]}, 'span_info': {'act_type': ['Hotel-Inform', 'Hotel-Inform', 'Hotel-Inform'], 'act_slot_name': ['bookstay', 'bookpeople', 'bookday'], 'act_slot_value': ['2', '2', 'saturday'], 'span_start': [22, 35, 58], 'span_end': [23, 36, 66]}}, {'dialog_act': {'act_type': ['Booking-Book', 'general-reqmore'], 'act_slots': [{'slot_name': ['ref'], 'slot_value': ['FRGZWQL2']}, {'slot_name': ['none'], 'slot_value': ['none']}]}, 'span_info': {'act_type': ['Booking-Book'], 'act_slot_name': ['ref'], 'act_slot_value': ['FRGZWQL2'], 'span_start': [54], 'span_end': [62]}}, {'dialog_act': {'act_type': ['general-bye'], 'act_slots': [{'slot_name': ['none'], 'slot_value': ['none']}]}, 'span_info': {'act_type': [], 'act_slot_name': [], 'act_slot_value': [], 'span_start': [], 'span_end': []}}, {'dialog_act': {'act_type': ['general-bye', 'general-welcome'], 'act_slots': [{'slot_name': ['none'], 'slot_value': ['none']}, {'slot_name': ['none'], 'slot_value': ['none']}]}, 'span_info': {'act_type': [], 'act_slot_name': [], 'act_slot_value': [], 'span_start': [], 'span_end': []}}]}}\n"]}],"source":["print(train_data_filtered[0])"]},{"cell_type":"markdown","metadata":{},"source":["# Identifying the slots"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T18:37:15.743945Z","iopub.status.busy":"2023-11-21T18:37:15.743528Z","iopub.status.idle":"2023-11-21T18:37:15.984342Z","shell.execute_reply":"2023-11-21T18:37:15.983566Z","shell.execute_reply.started":"2023-11-21T18:37:15.743919Z"},"trusted":true},"outputs":[],"source":["tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n","\n","\n","def label_utterances(dialogue):\n","    labeled_data = []\n","    data = dialogue['turns']\n","    \n","    # Loop through each turn in the dialogue\n","    for i, turn_id in enumerate(data['turn_id']):\n","        utterance = data['utterance'][i]\n","        # Tokenize the utterance and get the offset mappings\n","        encoded_input = tokenizer(utterance, add_special_tokens=False, return_offsets_mapping=True)\n","        tokens = tokenizer.convert_ids_to_tokens(encoded_input['input_ids'])\n","        labels = ['O'] * len(tokens)  # Initialize labels as 'O' (Outside)\n","        offset_mapping = encoded_input['offset_mapping']\n","        # Check if there are slot values for this turn\n","        if 'dialogue_acts' in data and i < len(data['dialogue_acts']):\n","            dialogue_act = data['dialogue_acts'][i]\n","            span_info = dialogue_act.get('span_info', {})\n","            for act_slot_name, act_slot_value, span_start, span_end in zip(\n","                    span_info.get('act_slot_name', []),\n","                    span_info.get('act_slot_value', []),\n","                    span_info.get('span_start', []),\n","                    span_info.get('span_end', [])):\n","                \n","                # Find the tokens that correspond to the start and end indices\n","                # start_token_idx = next((idx for idx, offset in enumerate(offset_mapping) if offset[0] == span_start), None)\n","                # end_token_idx = next((idx for idx, offset in enumerate(offset_mapping) if offset[1] == span_end), None)\n","                \n","                # Utilize the offset_mapping to find the token index for the start and end of the span\n","                start_token_idx = None\n","                end_token_idx = None\n","                \n","                for idx, offset in enumerate(offset_mapping):\n","                    if start_token_idx is None and offset[0] == span_start:\n","                        start_token_idx = idx\n","                    if offset[1] == span_end:\n","                        end_token_idx = idx\n","                        break\n","                \n","                if start_token_idx is not None and end_token_idx is not None:\n","                    if start_token_idx < len(tokens) and end_token_idx < len(tokens):\n","                        # Label tokens using IOB format with the actual ground truth slot value\n","                        labels[start_token_idx] = f\"B-{act_slot_name}\"\n","                        for j in range(start_token_idx + 1, end_token_idx + 1):\n","                            labels[j] = f\"I-{act_slot_name}\"\n","                    else:\n","                        print(f\"Warning: Index out of range for utterance '{utterance}' with span {span_start}-{span_end}\")\n","            \n","            try:\n","                # if the prev_dialogue_act is not None, then we need to label the tokens that are part of the previous dialogue act\n","                prev_dialogue_act = data['dialogue_acts'][i-1]['dialog_act']['act_type'][0] if i > 0 and data['dialogue_acts'][i]['dialog_act']['act_type'][0] else \"\"\n","                current_dialogue_act = data['dialogue_acts'][i]['dialog_act']['act_type'][0] if data['dialogue_acts'][i]['dialog_act']['act_type'][0] else \"\"\n","            except IndexError as e:\n","                prev_dialogue_act = \"\"\n","                current_dialogue_act = \"\"\n","            \n","            dialogue_act_str = f\"{prev_dialogue_act}|{current_dialogue_act}\"\n","            \n","            act_tokens = tokenizer.tokenize(dialogue_act_str)\n","            act_labels = ['X'] * len(act_tokens)\n","            tokens = act_tokens + ['[SEP]'] + tokens  # Add a separator token between acts and the utterance\n","            labels = act_labels + ['X'] + labels  # Add an 'X' label for the separator token\n","\n","        # Store the tokenized utterance along with its labels\n","        labeled_data.append((tokens, labels))\n","        \n","    return labeled_data\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T18:37:15.985464Z","iopub.status.busy":"2023-11-21T18:37:15.985196Z","iopub.status.idle":"2023-11-21T18:37:23.130618Z","shell.execute_reply":"2023-11-21T18:37:23.129854Z","shell.execute_reply.started":"2023-11-21T18:37:15.985438Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","def toDF(data):\n","    all_labeled_data = []\n","    for dialogue in data:\n","        all_labeled_data.extend(label_utterances(dialogue))\n","    return pd.DataFrame(all_labeled_data, columns=['Tokens', 'Labels'])\n","    \n","# Create DataFrames of labeled utterances\n","train_df = toDF(train_data_filtered)\n","test_df = toDF(test_data_filtered)\n","val_df = toDF(val_data_filtered)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T18:37:23.132070Z","iopub.status.busy":"2023-11-21T18:37:23.131779Z","iopub.status.idle":"2023-11-21T18:37:28.759349Z","shell.execute_reply":"2023-11-21T18:37:28.758512Z","shell.execute_reply.started":"2023-11-21T18:37:23.132045Z"},"trusted":true},"outputs":[],"source":["train_df.to_excel(\"output.xlsx\")  "]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T18:37:28.760789Z","iopub.status.busy":"2023-11-21T18:37:28.760488Z","iopub.status.idle":"2023-11-21T18:37:28.767285Z","shell.execute_reply":"2023-11-21T18:37:28.766071Z","shell.execute_reply.started":"2023-11-21T18:37:28.760763Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(28928, 2)\n","['hotel', '-', 'inform', '|', 'booking', '-', 'book', '[SEP]', 'your', 'booking', 'was', 'successful', '.', 'your', 'reference', 'number', 'is', 'fr', '##g', '##z', '##w', '##q', '##l', '##2', '.', 'may', 'i', 'help', 'you', 'further', '?']\n","['X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ref', 'I-ref', 'I-ref', 'I-ref', 'I-ref', 'I-ref', 'I-ref', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"]}],"source":["print(train_df.shape)\n","print(train_df[\"Tokens\"].iloc[9])\n","print(train_df[\"Labels\"].iloc[9])"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T18:37:28.769588Z","iopub.status.busy":"2023-11-21T18:37:28.768707Z","iopub.status.idle":"2023-11-21T18:37:28.847042Z","shell.execute_reply":"2023-11-21T18:37:28.846130Z","shell.execute_reply.started":"2023-11-21T18:37:28.769561Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['B-address', 'B-area', 'B-arriveby', 'B-bookday', 'B-bookpeople', 'B-bookstay', 'B-booktime', 'B-choice', 'B-day', 'B-department', 'B-departure', 'B-destination', 'B-entrancefee', 'B-food', 'B-leaveat', 'B-name', 'B-openhours', 'B-phone', 'B-postcode', 'B-price', 'B-pricerange', 'B-ref', 'B-stars', 'B-type', 'I-address', 'I-area', 'I-arriveby', 'I-bookday', 'I-bookpeople', 'I-bookstay', 'I-booktime', 'I-choice', 'I-department', 'I-departure', 'I-destination', 'I-entrancefee', 'I-food', 'I-leaveat', 'I-name', 'I-openhours', 'I-phone', 'I-postcode', 'I-price', 'I-pricerange', 'I-ref', 'I-stars', 'I-type', 'O']\n"]}],"source":["\n","all_labels = [label for sublist in train_df['Labels'].tolist() for label in sublist]\n","all_labels += [label for sublist in val_df['Labels'].tolist() for label in sublist]\n","all_labels += [label for sublist in test_df['Labels'].tolist() for label in sublist]\n","unique_labels = sorted(set(all_labels))\n","\n","unique_labels.__sizeof__()\n","\n","# We will ignore the 'X' label.\n","unique_labels.remove('X')\n","\n","print(unique_labels)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T18:37:28.848612Z","iopub.status.busy":"2023-11-21T18:37:28.848296Z","iopub.status.idle":"2023-11-21T18:37:28.853027Z","shell.execute_reply":"2023-11-21T18:37:28.852025Z","shell.execute_reply.started":"2023-11-21T18:37:28.848586Z"},"trusted":true},"outputs":[],"source":["label_map = {label: i for i, label in enumerate(unique_labels)}"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T18:37:28.856196Z","iopub.status.busy":"2023-11-21T18:37:28.855909Z","iopub.status.idle":"2023-11-21T18:37:28.865832Z","shell.execute_reply":"2023-11-21T18:37:28.864989Z","shell.execute_reply.started":"2023-11-21T18:37:28.856166Z"},"trusted":true},"outputs":[],"source":["def create_dataset(df, tokenizer, label_map):\n","    # Lists to store the tokenized inputs and labels\n","    input_ids = []\n","    attention_masks = []\n","    label_ids = []\n","\n","    # Iterate over the DataFrame rows\n","    for _, row in df.iterrows():\n","        tokens = row['Tokens']\n","        labels = row['Labels']\n","        \n","        # Convert the IOB labels to their corresponding IDs\n","        label_ids_for_tokens = [label_map.get(label, -100) for label in labels] # ignore the 'X' label\n","\n","        encoded_input = tokenizer(\n","            tokens,\n","            is_split_into_words=True,\n","            add_special_tokens=True,\n","            return_attention_mask=True,\n","            padding='max_length',\n","            truncation=True,\n","            max_length=256,\n","            return_offsets_mapping=True\n","        )\n","        \n","        # Create an empty array to hold the final label IDs\n","        aligned_labels = np.ones(len(encoded_input['input_ids']), dtype=int) * -100\n","\n","        # Set labels using the word_ids to align them with tokens\n","        for i, word_id in enumerate(encoded_input.word_ids()):\n","            if word_id is not None and tokens[word_id] not in [\"[CLS]\", \"[SEP]\"]:\n","                aligned_labels[i] = label_ids_for_tokens[word_id]\n","\n","        # Append the results to the lists\n","        input_ids.append(encoded_input['input_ids'])\n","        attention_masks.append(encoded_input['attention_mask'])\n","        label_ids.append(aligned_labels.tolist())\n","\n","    # Convert lists to tensors\n","    input_ids = torch.tensor(input_ids, dtype=torch.long)\n","    attention_masks = torch.tensor(attention_masks, dtype=torch.long)\n","    label_ids = torch.tensor(label_ids, dtype=torch.long)\n","\n","    # Create the TensorDataset\n","    dataset = TensorDataset(input_ids, attention_masks, label_ids)\n","\n","    return dataset"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T18:37:28.867065Z","iopub.status.busy":"2023-11-21T18:37:28.866809Z","iopub.status.idle":"2023-11-21T18:37:51.257583Z","shell.execute_reply":"2023-11-21T18:37:51.256553Z","shell.execute_reply.started":"2023-11-21T18:37:28.867042Z"},"trusted":true},"outputs":[],"source":["train_dataloader = DataLoader(create_dataset(train_df,tokenizer,label_map), batch_size=16, shuffle=True)\n","val_dataloader = DataLoader(create_dataset(val_df,tokenizer,label_map), batch_size=16, shuffle=True)\n","test_dataloader = DataLoader(create_dataset(test_df,tokenizer,label_map), batch_size=16, shuffle=True)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T18:37:51.259051Z","iopub.status.busy":"2023-11-21T18:37:51.258758Z","iopub.status.idle":"2023-11-21T18:37:53.119385Z","shell.execute_reply":"2023-11-21T18:37:53.118376Z","shell.execute_reply.started":"2023-11-21T18:37:51.259025Z"},"trusted":true},"outputs":[],"source":["from transformers import BertForTokenClassification, BertConfig\n","\n","# Define the number of labels\n","num_labels = len(label_map)  # Make sure label_map is defined in your environment\n","\n","# Create a configuration object with `num_labels` set\n","config = BertConfig.from_pretrained('bert-base-uncased', num_labels=num_labels)\n","\n","# Create the model with the standard token classification head\n","model = BertForTokenClassification(config).to(device)\n"]},{"cell_type":"code","execution_count":103,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T18:37:53.120935Z","iopub.status.busy":"2023-11-21T18:37:53.120606Z","iopub.status.idle":"2023-11-21T20:00:22.863809Z","shell.execute_reply":"2023-11-21T20:00:22.862823Z","shell.execute_reply.started":"2023-11-21T18:37:53.120908Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Epoch 1/7 Training:   0%|          | 0/1808 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 1/7 | Train Loss: 0.2906221937814695\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Epoch 1/7 Validation:   0%|          | 0/130 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 1/7 | Validation Loss: 0.15909734001526465\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Epoch 2/7 Training:   0%|          | 0/1808 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 2/7 | Train Loss: 0.13695577808822695\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Epoch 2/7 Validation:   0%|          | 0/130 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 2/7 | Validation Loss: 0.12666630146022026\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Epoch 3/7 Training:   0%|          | 0/1808 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 3/7 | Train Loss: 0.10949596863979705\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Epoch 3/7 Validation:   0%|          | 0/130 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 3/7 | Validation Loss: 0.11606963659421755\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Epoch 4/7 Training:   0%|          | 0/1808 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 4/7 | Train Loss: 0.09407084769171671\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Epoch 4/7 Validation:   0%|          | 0/130 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 4/7 | Validation Loss: 0.105181605146768\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Epoch 5/7 Training:   0%|          | 0/1808 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 5/7 | Train Loss: 0.0833234294634199\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Epoch 5/7 Validation:   0%|          | 0/130 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 5/7 | Validation Loss: 0.09879767420486762\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Epoch 6/7 Training:   0%|          | 0/1808 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 6/7 | Train Loss: 0.07670422744758497\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Epoch 6/7 Validation:   0%|          | 0/130 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 6/7 | Validation Loss: 0.11090743742310084\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Epoch 7/7 Training:   0%|          | 0/1808 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 7/7 | Train Loss: 0.06996452265258866\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Epoch 7/7 Validation:   0%|          | 0/130 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 7/7 | Validation Loss: 0.11091081942073427\n","Early stopping!\n","Training complete. Final model saved.\n"]}],"source":["from tqdm.auto import tqdm\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n","criterion = nn.CrossEntropyLoss(ignore_index=-100)\n","epochs = 7\n","patience = 2\n","\n","\n","# Initialize the early stopping counter\n","best_val_loss = float('inf')\n","patience_counter = 0\n","\n","# Training loop\n","for epoch in range(epochs):\n","    model.train()\n","    train_loss = 0\n","    train_progress_bar = tqdm(train_dataloader, desc=f'Epoch {epoch+1}/{epochs} Training', leave=False)\n","    \n","    # Training phase\n","    for batch in train_progress_bar:\n","        batch = tuple(t.to(device) for t in batch)\n","        b_input_ids, b_input_mask, b_labels = batch\n","        optimizer.zero_grad()\n","        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","        train_loss += loss.item()\n","        train_progress_bar.set_postfix(train_loss=loss.item())\n","    \n","    avg_train_loss = train_loss / len(train_dataloader)\n","    print(f'Epoch {epoch + 1}/{epochs} | Train Loss: {avg_train_loss}')\n","\n","    # Validation phase\n","    model.eval()\n","    val_loss = 0\n","    val_progress_bar = tqdm(val_dataloader, desc=f'Epoch {epoch+1}/{epochs} Validation', leave=False)\n","    for batch in val_progress_bar:\n","        batch = tuple(t.to(device) for t in batch)\n","        b_input_ids, b_input_mask, b_labels = batch\n","        with torch.no_grad():\n","            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n","            loss = outputs.loss\n","            val_loss += loss.item()\n","            val_progress_bar.set_postfix(val_loss=loss.item())\n","    \n","    avg_val_loss = val_loss / len(val_dataloader)\n","    print(f'Epoch {epoch + 1}/{epochs} | Validation Loss: {avg_val_loss}')\n","\n","    # Check if the validation loss is lower than the best one seen so far\n","    if avg_val_loss < best_val_loss:\n","        best_val_loss = avg_val_loss\n","        patience_counter = 0\n","        torch.save(model.state_dict(), f'checkpoint_epoch_{epoch+1}.pt')\n","    else:\n","        patience_counter += 1\n","        if patience_counter >= patience:\n","            print('Early stopping!')\n","            break\n","print('Training complete. Final model saved.')"]},{"cell_type":"code","execution_count":104,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T20:00:22.865652Z","iopub.status.busy":"2023-11-21T20:00:22.865393Z","iopub.status.idle":"2023-11-21T20:01:10.493607Z","shell.execute_reply":"2023-11-21T20:01:10.492509Z","shell.execute_reply.started":"2023-11-21T20:00:22.865629Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: seqeval in /opt/conda/lib/python3.10/site-packages (1.2.2)\n","Requirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.24.3)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.2.2)\n","Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.11.3)\n","Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\n","Test loss: 0.10447849362305757\n","              precision    recall  f1-score   support\n","\n","     address       0.59      0.82      0.68        79\n","        area       0.71      0.79      0.75       321\n","     bookday       0.92      0.99      0.95       205\n","  bookpeople       0.88      0.86      0.87       186\n","    bookstay       0.77      0.83      0.80       125\n","    booktime       0.90      0.97      0.93       114\n","      choice       0.83      0.90      0.86       215\n","        food       0.91      0.93      0.92       241\n","        name       0.62      0.77      0.69       427\n","       phone       0.82      0.94      0.88        53\n","    postcode       0.76      0.85      0.80        48\n","  pricerange       0.87      0.93      0.90       306\n","         ref       0.86      0.94      0.90       147\n","       stars       0.94      0.96      0.95       141\n","        type       0.63      0.71      0.66       187\n","\n","   micro avg       0.78      0.87      0.82      2795\n","   macro avg       0.80      0.88      0.84      2795\n","weighted avg       0.79      0.87      0.83      2795\n","\n"]}],"source":["!pip install seqeval\n","from seqeval.metrics import classification_report as seqeval_classification_report\n","import numpy as np\n","import torch\n","\n","# Reverse the label map to translate from numeric to string labels\n","label_map_reverse = {v: k for k, v in label_map.items()}\n","\n","model.eval()\n","total_loss = 0\n","all_predictions = []\n","all_true_labels = []\n","\n","with torch.no_grad():\n","    for batch in test_dataloader:\n","        batch = tuple(t.to(device) for t in batch)\n","        b_input_ids, b_attention_masks, b_labels = batch\n","\n","        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_attention_masks, labels=b_labels)\n","        loss = outputs.loss\n","        total_loss += loss.item()\n","\n","        # Move logits and labels to CPU\n","        logits = outputs.logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","\n","        # Convert logits to token predictions\n","        predictions = np.argmax(logits, axis=-1)\n","\n","        # For each item in the batch...\n","        for i in range(b_input_ids.size(0)):\n","            # Skip predictions for tokens with label_id == -100\n","            pred_label_sequence = []\n","            true_label_sequence = []\n","            for j, (pred_id, label_id) in enumerate(zip(predictions[i], label_ids[i])):\n","                if b_attention_masks[i][j] != 0 and label_id != -100:\n","                    pred_label_sequence.append(label_map_reverse.get(pred_id, 'O'))  # Default to 'O' if key is not found\n","                    true_label_sequence.append(label_map_reverse[label_id])\n","\n","            # Ensure the true and predicted sequences have the same length\n","            if len(true_label_sequence) != len(pred_label_sequence):\n","                print(f\"Length mismatch in sequence {i}: true labels {len(true_label_sequence)} vs. predicted labels {len(pred_label_sequence)}\")\n","                # Output the actual sequences to help diagnose the issue\n","                print(\"True labels:\", true_label_sequence)\n","                print(\"Pred labels:\", pred_label_sequence)\n","                continue\n","\n","            # ...extend the true labels and predicted labels lists\n","            all_true_labels.append(true_label_sequence)\n","            all_predictions.append(pred_label_sequence)\n","\n","# Calculate average loss over all the batches\n","avg_loss = total_loss / len(test_dataloader)\n","print(f\"Test loss: {avg_loss}\")\n","\n","# Use seqeval to compute a classification report\n","seqeval_report = seqeval_classification_report(all_true_labels, all_predictions)\n","print(seqeval_report)\n"]},{"cell_type":"code","execution_count":115,"metadata":{"execution":{"iopub.execute_input":"2023-11-21T20:11:06.494839Z","iopub.status.busy":"2023-11-21T20:11:06.494185Z","iopub.status.idle":"2023-11-21T20:11:06.521102Z","shell.execute_reply":"2023-11-21T20:11:06.520333Z","shell.execute_reply.started":"2023-11-21T20:11:06.494806Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['O', 'O', 'O', 'O', 'O', 'O', 'B-choice', 'O', 'O', 'B-food', 'O', 'O', 'O', 'O', 'O', 'O', 'B-pricerange', 'O']\n"]}],"source":["def query_model(model, tokenizer, label_map, utterance, device):\n","    model.eval()  \n","    \n","    \n","    # Reverse the label map to translate from numeric IDs to string labels\n","    label_map_reverse = {v: k for k, v in label_map.items()}\n","\n","    \n","    # Tokenize the new utterance directly with the tokenizer\n","    encoded_input = tokenizer(\n","        utterance,\n","        add_special_tokens=True,\n","        return_attention_mask=True,\n","        padding='max_length',\n","        truncation=True,\n","        max_length=256,\n","        return_tensors='pt'  # Return PyTorch tensors directly\n","    )\n","    \n","    # Move tensors to the correct device\n","    input_ids = encoded_input['input_ids'].to(device)\n","    attention_masks = encoded_input['attention_mask'].to(device)\n","    \n","    with torch.no_grad():\n","        outputs = model(input_ids, token_type_ids=None, attention_mask=attention_masks)\n","        logits = outputs.logits\n","\n","    # Convert logits to probabilities and get the most likely label index\n","    predictions = torch.argmax(logits, dim=2).squeeze().tolist()\n","\n","    # Map predictions to label strings, ignoring -100 and padding tokens\n","    predicted_labels = [label_map_reverse.get(label_id) for label_id, mask in zip(predictions, attention_masks.squeeze().tolist()) if mask != 0 and label_id != -100][1:-1]\n","\n","    return predicted_labels\n","\n","# Example usage\n","new_utterance = \"Can I book a table for five at a Spanish restaurant, the restaurant must be cheap?\"\n","predicted_labels = query_model(model, tokenizer, label_map, new_utterance, device)\n","print(predicted_labels)\n"]},{"cell_type":"markdown","metadata":{},"source":["# Mapping slots to values"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["def label_slots(dialogue):\n","    labeled_data = []\n","    data = dialogue['turns']\n","    \n","    # Loop through each turn in the dialogue\n","    for i, turn_id in enumerate(data['turn_id']):\n","        utterance = data['utterance'][i]\n","        tokens = utterance.split()  \n","\n","        # Check if there are slot values for this turn\n","        if 'dialogue_acts' in data and i < len(data['dialogue_acts']):\n","            dialogue_act = data['dialogue_acts'][i]\n","            span_info = dialogue_act.get('span_info', {})\n","            for act_slot_name, act_slot_value, span_start, span_end in zip(\n","                    span_info.get('act_slot_name', []),\n","                    span_info.get('act_slot_value', []),\n","                    span_info.get('span_start', []),\n","                    span_info.get('span_end', [])):\n","                \n","                # Find the tokens that correspond to the start and end indices\n","                start_token_idx = len(utterance[:span_start].split())\n","                end_token_idx = len(utterance[:span_end].split()) - 1\n","\n","                if start_token_idx < len(tokens) and end_token_idx < len(tokens):\n","                    # Label tokens using IOB format with the actual ground truth slot value\n","                    slot = f\"{utterance[span_start:span_end]}\"\n","                    value = act_slot_value\n","                    \n","                    labeled_data.append((slot, value))\n","                else:\n","                    print(f\"Warning: Index out of range for utterance '{utterance}' with span {span_start}-{span_end}\")\n","                \n","\n","        # Store the tokenized utterance along with its labels\n","        \n","        \n","    return labeled_data"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Warning: Index out of range for utterance 'Not sure of that, I am looking for a restaurant named \"ask.\"' with span 55-58\n","Warning: Index out of range for utterance 'Here is the information for Parkside Police Station Telephone:01223358966 Address :Parkside, Cambridge Postcode :CB11JG' with span 113-119\n","Warning: Index out of range for utterance 'Sure. The address and phone number for La Mimosa is Thompsons Lane Fen Ditton/01223362525 and for Shiraz restaurant it is 84 Regent Street City Centre/01223307581.' with span 151-162\n","Warning: Index out of range for utterance 'Addenbrookes Hospital is nearby. The telephone is 01223245151 and the address is Hills Rd, Cambridge,CB20QQ' with span 101-107\n","Warning: Index out of range for utterance 'Here is the contact info for your police station Telephone:01223358966 Address :Parkside, Cambridge Postcode :CB11JG' with span 110-116\n","Warning: Index out of range for utterance 'Here is the information for Parkside Police Station Telephone:01223358966 Address :Parkside, Cambridge Postcode :CB11JG' with span 113-119\n"]}],"source":["import pandas as pd\n","def slotsToDF(data):\n","    all_labeled_data = []\n","    for dialogue in data:\n","        all_labeled_data.extend(label_slots(dialogue))\n","    return pd.DataFrame(all_labeled_data, columns=['Slots', 'Values'])\n","    \n","# Create DataFrames of labeled utterances\n","train_df = slotsToDF(train_data_filtered)\n","test_df = slotsToDF(test_data_filtered)\n","val_df = slotsToDF(val_data_filtered)"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Slots</th>\n","      <th>Values</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>center</td>\n","      <td>centre</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>don't care</td>\n","      <td>dontcare</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>don't care</td>\n","      <td>dontcare</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>Cityroomz</td>\n","      <td>cityroomz</td>\n","    </tr>\n","    <tr>\n","      <th>53</th>\n","      <td>Chinese</td>\n","      <td>chinese</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>35526</th>\n","      <td>Saturday</td>\n","      <td>saturday</td>\n","    </tr>\n","    <tr>\n","      <th>35545</th>\n","      <td>Wednesday</td>\n","      <td>wednesday</td>\n","    </tr>\n","    <tr>\n","      <th>35552</th>\n","      <td>Indian</td>\n","      <td>indian</td>\n","    </tr>\n","    <tr>\n","      <th>35568</th>\n","      <td>doesn't matter</td>\n","      <td>dontcare</td>\n","    </tr>\n","    <tr>\n","      <th>35572</th>\n","      <td>Nandos</td>\n","      <td>nandos</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3869 rows × 2 columns</p>\n","</div>"],"text/plain":["                Slots     Values\n","0              center     centre\n","25         don't care   dontcare\n","26         don't care   dontcare\n","33          Cityroomz  cityroomz\n","53            Chinese    chinese\n","...               ...        ...\n","35526        Saturday   saturday\n","35545       Wednesday  wednesday\n","35552          Indian     indian\n","35568  doesn't matter   dontcare\n","35572          Nandos     nandos\n","\n","[3869 rows x 2 columns]"]},"metadata":{},"output_type":"display_data"}],"source":["mismatched_rows = train_df[train_df['Slots'] != train_df['Values']]\n","\n","# Display the filtered rows\n","display(mismatched_rows)"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Slots</th>\n","      <th>Values</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>center</td>\n","      <td>centre</td>\n","    </tr>\n","    <tr>\n","      <th>118</th>\n","      <td>african food.</td>\n","      <td>north african</td>\n","    </tr>\n","    <tr>\n","      <th>137</th>\n","      <td>same area</td>\n","      <td>centre</td>\n","    </tr>\n","    <tr>\n","      <th>141</th>\n","      <td>doesn't matter</td>\n","      <td>dontcare</td>\n","    </tr>\n","    <tr>\n","      <th>198</th>\n","      <td>any</td>\n","      <td>dontcare</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>35065</th>\n","      <td>same area</td>\n","      <td>west</td>\n","    </tr>\n","    <tr>\n","      <th>35160</th>\n","      <td>any</td>\n","      <td>dontcare</td>\n","    </tr>\n","    <tr>\n","      <th>35204</th>\n","      <td>any</td>\n","      <td>dontcare</td>\n","    </tr>\n","    <tr>\n","      <th>35243</th>\n","      <td>same day</td>\n","      <td>tuesday</td>\n","    </tr>\n","    <tr>\n","      <th>35568</th>\n","      <td>doesn't matter</td>\n","      <td>dontcare</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>584 rows × 2 columns</p>\n","</div>"],"text/plain":["                Slots         Values\n","0              center         centre\n","118     african food.  north african\n","137         same area         centre\n","141    doesn't matter       dontcare\n","198               any       dontcare\n","...               ...            ...\n","35065       same area           west\n","35160             any       dontcare\n","35204             any       dontcare\n","35243        same day        tuesday\n","35568  doesn't matter       dontcare\n","\n","[584 rows x 2 columns]"]},"metadata":{},"output_type":"display_data"}],"source":["def map_indiferece(text):\n","    \n","    indiferences = [\"don't care\", \"any\", \"don't mind\", \"no preference\", \"whatever\", \"doesn't matter\"]\n","    \n","    for indiference in indiferences:\n","        if indiference == text:\n","            return \"dontcare\"\n","        else:\n","            return text\n","    \n","    \n","\n","def text_to_num(text):\n","    \"\"\"\n","    Converts text numbers up to 20 into their integer representations as strings.\n","    If the provided text is not a number or out of range, it returns None.\n","    \"\"\"\n","    text_to_num_dict = {\n","        'zero': '0', 'one': '1', 'two': '2', 'three': '3', 'four': '4', \n","        'five': '5', 'six': '6', 'seven': '7', 'eight': '8', 'nine': '9', \n","        'ten': '10', 'eleven': '11', 'twelve': '12', 'thirteen': '13', \n","        'fourteen': '14', 'fifteen': '15', 'sixteen': '16', 'seventeen': '17', \n","        'eighteen': '18', 'nineteen': '19', 'twenty': '20'\n","    }\n","    # convert to lower case to make the function case-insensitive\n","    text = text.lower()\n","    return text_to_num_dict.get(text, False)\n","\n","\n","# Define the post process function\n","def post_process_slot_value(slot_value):\n","    slot_value = slot_value.lower().strip()\n","    # Check if the slot value is a number\n","    number = text_to_num(slot_value)\n","    if number:\n","        return number\n","    \n","    slot_value = map_indiferece(slot_value)\n","    \n","    return slot_value\n","\n","# Assuming train_df is your DataFrame\n","# Apply the post_process_slot_value function to each value in the 'Slots' column\n","train_df['Slots'] = train_df['Slots'].apply(post_process_slot_value)\n","\n","# Now you can filter out the mismatched rows\n","mismatched_rows = train_df[train_df['Slots'] != train_df['Values'].apply(post_process_slot_value)]\n","\n","# Assuming you want to display these mismatched rows\n","display(mismatched_rows)"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.5868494414004028\n"]}],"source":["map_indiferece(\"any\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from typing import Dict, List\n","\n","class DialogSlotMemory():\n","    slot_list_dict: Dict[str, List[str]] = {}\n","\n","    def __init__(self):\n","        self.slot_list_dict = {}\n","    \n","    def add_slot(self, slot_name: str, slot_value: str):\n","        if slot_name not in self.slot_list_dict:\n","            self.slot_list_dict[slot_name] = []\n","        self.slot_list_dict[slot_name].append(slot_value)\n","    \n","    def get_slot_values(self, slot_name: str):\n","        return self.slot_list_dict[slot_name]\n","\n","    def get_most_recent_slot_value(self, slot_name: str):\n","        return self.slot_list_dict[slot_name][-1] if slot_name in self.slot_list_dict else None\n","\n","    def get_all_slot_values(self):\n","        return self.slot_list_dict\n","\n","    def get_all_slot_names(self):\n","        return self.slot_list_dict.keys()\n","        \n","class ConversationDataset:\n","    id_dialog: str\n","    memory: DialogSlotMemory\n","    dataset: TensorDataset\n","\n","def generate_separate_dialogue_datasets(data) -> List[ConversationDataset]:\n","    \"\"\"\n","    Generates separate datasets for each dialogue in the provided data.\n","    \n","    Parameters:\n","    - data: list of dictionaries containing a \"services\" key, which is a list of services.\n","    \n","    Returns:\n","    - List of datasets, one for each dialogue.\n","    \"\"\"\n","    datasets = []\n","    for dialogue in data:\n","        # Create a dataset for the current dialogue\n","        dataset = ConversationDataset()\n","        dataset.memory = DialogSlotMemory()\n","        dataset.id_dialog = dialogue['dialogue_id']\n","        dataset.dataset = create_dataset(toDF([dialogue]), tokenizer, label_map)\n","        datasets.append(dataset)\n","    return datasets\n","\n","\n","def remove_tokens_before_sep(ids: torch.Tensor, tokenizer: BertTokenizerFast):\n","    \"\"\"\n","    Removes all the tokens before the SEP token, including the SEP token itself.\n","\n","    Parameters:\n","    - ids: list of token IDs.\n","    \n","    Returns:\n","    - List of token IDs with sep and all tokens before it removed.\n","    \"\"\"\n","    sep_token_id = tokenizer.sep_token_id\n","    sep_token_index = (ids == sep_token_id).nonzero(as_tuple=True)[0][0]\n","    return ids[sep_token_index+1:]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install seqeval\n","from seqeval.metrics import classification_report as seqeval_classification_report\n","import numpy as np\n","import torch\n","\n","# Load model from checkpoint\n","model.load_state_dict(torch.load('checkpoint_epoch_1.pt'))\n","model.eval()\n","\n","\n","# Separate the test data into separate datasets for each dialogue\n","test_datasets = generate_separate_dialogue_datasets(test_data_filtered)\n","\n","# Reverse the label map to translate from numeric to string labels\n","label_map_reverse = {v: k for k, v in label_map.items()}\n","\n","model.eval()\n","total_loss = 0\n","all_predictions = []\n","all_true_labels = []\n","\n","with torch.no_grad():\n","    for dataset in test_datasets:\n","        # Get the input_ids, attention_masks and labels from the dataset\n","        input_ids = dataset.dataset.tensors[0]\n","        attention_masks = dataset.dataset.tensors[1]\n","        labels = dataset.dataset.tensors[2]\n","\n","        # Move tensors to the correct device\n","        input_ids = input_ids.to(device)\n","        attention_masks = attention_masks.to(device)\n","        labels = labels.to(device)\n","        \n","\n","        outputs = model(input_ids, token_type_ids=None, attention_mask=attention_masks, labels=labels)\n","        loss = outputs.loss\n","        total_loss += loss.item()\n","\n","        # Move logits and labels to CPU\n","        logits = outputs.logits.detach().cpu().numpy()\n","        label_ids = labels.to('cpu').numpy()\n","\n","        # Convert logits to token predictions\n","        predictions = np.argmax(logits, axis=-1)\n","\n","        # For each item in the batch...\n","        for i in range(input_ids.size(0)):\n","            # Skip predictions for tokens with label_id == -100\n","            pred_label_sequence = []\n","            true_label_sequence = []\n","            for j, (pred_id, label_id) in enumerate(zip(predictions[i], label_ids[i])):\n","                if attention_masks[i][j] != 0 and label_id != -100:\n","                    pred_label_sequence.append(label_map_reverse.get(pred_id, 'O'))\n","\n","                    # Get the true label from the dataset\n","                    true_label_id = label_ids[i][j]\n","                    true_label_sequence.append(label_map_reverse[true_label_id])\n","\n","            # Ensure the true and predicted sequences have the same length\n","            if len(true_label_sequence) != len(pred_label_sequence):\n","                print(f\"Length mismatch in sequence {i}: true labels {len(true_label_sequence)} vs. predicted labels {len(pred_label_sequence)}\")\n","                # Output the actual sequences to help diagnose the issue\n","                print(\"True labels:\", true_label_sequence)\n","                print(\"Pred labels:\", pred_label_sequence)\n","                continue\n","                \n","            # ...extend the true labels and predicted labels lists\n","            all_true_labels.append(true_label_sequence)\n","            all_predictions.append(pred_label_sequence)\n","            \n","            # Map slot values to slot names based on the predicted labels and add them to the memory\n","            # Skip all the tokens before (and including) the [SEP] token           \n","            ids = dataset.dataset.tensors[0][i][1:]\n","            ids = remove_tokens_before_sep(ids, tokenizer)\n","            for token, pred_label in zip(ids, pred_label_sequence):\n","                if pred_label != 'O':\n","                    slot_name = pred_label[2:]\n","                    \n","                    # Get the slot value\n","                    slot_value = tokenizer.convert_tokens_to_string([tokenizer.convert_ids_to_tokens(token.item())])                    \n","                    dataset.memory.add_slot(slot_name, slot_value)\n","            \n","            # Print the memory for the current dialogue\n","            print(f\"Memory for dialogue {dataset.id_dialog}: {dataset.memory.get_all_slot_values()}\")\n","\n","# Calculate average loss over all the batches\n","avg_loss = total_loss / len(test_datasets)\n","print(f\"Test loss: {avg_loss}\")\n","\n","# Use seqeval to compute a classification report\n","seqeval_report = seqeval_classification_report(all_true_labels, all_predictions)\n","print(seqeval_report)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30588,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":4}
