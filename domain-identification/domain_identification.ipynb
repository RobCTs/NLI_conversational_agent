{"cells":[{"cell_type":"markdown","metadata":{},"source":["### -1. To use with Google Colab or Kaggle Notebooks"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-07T20:08:27.978851Z","iopub.status.busy":"2023-10-07T20:08:27.978511Z","iopub.status.idle":"2023-10-07T20:10:23.429771Z","shell.execute_reply":"2023-10-07T20:10:23.428550Z","shell.execute_reply.started":"2023-10-07T20:08:27.978821Z"},"trusted":true},"outputs":[],"source":["# To use in Google Colab or Kaggle Notebooks\n","!pip install datasets==2.14.5\n","!pip install torch==2.0.0\n","!pip install transformers==4.34.0\n","!pip install tqdm==4.66.1\n","!pip install tokenizers==0.14.1\n","!pip install pandas==2.0.3\n","!pip install scikit-learn==1.3.1\n","!pip install matplotlib==3.8.0\n","!pip install --upgrade numpy==1.22.4"]},{"cell_type":"markdown","metadata":{},"source":["### 0. Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-07T20:10:23.432236Z","iopub.status.busy":"2023-10-07T20:10:23.431859Z","iopub.status.idle":"2023-10-07T20:10:34.991446Z","shell.execute_reply":"2023-10-07T20:10:34.990524Z","shell.execute_reply.started":"2023-10-07T20:10:23.432200Z"},"trusted":true},"outputs":[],"source":["# Importing Libraries\n","from datasets import load_dataset\n","import pandas as pd\n","from tqdm import tqdm\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","from transformers import DistilBertTokenizer, DistilBertModel\n","from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n","from enum import Enum\n","import matplotlib.pyplot as plt\n","\n","\n","# Use GPU if available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n"]},{"cell_type":"markdown","metadata":{},"source":["### 1. Dataset Loading and Train_Validation_Test split\n","\n","In this section, we load the dataset and split it into train, validation and test sets."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-07T20:10:34.993851Z","iopub.status.busy":"2023-10-07T20:10:34.992856Z","iopub.status.idle":"2023-10-07T20:11:31.262826Z","shell.execute_reply":"2023-10-07T20:11:31.261920Z","shell.execute_reply.started":"2023-10-07T20:10:34.993818Z"},"trusted":true},"outputs":[],"source":["dataset = load_dataset(\"multi_woz_v22\")\n","train_data = dataset['train']\n","val_data = dataset['validation']\n","test_data = dataset['test']"]},{"cell_type":"markdown","metadata":{},"source":["### 2. Data Preprocessing and Formatting\n","\n","In this section, we filter out the data to get only utterances whose services are labelled as \"Restaurant\" or \"Hotel\" and also convert the filtered data into dataframes for the X (utterances) and y (labels / service) sets for train, validation and test sets."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-07T20:11:31.265745Z","iopub.status.busy":"2023-10-07T20:11:31.265416Z","iopub.status.idle":"2023-10-07T20:11:43.346785Z","shell.execute_reply":"2023-10-07T20:11:43.345902Z","shell.execute_reply.started":"2023-10-07T20:11:31.265709Z"},"trusted":true},"outputs":[],"source":["\n","def filter_and_preprocess(data):\n","    \"\"\"\n","    Filters a list of dictionaries by only including entries with services\n","    either \"restaurant\" or \"hotel\" and having only one service.\n","\n","    Parameters:\n","    - data: list of dictionaries containing a \"services\" key, which is a list of services.\n","\n","    Returns:\n","    - List of filtered dictionaries.\n","    \"\"\"\n","    return [entry for entry in data if set(entry[\"services\"]).issubset({\"restaurant\", \"hotel\"}) and len(entry[\"services\"]) == 1]\n","\n","\n","train_data_filtered = filter_and_preprocess(train_data)\n","val_data_filtered = filter_and_preprocess(val_data)\n","test_data_filtered = filter_and_preprocess(test_data)\n","\n","def toDataFrame(raw_data):\n","    # Initialize an empty list to store rows as dictionaries\n","    data = []\n","\n","    # Loop through each dialogue in the training data\n","    for dialogue in raw_data:\n","\n","        # Get the number of turns in this dialogue\n","        num_turns = len(dialogue['turns']['utterance'])\n","\n","        # Loop through each turn in the dialogue\n","        for i in range(num_turns):\n","\n","            # Extract the utterance and corresponding service for this turn\n","            utterance = dialogue['turns']['utterance'][i]\n","            service = dialogue['services']\n","\n","            # Append as a dictionary to the data list\n","            data.append({'Utterance': utterance, 'Service': service})\n","\n","    # Save data as pandas df\n","    df = pd.DataFrame(data)\n","\n","    # Separate features and labels\n","    X = df['Utterance']\n","    Y = df['Service']\n","    \n","    return X, Y\n","        \n","X_train, Y_train = toDataFrame(train_data_filtered)\n","X_val, Y_val = toDataFrame(val_data_filtered)\n","X_test, Y_test = toDataFrame(test_data_filtered)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-07T20:11:43.348569Z","iopub.status.busy":"2023-10-07T20:11:43.348240Z","iopub.status.idle":"2023-10-07T20:11:52.783221Z","shell.execute_reply":"2023-10-07T20:11:52.782202Z","shell.execute_reply.started":"2023-10-07T20:11:43.348539Z"},"trusted":true},"outputs":[],"source":["# Tokenization using BERT models\n","\n","# Bert Models Enumeration\n","class BertModelsEnum(str, Enum):\n","    bert_base_uncased = 'bert-base-uncased'\n","    bert_base_cased = 'bert-base-cased'\n","    distilbert_base_uncased = 'distilbert-base-uncased'\n","    distilbert_base_cased = 'distilbert-base-cased'\n","\n","bert_model_type = BertModelsEnum.distilbert_base_uncased.value\n","tokenizer = DistilBertTokenizer.from_pretrained(bert_model_type)\n","def tokenize_data(X, Y):\n","    input_encoded = []\n","    attention_masks = []\n","    labels = []\n","    for x,y in zip(X,Y):\n","        encoded = tokenizer.encode_plus(\n","            x, \n","            add_special_tokens=True, \n","            max_length=128, \n","            padding='max_length', \n","            truncation=True, \n","            return_attention_mask=True)\n","        input_encoded.append(encoded['input_ids'])\n","        attention_masks.append(encoded['attention_mask'])\n","        labels.append(0 if y[0] == 'hotel' else 1)\n","    return torch.tensor(input_encoded), torch.tensor(attention_masks), torch.tensor(labels)\n","\n","train_input, train_masks, train_labels = tokenize_data(X_train, Y_train)\n","val_input, val_masks, val_labels = tokenize_data(X_val, Y_val)\n","test_input, test_masks, test_labels = tokenize_data(X_test, Y_test)"]},{"cell_type":"markdown","metadata":{},"source":["### 3. Loading the Data into DataLoaders for Training\n","\n","We finally convert the dataframes into dataloaders for training and validation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-07T20:11:52.785518Z","iopub.status.busy":"2023-10-07T20:11:52.784591Z","iopub.status.idle":"2023-10-07T20:11:52.792247Z","shell.execute_reply":"2023-10-07T20:11:52.791316Z","shell.execute_reply.started":"2023-10-07T20:11:52.785487Z"},"trusted":true},"outputs":[],"source":["batch_size = 64 # Adjust based on GPU memory requirements\n","num_workers = 2 # Adjust based on the number of CPU cores\n","\n","train_data = TensorDataset(train_input, train_masks, train_labels)\n","val_data = TensorDataset(val_input, val_masks, val_labels)\n","test_data = TensorDataset(test_input, test_masks, test_labels)\n","\n","train_loader = DataLoader(train_data, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n","val_loader = DataLoader(val_data, batch_size=batch_size, num_workers=num_workers)\n","test_loader = DataLoader(test_data, batch_size=batch_size, num_workers=num_workers)"]},{"cell_type":"markdown","metadata":{},"source":["### 4. Definition and Architecture of Classifier Model\n","\n","We define the classifier model architecture and the forward function."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-07T20:11:52.794415Z","iopub.status.busy":"2023-10-07T20:11:52.793569Z","iopub.status.idle":"2023-10-07T20:11:52.805637Z","shell.execute_reply":"2023-10-07T20:11:52.804603Z","shell.execute_reply.started":"2023-10-07T20:11:52.794378Z"},"trusted":true},"outputs":[],"source":["class BertClassifier(nn.Module):\n","    def __init__(self):\n","        super(BertClassifier, self).__init__()\n","        self.bert = DistilBertModel.from_pretrained(bert_model_type)\n","        self.fc = nn.Linear(768, 2)\n","\n","    def forward(self, input_ids, attention_mask):\n","        outputs = self.bert(input_ids, attention_mask=attention_mask)\n","        cls_output = outputs.last_hidden_state[:, 0, :] # selects the [CLS] token position.\n","        logits = self.fc(cls_output)\n","        return logits"]},{"cell_type":"markdown","metadata":{},"source":["### 5. Training\n","\n","We train the model on the train set and validate it on the validation set. We first define a class to save the best model during the training process."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-07T20:11:52.807786Z","iopub.status.busy":"2023-10-07T20:11:52.806852Z","iopub.status.idle":"2023-10-07T20:11:52.819914Z","shell.execute_reply":"2023-10-07T20:11:52.819009Z","shell.execute_reply.started":"2023-10-07T20:11:52.807755Z"},"trusted":true},"outputs":[],"source":["class SaveModelWithBestValLoss:\n","    \"\"\"\n","    Class that saves the best model during a training process. If the current epoch's validation loss \n","    is smaller than the previous smallest validation loss saved, it saves the model state.\n","    The best valid loss is initialized to infinity.\n","    \"\"\"\n","    def __init__(\n","        self, smallest_valid_loss=float('inf')\n","    ):\n","        self.smallest_valid_loss = smallest_valid_loss\n","        \n","    def save(\n","        self, current_valid_loss, epoch, model, optimizer, criterion\n","    ):\n","        if current_valid_loss < self.smallest_valid_loss:\n","            self.smallest_valid_loss = current_valid_loss\n","            print(f\"Saving best model for epoch {epoch+1}, with new best validation loss: {self.smallest_valid_loss}\\n\")\n","            torch.save({\n","                'epoch': epoch+1,\n","                'model_state_dict': model.state_dict(),\n","                'optimizer_state_dict': optimizer.state_dict(),\n","                'loss': criterion,\n","                }, 'best_model.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-07T20:11:52.822222Z","iopub.status.busy":"2023-10-07T20:11:52.821257Z","iopub.status.idle":"2023-10-07T20:22:00.961962Z","shell.execute_reply":"2023-10-07T20:22:00.960829Z","shell.execute_reply.started":"2023-10-07T20:11:52.822190Z"},"trusted":true},"outputs":[],"source":["model = BertClassifier()\n","model = model.to(device)\n","optimizer = optim.Adam(model.parameters(), lr=5e-6)\n","criterion = nn.CrossEntropyLoss()\n","\n","save_best_model_instance = SaveModelWithBestValLoss()\n","\n","# For storing training and validation loss\n","train_losses = []\n","val_losses = []\n","\n","for epoch in range(7):\n","    model.train()\n","    \n","    # Initialize tqdm progress bar\n","    train_bar = tqdm(train_loader, desc=f'Training Epoch {epoch}')\n","    \n","    train_loss = 0.0\n","    for batch in train_bar:\n","        input_ids, attention_mask, labels = batch\n","        input_ids = input_ids.to(device)\n","        attention_mask = attention_mask.to(device)\n","        labels = labels.to(device)\n","\n","        \n","        # Zero the parameter gradients\n","        optimizer.zero_grad()\n","        \n","        # Forward pass\n","        logits = model(input_ids, attention_mask)\n","        loss = criterion(logits, labels)\n","        \n","        # Backward pass and optimization\n","        loss.backward()\n","        optimizer.step()\n","        \n","        # Update training loss\n","        train_loss += loss.item()\n","        train_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n","    \n","    # Average training loss\n","    avg_train_loss = train_loss / len(train_loader)\n","    train_losses.append(avg_train_loss)\n","    \n","    # Validation\n","    val_loss = 0.0\n","    model.eval()\n","\n","    # Initialize tqdm progress bar for validation\n","    val_bar = tqdm(val_loader, desc=f'Validation Epoch {epoch}')\n","    \n","    with torch.no_grad():\n","        for batch in val_bar:\n","            input_ids, attention_mask, labels = batch\n","            input_ids = input_ids.to(device)\n","            attention_mask = attention_mask.to(device)\n","            labels = labels.to(device)\n","\n","            \n","            # Forward pass\n","            logits = model(input_ids, attention_mask)\n","            loss = criterion(logits, labels)\n","            \n","            val_loss += loss.item()\n","            val_bar.set_postfix({'validation_loss': '{:.3f}'.format(loss.item()/len(batch))})\n","            \n","    # Average validation loss\n","    avg_val_loss = val_loss / len(val_loader)\n","    val_losses.append(avg_val_loss)\n","    \n","    print(f'Epoch {epoch}, Training Loss: {avg_train_loss}, Validation Loss: {avg_val_loss}')\n","    \n","    # Save model if validation model loss decreases\n","    save_best_model_instance.save(\n","        avg_val_loss, epoch, model, optimizer, criterion\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-07T20:25:40.074848Z","iopub.status.busy":"2023-10-07T20:25:40.074486Z","iopub.status.idle":"2023-10-07T20:25:40.314596Z","shell.execute_reply":"2023-10-07T20:25:40.313752Z","shell.execute_reply.started":"2023-10-07T20:25:40.074819Z"},"trusted":true},"outputs":[],"source":["# Plotting validation and training loss\n","import matplotlib.pyplot as plt\n","plt.plot(train_losses, label='Training loss')\n","plt.plot(val_losses, label='Validation loss')\n","plt.legend()\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### 6. Evaluation\n","\n","We evaluate the best model on the Test Set. We first load the best model saved on the disk."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-07T20:25:46.331298Z","iopub.status.busy":"2023-10-07T20:25:46.325113Z","iopub.status.idle":"2023-10-07T20:25:48.977691Z","shell.execute_reply":"2023-10-07T20:25:48.976620Z","shell.execute_reply.started":"2023-10-07T20:25:46.331258Z"},"trusted":true},"outputs":[],"source":["# build the model, without loading the pre-trained weights or fine-tune layers\n","saved_model = BertClassifier().to(device)\n","best_model = torch.load('best_model.pth')\n","saved_model.load_state_dict(best_model['model_state_dict'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-07T20:25:48.987321Z","iopub.status.busy":"2023-10-07T20:25:48.984089Z","iopub.status.idle":"2023-10-07T20:25:52.306106Z","shell.execute_reply":"2023-10-07T20:25:52.304975Z","shell.execute_reply.started":"2023-10-07T20:25:48.987283Z"},"trusted":true},"outputs":[],"source":["\n","all_preds = []\n","all_labels = []\n","\n","# Put the Saved Model in evaluation mode\n","saved_model.eval()\n","\n","# Disable gradient computation\n","with torch.no_grad():\n","    # Initialize tqdm progress bar for validation\n","    val_bar = tqdm(test_loader, desc='Test')\n","\n","    for batch in val_bar:\n","        input_ids, attention_mask, labels = batch\n","        input_ids = input_ids.to(device)\n","        attention_mask = attention_mask.to(device)\n","        labels = labels.to(device)\n","\n","        # Forward pass\n","        logits = saved_model(input_ids, attention_mask)\n","\n","        # Get the predicted labels\n","        _, preds = torch.max(logits, dim=1)\n","\n","        # Move preds and labels to CPU for further evaluation (if using GPU)\n","        preds = preds.cpu().numpy()\n","        labels = labels.cpu().numpy()\n","\n","        # Extend the list of predictions and labels\n","        all_preds.extend(preds)\n","        all_labels.extend(labels)\n","\n","# Evaluate the saved model's performance\n","accuracy = accuracy_score(all_labels, all_preds)\n","print(f'Test Accuracy: {accuracy}')\n","\n","# Confusion Matrix\n","conf_matrix = confusion_matrix(all_labels,all_preds)\n","confusion_matrix_plot = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)\n","confusion_matrix_plot.plot()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["### 7. Playground\n","\n","We play with the model by giving it some utterances and seeing what it predicts."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-07T20:22:06.519585Z","iopub.status.busy":"2023-10-07T20:22:06.517537Z","iopub.status.idle":"2023-10-07T20:22:06.544730Z","shell.execute_reply":"2023-10-07T20:22:06.543770Z","shell.execute_reply.started":"2023-10-07T20:22:06.519549Z"},"trusted":true},"outputs":[],"source":["def predict(model, sentence, tokenizer):\n","    model.eval()\n","    \n","    encoded = tokenizer.encode_plus(\n","        sentence, \n","        add_special_tokens=True, \n","        max_length=256, \n","        padding='max_length', \n","        truncation=True, \n","        return_attention_mask=True)\n","    \n","    input_ids = torch.tensor([encoded['input_ids']], dtype=torch.long).to(device)\n","    attention_mask = torch.tensor([encoded['attention_mask']], dtype=torch.long).to(device)\n","    \n","    # Make a prediction\n","    with torch.no_grad():\n","        logits = model(input_ids, attention_mask)\n","    \n","    # Decode the prediction to label\n","    _, preds = torch.max(logits, dim=1)\n","    label = \"hotel\" if preds.item() == 0 else \"restaurant\"\n","    \n","    return label\n","\n","\n","sentence = \"I would like to reserve for October 12th at 8:00.\"\n","predicted_label = predict(saved_model, sentence, tokenizer)\n","print(\"Predicted label:\", predicted_label)\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
