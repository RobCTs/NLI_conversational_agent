{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "DUeZkBL-bKDE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in c:\\users\\camil\\anaconda3\\envs\\my-torch\\lib\\site-packages (2.14.5)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\camil\\anaconda3\\envs\\my-torch\\lib\\site-packages (from datasets) (1.24.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\camil\\anaconda3\\envs\\my-torch\\lib\\site-packages (from datasets) (13.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in c:\\users\\camil\\anaconda3\\envs\\my-torch\\lib\\site-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in c:\\users\\camil\\anaconda3\\envs\\my-torch\\lib\\site-packages (from datasets) (2.1.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in c:\\users\\camil\\anaconda3\\envs\\my-torch\\lib\\site-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\camil\\anaconda3\\envs\\my-torch\\lib\\site-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in c:\\users\\camil\\anaconda3\\envs\\my-torch\\lib\\site-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in c:\\users\\camil\\anaconda3\\envs\\my-torch\\lib\\site-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in c:\\users\\camil\\anaconda3\\envs\\my-torch\\lib\\site-packages (from datasets) (2023.4.0)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\camil\\anaconda3\\envs\\my-torch\\lib\\site-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in c:\\users\\camil\\anaconda3\\envs\\my-torch\\lib\\site-packages (from datasets) (0.17.3)\n",
            "Requirement already satisfied: packaging in c:\\users\\camil\\anaconda3\\envs\\my-torch\\lib\\site-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\camil\\anaconda3\\envs\\my-torch\\lib\\site-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\camil\\anaconda3\\envs\\my-torch\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\camil\\anaconda3\\envs\\my-torch\\lib\\site-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\camil\\anaconda3\\envs\\my-torch\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\camil\\anaconda3\\envs\\my-torch\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\camil\\anaconda3\\envs\\my-torch\\lib\\site-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\camil\\anaconda3\\envs\\my-torch\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\camil\\anaconda3\\envs\\my-torch\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in c:\\users\\camil\\anaconda3\\envs\\my-torch\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\camil\\anaconda3\\envs\\my-torch\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\camil\\anaconda3\\envs\\my-torch\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\camil\\anaconda3\\envs\\my-torch\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.13)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\camil\\anaconda3\\envs\\my-torch\\lib\\site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: colorama in c:\\users\\camil\\anaconda3\\envs\\my-torch\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\camil\\anaconda3\\envs\\my-torch\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\camil\\anaconda3\\envs\\my-torch\\lib\\site-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\camil\\anaconda3\\envs\\my-torch\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\camil\\anaconda3\\envs\\my-torch\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: seqeval in c:\\users\\camil\\anaconda3\\envs\\my-torch\\lib\\site-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in c:\\users\\camil\\anaconda3\\envs\\my-torch\\lib\\site-packages (from seqeval) (1.24.1)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in c:\\users\\camil\\anaconda3\\envs\\my-torch\\lib\\site-packages (from seqeval) (1.3.1)\n",
            "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\camil\\anaconda3\\envs\\my-torch\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\camil\\anaconda3\\envs\\my-torch\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\camil\\anaconda3\\envs\\my-torch\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "!pip install seqeval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "dLA-joRTbDCi"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertTokenizerFast, BertForTokenClassification, DistilBertTokenizer, DistilBertModel, BertTokenizer, BertModel\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from seqeval.metrics import classification_report as seqeval_classification_report\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import typing\n",
        "import random\n",
        "import pickle\n",
        "import joblib\n",
        "from pydantic import BaseModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEVz1qkBm_hh",
        "outputId": "f571e11a-3758-4914-b7dd-b120e0d13629"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[WinError 3] The system cannot find the path specified: 'gdrive/My Drive/Colab Notebooks/NLI course'\n",
            "c:\\Users\\camil\\OneDrive\\Desktop\\emai_nli_project\n"
          ]
        }
      ],
      "source": [
        "# change the path to where your notebook is located\n",
        "%cd \"gdrive/My Drive/Colab Notebooks/NLI course\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation of our Dialog Act Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DAClassifier(nn.Module):\n",
        "    def __init__(self, num_classes, bert_model_type='distilbert-base-uncased'):\n",
        "        super(DAClassifier, self).__init__()\n",
        "        self.bert = DistilBertModel.from_pretrained(bert_model_type)\n",
        "        self.fc = nn.Linear(768, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
        "        cls_output = outputs.last_hidden_state[:, 0, :] # selects the [CLS] token position.\n",
        "        logits = torch.sigmoid(self.fc(cls_output))\n",
        "        return logits\n",
        "\n",
        "class DialogItemIdentifier(BaseModel):\n",
        "    id_dialog: str\n",
        "    order_in_dialog: int\n",
        "\n",
        "class DialogItem(BaseModel):\n",
        "    id_dialog: str\n",
        "    order_in_dialog: int\n",
        "    utterance: str\n",
        "    speaker: str\n",
        "    dialogue_acts: typing.List[str]\n",
        "    gt_dialogue_acts: typing.List[str]\n",
        "    previous_dialog_items: typing.List[DialogItemIdentifier] = []\n",
        "\n",
        "class DialogActModel:\n",
        "    PAST_HISTORY_LENGTH = 2\n",
        "\n",
        "    def __init__(self, model_path=\"\", mlb_path=\"\", bert_model_type='distilbert-base-uncased'):\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.tokenizer = DistilBertTokenizer.from_pretrained(bert_model_type)\n",
        "        self.mlb: MultiLabelBinarizer = pickle.load(open(mlb_path, 'rb'))\n",
        "        # Load the best model\n",
        "        saved_model = DAClassifier(num_classes=len(self.mlb.classes_)).to(self.device)\n",
        "        best_model = torch.load(model_path, map_location=torch.device(self.device))\n",
        "        saved_model.load_state_dict(best_model['model_state_dict'])\n",
        "\n",
        "        self.model = saved_model\n",
        "\n",
        "    def convert_dialogitem_encoded_history(self, dialog_item: DialogItem, dialog_item_dataset: typing.List[DialogItem], past_history_length: int = 2):\n",
        "        \"\"\"\n",
        "        Converts a DialogItem object into an Encoded History string.\n",
        "\n",
        "        Parameters:\n",
        "        - dialog_item: DialogItem object.\n",
        "        - dialog_item_dataset: List of DialogItem objects.\n",
        "        - past_history_length: Length of the past history to consider.\n",
        "\n",
        "\n",
        "        Returns:\n",
        "        - Encoded History string.\n",
        "        \"\"\"\n",
        "        encoded_history = \"\"\n",
        "\n",
        "        # Initialize the history of the user and agent as lists with empty DialogItem objects\n",
        "        agent_history: typing.List[DialogItem] = []\n",
        "        user_history: typing.List[DialogItem] = []\n",
        "        \n",
        "        # For each identifier of the previous dialog items, search in the dialog_item_dataset for the corresponding DialogItem object\n",
        "        # and append it to the agent or user history. If no DialogItem object is found, append a dialog item with empty utterance \"\" and dialogue acts []\n",
        "        for i in range(len(dialog_item.previous_dialog_items)):\n",
        "            for j in range(len(dialog_item_dataset)):\n",
        "                if dialog_item_dataset[j].id_dialog == dialog_item.previous_dialog_items[i].id_dialog and dialog_item_dataset[j].order_in_dialog == dialog_item.previous_dialog_items[i].order_in_dialog:\n",
        "                    if dialog_item_dataset[j].speaker == \"Agent\":\n",
        "                        agent_history.append(dialog_item_dataset[j])\n",
        "                        # If the agent history is longer than the past history length, remove the oldest turn\n",
        "                        if len(agent_history) > past_history_length:\n",
        "                            agent_history.pop(0)\n",
        "                    elif dialog_item_dataset[j].speaker == \"User\":\n",
        "                        user_history.append(dialog_item_dataset[j])\n",
        "                        # If the user history is longer than the past history length, remove the oldest turn\n",
        "                        if len(user_history) > past_history_length:\n",
        "                            user_history.pop(0)\n",
        "\n",
        "        # Fill up the encoded history of the user with the beggining of the array with DialogItems with empty utterance \"\" and dialogue acts [], for the amount\n",
        "        # of turns that are missing to reach the past history length\n",
        "        past_encoded_user_history = \"\"\n",
        "        for i in range(past_history_length - len(user_history)):\n",
        "            past_encoded_user_history += \">\".join([\"\", \"\"]) + \"|\"\n",
        "        for i in range(len(user_history)):\n",
        "            past_encoded_user_history += \">\".join([user_history[i].utterance, \"_\".join(user_history[i].dialogue_acts)]) + \"|\"\n",
        "\n",
        "        # Fill up the encoded history of the agent with the beggining of the array with DialogItems with empty utterance \"\" and dialogue acts [], for the amount\n",
        "        # of turns that are missing to reach the past history length\n",
        "        past_encoded_agent_history = \"\"\n",
        "        for i in range(past_history_length - len(agent_history)):\n",
        "            past_encoded_agent_history += \">\".join([\"\", \"\"]) + \"|\"\n",
        "        \n",
        "        for i in range(len(agent_history)):\n",
        "            past_encoded_agent_history += \">\".join([agent_history[i].utterance, \"_\".join(agent_history[i].dialogue_acts)]) + \"|\"\n",
        "\n",
        "        encoded_history = past_encoded_user_history + past_encoded_agent_history + dialog_item.utterance\n",
        "\n",
        "        return encoded_history\n",
        "    \n",
        "    def predict(self, encoded_history):\n",
        "        \"\"\"\n",
        "        Given an encoded history, predicts the dialogue acts of the last turn.\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "\n",
        "        # Separate the history from the current utterance splitting by the last \"|\" character, but don't remove it\n",
        "        history, utterance = encoded_history.rsplit(\"|\", 1)\n",
        "        \n",
        "        # Merge the history and sentence into a single string adding a [SEP] token between them\n",
        "        encoded_history = \"\".join(history) + \" [SEP] \" + utterance\n",
        "        \n",
        "        encoded = self.tokenizer.encode_plus(\n",
        "            encoded_history, \n",
        "            add_special_tokens=True, \n",
        "            max_length=256, \n",
        "            padding='max_length', \n",
        "            truncation=True, \n",
        "            return_attention_mask=True)\n",
        "        \n",
        "        input_ids = torch.tensor([encoded['input_ids']], dtype=torch.long).to(self.device)\n",
        "        attention_mask = torch.tensor([encoded['attention_mask']], dtype=torch.long).to(self.device)\n",
        "        \n",
        "        # Make a prediction\n",
        "        with torch.no_grad():\n",
        "            logits = self.model(input_ids, attention_mask)\n",
        "        \n",
        "        logits_cpu = logits.to('cpu')\n",
        "        return logits_cpu.numpy()\n",
        "\n",
        "    def predict_only_last_dialog_item(self, dialog_item_dataset: typing.List[DialogItem]) -> typing.List[str]:\n",
        "        \"\"\"\n",
        "        Predicts the dialogue act of last User turn in the dialog. For the Agent, the DAs are filled using the ground truth from the pre-processing function and it arrives here filled, since\n",
        "        we know what dialog acts the agent is performing. For previous User DAs, the ground truth is also used (allowed by professors)\n",
        "\n",
        "        Parameters:\n",
        "        - dialog_item_dataset: List of DialogItem objects.\n",
        "\n",
        "\n",
        "        Returns:\n",
        "        - List of predicted dialogue acts\n",
        "        \"\"\"\n",
        "\n",
        "        dataset_copy = dialog_item_dataset.copy()\n",
        "\n",
        "\n",
        "        # If the speaker is the User, predict the dialogue act\n",
        "        encoded_history = self.convert_dialogitem_encoded_history(dataset_copy[-1], dataset_copy)\n",
        "        preds = self.predict(encoded_history)\n",
        "        threshold = 0.5\n",
        "        all_preds_binary = []\n",
        "        for all_pred in preds:\n",
        "            local_pred = []\n",
        "            for old_local_pred in all_pred:\n",
        "                binary_local_pred = (old_local_pred > threshold).astype(int)\n",
        "                local_pred.append(binary_local_pred)\n",
        "            all_preds_binary.append(local_pred)\n",
        "        labels_preds = self.mlb.inverse_transform(np.array(all_preds_binary))\n",
        "        dialogue_acts = labels_preds[0]\n",
        "\n",
        "        return dialogue_acts\n",
        "\n",
        "    @staticmethod\n",
        "    def relabel_dialogue_act(dialogue_act: str):\n",
        "        if dialogue_act.split('-')[0].upper() not in ['RESTAURANT', 'HOTEL', 'BOOKING', 'GENERAL']:\n",
        "            new_dialog_act = dialogue_act.split('-')[0]\n",
        "        else:\n",
        "            new_dialog_act = dialogue_act\n",
        "\n",
        "        return new_dialog_act\n",
        "    \n",
        "    @staticmethod\n",
        "    def add_dialogue_items_to_dialogue_history(utterance: str, speaker: str, dialog_acts: typing.List[str], id_dialog: int, order_in_dialog: int, previous_dialog_history_ids: typing.List[DialogItemIdentifier], dialog_history: typing.List[DialogItem]):\n",
        "        dialogue_act_relabeled = []\n",
        "        for j in range(len(dialog_acts)):\n",
        "            dialogue_act_relabeled.append(DialogActModel.relabel_dialogue_act(dialog_acts[j]))\n",
        "\n",
        "        # Create a DialogItem object for this turn\n",
        "        dialog_item = DialogItem(\n",
        "            id_dialog=id_dialog,\n",
        "            order_in_dialog=order_in_dialog,\n",
        "            utterance=utterance,\n",
        "            speaker=speaker,\n",
        "            dialogue_acts=dialogue_act_relabeled,\n",
        "            gt_dialogue_acts=[],\n",
        "            previous_dialog_items=previous_dialog_history_ids\n",
        "        )\n",
        "\n",
        "        # Append the DialogItem object to the list of DialogItem objects\n",
        "        dialog_history.append(dialog_item)\n",
        "\n",
        "        # Append the DialogItemIdentifier object to the history of the user and agent\n",
        "        dialog_item_identifier = DialogItemIdentifier(\n",
        "            id_dialog=id_dialog,\n",
        "            order_in_dialog=order_in_dialog\n",
        "        )\n",
        "        previous_dialog_history_ids.append(dialog_item_identifier)\n",
        "\n",
        "        # If the history of the user and agent is longer than the past history length multiplied by two, which guarantees that this wont fail on the conversion and speeds up the process\n",
        "        # , remove the oldest turn\n",
        "        if len(previous_dialog_history_ids) > DialogActModel.PAST_HISTORY_LENGTH*2:\n",
        "            previous_dialog_history_ids.pop(0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwGUgusJbf9t"
      },
      "source": [
        "# Evaluation of our slot-filling Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "hFoka4tDbfPB"
      },
      "outputs": [],
      "source": [
        "class SlotFillingDataset:\n",
        "    def __init__(\n",
        "        self,\n",
        "        tokenizer=BertTokenizerFast.from_pretrained(\"bert-base-uncased\"),\n",
        "        max_length=128,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initializes the SlotFillingDataset class.\n",
        "        :param tokenizer: Tokenizer object used for tokenizing texts.\n",
        "        :param max_length: Maximum length of the tokenized inputs.\n",
        "        \"\"\"\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.train_dataset = None\n",
        "        self.val_dataset = None\n",
        "        self.test_dataset = None\n",
        "\n",
        "    def load(self, dataset_name=\"multi_woz_v22\"):\n",
        "        \"\"\"\n",
        "        Loads the dataset.\n",
        "        :param dataset_name: Name of the dataset to load.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            dataset = load_dataset(dataset_name)\n",
        "            self.train_dataset = dataset[\"train\"]\n",
        "            self.val_dataset = dataset[\"validation\"]\n",
        "            self.test_dataset = dataset[\"test\"]\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading dataset: {e}\")\n",
        "\n",
        "    def get_relevant_data(self, services={\"restaurant\", \"hotel\"}):\n",
        "        \"\"\"\n",
        "        Filters the dataset to include only entries with specified services.\n",
        "        :param services: A set of services to filter the data.\n",
        "        \"\"\"\n",
        "        if not self.train_dataset or not self.val_dataset or not self.test_dataset:\n",
        "            print(\"Dataset not loaded.\")\n",
        "            return\n",
        "        # Filter the dataset\n",
        "        self.train_dataset = [\n",
        "            entry\n",
        "            for entry in self.train_dataset\n",
        "            if set(entry[\"services\"]).issubset(services)\n",
        "        ]\n",
        "        self.val_dataset = [\n",
        "            entry\n",
        "            for entry in self.val_dataset\n",
        "            if set(entry[\"services\"]).issubset(services)\n",
        "        ]\n",
        "        self.test_dataset = [\n",
        "            entry\n",
        "            for entry in self.test_dataset\n",
        "            if set(entry[\"services\"]).issubset(services)\n",
        "        ]\n",
        "\n",
        "    def create_labelled_data(self, dataset):\n",
        "        \"\"\"\n",
        "        Creates the labelled data.\n",
        "        :param dataset: Dataset to create the labelled data from.\n",
        "        \"\"\"\n",
        "\n",
        "        # Initialize the labelled data\n",
        "        labelled_data = []\n",
        "\n",
        "        for dialogue in dataset:\n",
        "            turns = dialogue[\"turns\"]\n",
        "            for i, _ in enumerate(turns[\"turn_id\"]):\n",
        "                utterance = turns[\"utterance\"][i]\n",
        "\n",
        "                # Tokenize the combined text (do not use history for labelling)\n",
        "                encoded = self.tokenizer(\n",
        "                    utterance,\n",
        "                    add_special_tokens=True,\n",
        "                    return_offsets_mapping=True,\n",
        "                    padding=\"max_length\",\n",
        "                    truncation=True,\n",
        "                    max_length=self.max_length,\n",
        "                )\n",
        "                tokens = self.tokenizer.convert_ids_to_tokens(encoded[\"input_ids\"])\n",
        "                attention_mask = encoded[\"attention_mask\"]\n",
        "                offset_mapping = encoded[\"offset_mapping\"]\n",
        "\n",
        "                # Initialize labels\n",
        "                labels = [\"O\"] * len(tokens)\n",
        "\n",
        "                # If the turn has dialogue acts, label the tokens\n",
        "                if \"dialogue_acts\" in turns and i < len(turns[\"dialogue_acts\"]):\n",
        "                    act = turns[\"dialogue_acts\"][i]\n",
        "                    span_info = act.get(\"span_info\", {})\n",
        "\n",
        "                    # Iterate over the span info\n",
        "                    for act_type, act_slot_name, _, span_start, span_end in zip(\n",
        "                        span_info.get(\"act_type\", []),\n",
        "                        span_info.get(\"act_slot_name\", []),\n",
        "                        span_info.get(\"act_slot_value\", []),\n",
        "                        span_info.get(\"span_start\", []),\n",
        "                        span_info.get(\"span_end\", []),\n",
        "                    ):\n",
        "                        # Initialize the start and end token index\n",
        "                        start_token_idx = None\n",
        "                        end_token_idx = None\n",
        "                        \n",
        "                        prefix = act_type.split(\"-\")[0].lower()\n",
        "\n",
        "                        # Find the tokens corresponding to the span\n",
        "                        for idx, offset in enumerate(offset_mapping):\n",
        "                            if start_token_idx is None and offset[0] == span_start:\n",
        "                                start_token_idx = idx\n",
        "                            if offset[1] == span_end:\n",
        "                                end_token_idx = idx\n",
        "                                break\n",
        "\n",
        "                        # If the span is found, label the tokens\n",
        "                        if start_token_idx is not None and end_token_idx is not None:\n",
        "                            if start_token_idx < len(tokens) and end_token_idx < len(\n",
        "                                tokens\n",
        "                            ):\n",
        "                                # Label tokens using IOB format with the actual ground truth slot value\n",
        "                                labels[start_token_idx] = f\"B-{prefix}-{act_slot_name}\"\n",
        "                                for j in range(start_token_idx + 1, end_token_idx + 1):\n",
        "                                    labels[j] = f\"I-{prefix}-{act_slot_name}\"\n",
        "                            else:\n",
        "                                print(\n",
        "                                    f\"Warning: Index out of range for utterance '{utterance}' with span {span_start}-{span_end}\"\n",
        "                                )\n",
        "                # Add the encoded text and labels to the labelled data\n",
        "                labelled_data.append((encoded, labels))\n",
        "\n",
        "        return labelled_data\n",
        "\n",
        "\n",
        "    def create_labelled_dialogue_data(self, dataset):\n",
        "        \"\"\"\n",
        "        Creates the labelled data in the format (utterance, {slots, values}) for each dialogue.\n",
        "        :param dataset: Dataset to create the labelled data from.\n",
        "        \"\"\"\n",
        "        labelled_data = []\n",
        "\n",
        "        for dialogue in dataset:\n",
        "            turns = dialogue[\"turns\"]\n",
        "            dialogue_data = []\n",
        "            for i, _ in enumerate(turns[\"turn_id\"]):\n",
        "                utterance = turns[\"utterance\"][i]\n",
        "                slot_values = {}\n",
        "\n",
        "                # If the turn has dialogue acts, extract slots and values\n",
        "                if \"dialogue_acts\" in turns and i < len(turns[\"dialogue_acts\"]):\n",
        "                    act = turns[\"dialogue_acts\"][i]\n",
        "                    span_info = act.get(\"span_info\", {})\n",
        "\n",
        "                    for act_slot_name, act_slot_value in zip(\n",
        "                        span_info.get(\"act_slot_name\", []),\n",
        "                        span_info.get(\"act_slot_value\", []),\n",
        "                    ):\n",
        "                        slot_values[act_slot_name] = act_slot_value\n",
        "\n",
        "                # Append the utterance and extracted slot values to the dialogue data\n",
        "                dialogue_data.append((utterance, slot_values))\n",
        "\n",
        "            # Add each complete dialogue to the labelled data\n",
        "            labelled_data.append(dialogue_data)\n",
        "\n",
        "        return labelled_data\n",
        "\n",
        "    def create_label2id(self, labelled_data):\n",
        "        \"\"\"\n",
        "        Creates the label2id mapping.\n",
        "        :param labelled_data: Processed data from create_labelled_data method (dictionary).\n",
        "        Returns:\n",
        "        label2id: Mapping from labels to ids.\n",
        "        num_labels: Number of unique labels.\n",
        "        \"\"\"\n",
        "        unique_labels = set()\n",
        "        for _, labels in labelled_data:\n",
        "            unique_labels.update(set(labels))\n",
        "        label2id = {label: idx for idx, label in enumerate(sorted(unique_labels))}\n",
        "        return label2id, len(unique_labels)\n",
        "\n",
        "    def id2label(self, label2id):\n",
        "        \"\"\"\n",
        "        Creates the id2label mapping.\n",
        "        :param label2id: Mapping from labels to ids.\n",
        "        \"\"\"\n",
        "        id2label = {idx: label for label, idx in label2id.items()}\n",
        "        return id2label\n",
        "\n",
        "\n",
        "class TensorDataset(Dataset):\n",
        "    def __init__(self, labelled_data, label2id):\n",
        "        \"\"\"\n",
        "        Initializes the SlotFillingData class.\n",
        "        :param labelled_data: Processed data from create_labelled_data method [(encoded, label)].\n",
        "        :param label2id: Mapping from labels to ids.\n",
        "        \"\"\"\n",
        "        self.labelled_data = labelled_data\n",
        "        self.label2id = label2id\n",
        "        self.attention_mask = None\n",
        "        self.input_ids = None\n",
        "        self.labels = None\n",
        "\n",
        "    def create_tensors(self):\n",
        "        \"\"\"\n",
        "        Creates the tensors.\n",
        "        \"\"\"\n",
        "        self.input_ids = torch.tensor([t[0][\"input_ids\"] for t in self.labelled_data])\n",
        "        self.attention_mask = torch.tensor(\n",
        "            [t[0][\"attention_mask\"] for t in self.labelled_data]\n",
        "        )\n",
        "        self.labels = [\n",
        "            [self.label2id[label] for label in t[1]] for t in self.labelled_data\n",
        "        ]\n",
        "        self.labels = torch.tensor(self.labels)\n",
        "\n",
        "    def create_dataloader(self, batch_size=32):\n",
        "        \"\"\"\n",
        "        Creates the dataloader.\n",
        "        :param batch_size: Batch size for training.\n",
        "        \"\"\"\n",
        "        dataset = torch.utils.data.TensorDataset(\n",
        "            self.input_ids, self.attention_mask, self.labels\n",
        "        )\n",
        "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n",
        "        return dataloader\n",
        "\n",
        "\n",
        "class SlotFillingModel:\n",
        "    def __init__(\n",
        "        self,\n",
        "        train_dataloader,\n",
        "        val_dataloader,\n",
        "        test_dataloader,\n",
        "        tokenizer,\n",
        "        num_labels,\n",
        "        label2id,\n",
        "        id2label,\n",
        "        max_length=128,\n",
        "        batch_size=32,\n",
        "        epochs=7,\n",
        "        patience=2,\n",
        "        lr=2e-5,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initializes the SlotFillingModel class.\n",
        "        :param train_dataloader: Dataloader for training.\n",
        "        :param val_dataloader: Dataloader for validation.\n",
        "        :param test_dataloader: Dataloader for testing.\n",
        "        :param tokenizer: Tokenizer object used for tokenizing texts.\n",
        "        :param num_labels: Number of unique labels.\n",
        "        :param label2id: Mapping from labels to ids.\n",
        "        :param id2label: Mapping from ids to labels.\n",
        "        :param max_length: Maximum length of the tokenized inputs.\n",
        "        :param batch_size: Batch size for training.\n",
        "        :param epochs: Number of epochs for training.\n",
        "        :param patience: Number of epochs to wait for improvement before early stopping.\n",
        "        :param lr: Learning rate for the optimizer.\n",
        "        \"\"\"\n",
        "        self.model = None\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.batch_size = batch_size\n",
        "        self.epochs = epochs\n",
        "        self.patience = patience\n",
        "        self.lr = lr\n",
        "        self.num_labels = num_labels\n",
        "        self.label2id = label2id\n",
        "        self.id2label = id2label\n",
        "        self.device = (\n",
        "            torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "        )\n",
        "        self.train_dataloader = train_dataloader\n",
        "        self.val_dataloader = val_dataloader\n",
        "        self.test_dataloader = test_dataloader\n",
        "\n",
        "    def create_model(self, model_name=\"bert-base-uncased\"):\n",
        "        \"\"\"\n",
        "        Creates the model.\n",
        "        :param model_name: Name of the model to use, default is 'bert-base-uncased'.\n",
        "        \"\"\"\n",
        "        self.model = BertForTokenClassification.from_pretrained(\n",
        "            model_name, num_labels=self.num_labels\n",
        "        ).to(self.device)\n",
        "\n",
        "    def train(self):\n",
        "        if not self.model:\n",
        "            print(\"Model not created.\")\n",
        "            return\n",
        "\n",
        "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
        "        criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "        # Initialize the early stopping counter\n",
        "        best_val_loss = float(\"inf\")\n",
        "        patience_counter = 0\n",
        "\n",
        "        # Training loop\n",
        "        for epoch in range(self.epochs):\n",
        "            self.model.train()\n",
        "            train_loss = 0\n",
        "            train_progress_bar = tqdm(\n",
        "                self.train_dataloader,\n",
        "                desc=f\"Epoch {epoch+1}/{self.epochs} Training\",\n",
        "                leave=False,\n",
        "            )\n",
        "\n",
        "            # Training phase\n",
        "            for batch in train_progress_bar:\n",
        "                batch = tuple(t.to(self.device) for t in batch)\n",
        "                b_input_ids, b_input_mask, b_labels = batch\n",
        "                optimizer.zero_grad()\n",
        "                outputs = self.model(\n",
        "                    b_input_ids,\n",
        "                    token_type_ids=None,\n",
        "                    attention_mask=b_input_mask,\n",
        "                    labels=b_labels,\n",
        "                )\n",
        "                loss = outputs.loss\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                train_loss += loss.item()\n",
        "                train_progress_bar.set_postfix(train_loss=loss.item())\n",
        "\n",
        "            avg_train_loss = train_loss / len(self.train_dataloader)\n",
        "            print(f\"Epoch {epoch + 1}/{self.epochs} | Train Loss: {avg_train_loss}\")\n",
        "\n",
        "            # Validation phase\n",
        "            self.model.eval()\n",
        "            val_loss = 0\n",
        "            val_progress_bar = tqdm(\n",
        "                self.val_dataloader,\n",
        "                desc=f\"Epoch {epoch+1}/{self.epochs} Validation\",\n",
        "                leave=False,\n",
        "            )\n",
        "            for batch in val_progress_bar:\n",
        "                batch = tuple(t.to(self.device) for t in batch)\n",
        "                b_input_ids, b_input_mask, b_labels = batch\n",
        "                with torch.no_grad():\n",
        "                    outputs = self.model(\n",
        "                        b_input_ids,\n",
        "                        token_type_ids=None,\n",
        "                        attention_mask=b_input_mask,\n",
        "                        labels=b_labels,\n",
        "                    )\n",
        "                    loss = outputs.loss\n",
        "                    val_loss += loss.item()\n",
        "                    val_progress_bar.set_postfix(val_loss=loss.item())\n",
        "\n",
        "            avg_val_loss = val_loss / len(self.val_dataloader)\n",
        "            print(f\"Epoch {epoch + 1}/{self.epochs} | Validation Loss: {avg_val_loss}\")\n",
        "\n",
        "            # Check if the validation loss is lower than the best one seen so far\n",
        "            if avg_val_loss < best_val_loss:\n",
        "                best_val_loss = avg_val_loss\n",
        "                patience_counter = 0\n",
        "                torch.save(self.model.state_dict(), f\"checkpoint_epoch_{epoch+1}.pt\")\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= self.patience:\n",
        "                    print(\"Early stopping!\")\n",
        "                    break\n",
        "        print(\"Training complete. Final model saved.\")\n",
        "\n",
        "    def test(self):\n",
        "        if not self.model:\n",
        "            print(\"Model not created.\")\n",
        "            return\n",
        "\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        all_predictions = []\n",
        "        all_true_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in self.test_dataloader:\n",
        "                batch = tuple(t.to(self.device) for t in batch)\n",
        "                b_input_ids, b_attention_masks, b_labels = batch\n",
        "\n",
        "                outputs = self.model(\n",
        "                    b_input_ids,\n",
        "                    token_type_ids=None,\n",
        "                    attention_mask=b_attention_masks,\n",
        "                    labels=b_labels,\n",
        "                )\n",
        "                loss = outputs.loss\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                # Move logits and labels to CPU\n",
        "                logits = outputs.logits.detach().cpu().numpy()\n",
        "                label_ids = b_labels.to(\"cpu\").numpy()\n",
        "\n",
        "                # Convert logits to token predictions\n",
        "                predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "                # For each item in the batch...\n",
        "                for i in range(b_input_ids.size(0)):\n",
        "                    # Skip predictions for tokens with label_id == -100\n",
        "                    pred_label_sequence = []\n",
        "                    true_label_sequence = []\n",
        "                    for j, (pred_id, label_id) in enumerate(\n",
        "                        zip(predictions[i], label_ids[i])\n",
        "                    ):\n",
        "                        if b_attention_masks[i][j] != 0 and label_id != -100:\n",
        "                            pred_label_sequence.append(\n",
        "                                self.id2label.get(pred_id, \"O\")\n",
        "                            )  # Default to 'O' if key is not found\n",
        "                            true_label_sequence.append(self.id2label[label_id])\n",
        "\n",
        "                    # Ensure the true and predicted sequences have the same length\n",
        "                    if len(true_label_sequence) != len(pred_label_sequence):\n",
        "                        print(\n",
        "                            f\"Length mismatch in sequence {i}: true labels {len(true_label_sequence)} vs. predicted labels {len(pred_label_sequence)}\"\n",
        "                        )\n",
        "                        # Output the actual sequences to help diagnose the issue\n",
        "                        print(\"True labels:\", true_label_sequence)\n",
        "                        print(\"Pred labels:\", pred_label_sequence)\n",
        "                        continue\n",
        "\n",
        "                    # ...extend the true labels and predicted labels lists\n",
        "                    all_true_labels.append(true_label_sequence)\n",
        "                    all_predictions.append(pred_label_sequence)\n",
        "\n",
        "        # Calculate average loss over all the batches\n",
        "        avg_loss = total_loss / len(self.test_dataloader)\n",
        "        print(f\"Test loss: {avg_loss}\")\n",
        "\n",
        "        # Use seqeval to compute a classification report\n",
        "        seqeval_report = seqeval_classification_report(all_true_labels, all_predictions)\n",
        "        print(seqeval_report)\n",
        "\n",
        "    def query(self, utterance):\n",
        "        if not self.model:\n",
        "            print(\"Model not created.\")\n",
        "            return\n",
        "\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            encoded_input = self.tokenizer(\n",
        "                utterance,\n",
        "                add_special_tokens=True,\n",
        "                return_offsets_mapping=True,\n",
        "                padding=\"max_length\",\n",
        "                truncation=True,\n",
        "                max_length=self.max_length,\n",
        "                return_tensors=\"pt\",\n",
        "            )\n",
        "\n",
        "        # Move tensors to the correct device\n",
        "        input_ids = encoded_input[\"input_ids\"].to(self.device)\n",
        "        attention_masks = encoded_input[\"attention_mask\"].to(self.device)\n",
        "\n",
        "        # Forward pass\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(input_ids, attention_mask=attention_masks)\n",
        "            logits = outputs.logits\n",
        "            logits = logits.detach().cpu()\n",
        "\n",
        "        # Use logits and id2label to get the predicted labels\n",
        "        predictions = torch.argmax(logits, dim=2).squeeze().tolist()\n",
        "        offset_mapping = encoded_input[\"offset_mapping\"].squeeze().tolist()\n",
        "\n",
        "        # Map predictions back to original words\n",
        "        word_labels = []\n",
        "        last_word_end = None\n",
        "        for label_id, offset in zip(predictions, offset_mapping):\n",
        "            word_start, word_end = offset\n",
        "\n",
        "            # Check if this is the start of a new word\n",
        "            if word_start != last_word_end:\n",
        "                word_label = self.id2label.get(label_id, \"O\")\n",
        "                word_labels.append(word_label)\n",
        "\n",
        "            last_word_end = word_end\n",
        "\n",
        "        return word_labels\n",
        "\n",
        "    def query_slots(self, utterance):\n",
        "        if not self.model:\n",
        "            print(\"Model not created.\")\n",
        "            return\n",
        "\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            encoded_input = self.tokenizer(\n",
        "                utterance,\n",
        "                add_special_tokens=True,\n",
        "                return_offsets_mapping=True,\n",
        "                padding=\"max_length\",\n",
        "                truncation=True,\n",
        "                max_length=self.max_length,\n",
        "                return_tensors=\"pt\",\n",
        "            )\n",
        "\n",
        "        # Move tensors to the correct device\n",
        "        input_ids = encoded_input[\"input_ids\"].to(self.device)\n",
        "        attention_masks = encoded_input[\"attention_mask\"].to(self.device)\n",
        "\n",
        "        # Forward pass\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(input_ids, attention_mask=attention_masks)\n",
        "            logits = outputs.logits\n",
        "            logits = logits.detach().cpu()\n",
        "\n",
        "        # Use logits and id2label to get the predicted labels\n",
        "        predictions = torch.argmax(logits, dim=2).squeeze().tolist()\n",
        "        offset_mapping = encoded_input[\"offset_mapping\"].squeeze().tolist()\n",
        "        tokens = self.tokenizer.convert_ids_to_tokens(input_ids.squeeze().tolist())\n",
        "\n",
        "        # Extracting slot values from the utterance\n",
        "        slot_values = {}\n",
        "        current_slot = None\n",
        "        current_value = \"\"\n",
        "\n",
        "        for token, label_id, (word_start, word_end) in zip(\n",
        "            tokens, predictions, offset_mapping\n",
        "        ):\n",
        "            label = self.id2label.get(label_id, \"O\")\n",
        "\n",
        "            if label.startswith(\"B-\"):\n",
        "                # Save the previous slot and value if any\n",
        "                if current_slot:\n",
        "                    slot_values[current_slot] = current_value.strip()\n",
        "\n",
        "                current_slot = label[2:]  # Remove the 'B-' prefix\n",
        "                current_value = utterance[word_start:word_end]\n",
        "\n",
        "            elif label.startswith(\"I-\") and current_slot:\n",
        "                current_value += \" \" + utterance[word_start:word_end]\n",
        "\n",
        "            elif label == \"O\":\n",
        "                # Save the previous slot and value if any\n",
        "                if current_slot:\n",
        "                    slot_values[current_slot] = current_value.strip()\n",
        "                    current_slot = None\n",
        "                    current_value = \"\"\n",
        "\n",
        "        # Save the last found slot and value\n",
        "        if current_slot:\n",
        "            slot_values[current_slot] = current_value.strip()\n",
        "\n",
        "        return slot_values\n",
        "\n",
        "\n",
        "class DialogSlotMemory:\n",
        "    slot_list_dict: typing.Dict[str, typing.List[str]] = {}\n",
        "\n",
        "    def __init__(self):\n",
        "        self.slot_list_dict = {}\n",
        "\n",
        "    def add_slot(self, slot_name: str, slot_value: str):\n",
        "        if slot_name not in self.slot_list_dict:\n",
        "            self.slot_list_dict[slot_name] = []\n",
        "        self.slot_list_dict[slot_name].append(slot_value)\n",
        "\n",
        "    def get_slot_values(self, slot_name: str):\n",
        "        return self.slot_list_dict[slot_name]\n",
        "\n",
        "    def get_most_recent_slot_value(self, slot_name: str):\n",
        "        return (\n",
        "            self.slot_list_dict[slot_name][-1]\n",
        "            if slot_name in self.slot_list_dict\n",
        "            else None\n",
        "        )\n",
        "\n",
        "    def get_all_slot_values(self):\n",
        "        return self.slot_list_dict\n",
        "\n",
        "    def get_all_slot_names(self):\n",
        "        return self.slot_list_dict.keys()\n",
        "\n",
        "\n",
        "class SlotValueParser:\n",
        "    def extract_slot_spans(self, dataset):\n",
        "        \"\"\"\n",
        "        Extracts unique tuples of actual slot values and their spans in the utterance.\n",
        "        :param dataset: Dataset to extract slot spans from.\n",
        "        \"\"\"\n",
        "        extracted_data = []\n",
        "\n",
        "        for dialogue in dataset:\n",
        "            turns = dialogue[\"turns\"]\n",
        "            for i, _ in enumerate(turns[\"turn_id\"]):\n",
        "                utterance = turns[\"utterance\"][i]\n",
        "\n",
        "                # Extracting slot values and their spans\n",
        "                if \"dialogue_acts\" in turns and i < len(turns[\"dialogue_acts\"]):\n",
        "                    act = turns[\"dialogue_acts\"][i]\n",
        "                    span_info = act.get(\"span_info\", {})\n",
        "\n",
        "                    for act_slot_name, act_slot_value, span_start, span_end in zip(\n",
        "                        span_info.get(\"act_slot_name\", []),\n",
        "                        span_info.get(\"act_slot_value\", []),\n",
        "                        span_info.get(\"span_start\", []),\n",
        "                        span_info.get(\"span_end\", []),\n",
        "                    ):\n",
        "                        # Finding the actual span in the utterance\n",
        "                        actual_span = utterance[span_start:span_end]\n",
        "\n",
        "                        # Storing unique slot value and span tuples\n",
        "                        slot_span_tuple = (act_slot_name, act_slot_value, actual_span)\n",
        "                        if slot_span_tuple not in extracted_data:\n",
        "                            extracted_data.append(slot_span_tuple)\n",
        "\n",
        "        return extracted_data\n",
        "\n",
        "    def get_mismatched_slot_values(self, extracted_data):\n",
        "        \"\"\"\n",
        "        Get the mismatched slot values stored as a dataframe\n",
        "        :param extracted_data: Extracted slot spans from the dataset in the format (slot_name, slot_value, actual_span).\n",
        "        \"\"\"\n",
        "\n",
        "        mismatched_slot_values = []\n",
        "        for slot_name, slot_value, actual_span in extracted_data:\n",
        "            if slot_value != actual_span:\n",
        "                mismatched_slot_values.append([slot_name, slot_value, actual_span])\n",
        "        mismatched_slot_values = pd.DataFrame(\n",
        "            mismatched_slot_values, columns=[\"slot_name\", \"slot_value\", \"actual_span\"]\n",
        "        )\n",
        "        return mismatched_slot_values\n",
        "\n",
        "    def get_mismatched_slot_values(self, extracted_data):\n",
        "        \"\"\"\n",
        "        Get the mismatched slot values stored as a dataframe\n",
        "        :param extracted_data: Extracted slot spans from the dataset in the format (slot_name, slot_value, actual_span).\n",
        "        \"\"\"\n",
        "\n",
        "        mismatched_slot_values = []\n",
        "        for slot_name, slot_value, actual_span in extracted_data:\n",
        "            if slot_value != actual_span:\n",
        "                mismatched_slot_values.append([slot_name, slot_value, actual_span])\n",
        "        mismatched_slot_values = pd.DataFrame(\n",
        "            mismatched_slot_values, columns=[\"slot_name\", \"slot_value\", \"actual_span\"]\n",
        "        )\n",
        "        return mismatched_slot_values\n",
        "\n",
        "    def convert_span_to_slot_value(\n",
        "        self, slot_name, actual_span, dialogue_slot_memory=None ):\n",
        "        \"\"\"\n",
        "        Convert the actual span to the slot value\n",
        "        :param slot_name: Name of the slot.\n",
        "        :param actual_span: Actual span in the utterance.\n",
        "        :param dialogue_slot_memory: Dialogue slot memory object containing the slot values from the dialogue history.\n",
        "        \"\"\"\n",
        "\n",
        "        # Remove the lowercase and whitespace initially\n",
        "        standardized_span = actual_span.lower().strip()\n",
        "\n",
        "        # dictionary to convert strings to number representations\n",
        "        number_dict = {\n",
        "            \"zero\": \"0\",\n",
        "            \"one\": \"1\",\n",
        "            \"two\": \"2\",\n",
        "            \"three\": \"3\",\n",
        "            \"four\": \"4\",\n",
        "            \"five\": \"5\",\n",
        "            \"six\": \"6\",\n",
        "            \"seven\": \"7\",\n",
        "            \"eight\": \"8\",\n",
        "            \"nine\": \"9\",\n",
        "            \"ten\": \"10\",\n",
        "            \"eleven\": \"11\",\n",
        "            \"twelve\": \"12\",\n",
        "            \"thirteen\": \"13\",\n",
        "            \"fourteen\": \"14\",\n",
        "            \"fifteen\": \"15\",\n",
        "            \"sixteen\": \"16\",\n",
        "            \"seventeen\": \"17\",\n",
        "            \"eighteen\": \"18\",\n",
        "            \"nineteen\": \"19\",\n",
        "            \"twenty\": \"20\",\n",
        "        }\n",
        "\n",
        "        # if span is a number word, convert it to a number and return\n",
        "        if standardized_span in number_dict:\n",
        "            return number_dict[standardized_span]\n",
        "\n",
        "        # if the slot is a postcode, uppercase the letters and return\n",
        "        if slot_name == \"postcode\":\n",
        "            return standardized_span.upper()\n",
        "\n",
        "        # arbitrary spelling for \"centre\" in the dataset\n",
        "        if standardized_span == \"center\":\n",
        "            return \"centre\"\n",
        "\n",
        "        # if slot is a time, convert the first word to a number and uppercase the second word\n",
        "        if slot_name == \"booktime\":\n",
        "            # check if the span consists of two words\n",
        "            if len(standardized_span.split()) >= 2:\n",
        "                # convert the first word to a number\n",
        "                partial = number_dict.get(\n",
        "                    standardized_span.split()[0], standardized_span.split()[0]\n",
        "                )\n",
        "                # keep the distance between number and second word e.g.'9 pm': '9 PM'\n",
        "                standardized_span = partial + \" \" + standardized_span.split()[1]\n",
        "                return standardized_span\n",
        "            else:\n",
        "                return standardized_span\n",
        "\n",
        "        # if span indicates indifference to a slot value, return \"dontcare\"\n",
        "        if standardized_span in [\n",
        "            \"any\",\n",
        "            \"anything\",\n",
        "            \"anywhere\",\n",
        "            \"dont care\",\n",
        "            \"dontcare\",\n",
        "            \"dontcare\",\n",
        "            \"don't care\",\n",
        "            \"do not care\",\n",
        "            \"don't really care\",\n",
        "            \"doesnt matter\",\n",
        "            \"doesn't matter\",\n",
        "            \"does not matter\",\n",
        "            \"doesn't really matter\",\n",
        "            \"not really\",\n",
        "            \"no preference\",\n",
        "            \"no particular\",\n",
        "            \"not particular\",\n",
        "            \"either one is fine\",\n",
        "            \"either is fine\",\n",
        "            \"don't have a preference\",\n",
        "            \"do not have a preference\",\n",
        "        ]:\n",
        "            return \"dontcare\"\n",
        "        \n",
        "        if slot_name == \"name\":\n",
        "            s = standardized_span.split()\n",
        "            if actual_span.islower(): #or if s[1][0].isupper(): #for single case: name 'The Allenbell':'the Allenbell'\n",
        "                for i in range(len(s)): #name 'Nandos': 'nandos', name 'Lensfield Hotel': 'lensfield hotel'\n",
        "                    s[i] = s[i].capitalize()\n",
        "                return ' '.join(s)\n",
        "            \n",
        "        #each word in address always capitalized regardless of actual_span value\n",
        "        if slot_name == \"address\":\n",
        "            s = standardized_span.split()\n",
        "            for i in range(len(s)):\n",
        "                s[i] = s[i].capitalize()\n",
        "            return' '.join(s)\n",
        "\n",
        "        # if span contains 'same' return the relevant value from dialogue history\n",
        "        if \"same\" in standardized_span.split() and dialogue_slot_memory is not None:\n",
        "            # get the most recent slot value from the dialogue history\n",
        "            return dialogue_slot_memory.get_most_recent_slot_value(slot_name)\n",
        "\n",
        "        return standardized_span\n",
        "\n",
        "\n",
        "def train_and_test_model():\n",
        "    # Create the dataset\n",
        "    dataset = SlotFillingDataset()\n",
        "    # Load the dataset\n",
        "    dataset.load()\n",
        "    # Filter the dataset\n",
        "    dataset.get_relevant_data({\"restaurant\", \"hotel\"})\n",
        "    # Create the labelled training data\n",
        "    labelled_data = dataset.create_labelled_data(dataset.train_dataset)\n",
        "    # Create the labelled validation data\n",
        "    labelled_data_val = dataset.create_labelled_data(dataset.val_dataset)\n",
        "    # Create the labelled test data\n",
        "    labelled_data_test = dataset.create_labelled_data(dataset.test_dataset)\n",
        "    # Create the label2id mapping\n",
        "    label2id, num_labels = dataset.create_label2id(labelled_data)\n",
        "    # Create the TensorDataset for training\n",
        "    tensor_dataset_train = TensorDataset(labelled_data, label2id)\n",
        "    tensor_dataset_train.create_tensors()\n",
        "    # Create the TensorDataset for validation\n",
        "    tensor_dataset_val = TensorDataset(labelled_data_val, label2id)\n",
        "    tensor_dataset_val.create_tensors()\n",
        "    # Create the TensorDataset for test\n",
        "    tensor_dataset_test = TensorDataset(labelled_data_test, label2id)\n",
        "    tensor_dataset_test.create_tensors()\n",
        "    # Create the dataloader for training\n",
        "    train_dataloader = tensor_dataset_train.create_dataloader()\n",
        "    # Create the dataloader for validation\n",
        "    val_dataloader = tensor_dataset_val.create_dataloader()\n",
        "    # Create the dataloader for test\n",
        "    test_dataloader = tensor_dataset_test.create_dataloader()\n",
        "    # Create the model\n",
        "    model = SlotFillingModel(\n",
        "        train_dataloader,\n",
        "        val_dataloader,\n",
        "        test_dataloader,\n",
        "        dataset.tokenizer,\n",
        "        num_labels,\n",
        "        label2id,\n",
        "        dataset.id2label(label2id),\n",
        "    )\n",
        "    model.create_model()\n",
        "    # Train the model\n",
        "    model.train()\n",
        "    # Test the model\n",
        "    model.test()\n",
        "    # Query the model\n",
        "    res = model.query(\"I want to book a table for 4 people at 7pm tonight.\")\n",
        "    print(res)\n",
        "\n",
        "def return_model(model_path=\"checkpoint_epoch_3.pt\"):\n",
        "    # Create dialogues dataset'\n",
        "    dataset = SlotFillingDataset()\n",
        "    # Load the dataset\n",
        "    dataset.load()\n",
        "    # Filter the dataset\n",
        "    dataset.get_relevant_data({\"restaurant\", \"hotel\"})\n",
        "    # Create the labelled training data\n",
        "    labelled_data = dataset.create_labelled_data(dataset.train_dataset)\n",
        "    # Create the labelled validation data\n",
        "    labelled_data_val = dataset.create_labelled_data(dataset.val_dataset)\n",
        "    # Create the labelled test data\n",
        "    labelled_data_test = dataset.create_labelled_data(dataset.test_dataset)\n",
        "    # Create the label2id mapping\n",
        "    label2id, num_labels = dataset.create_label2id(labelled_data)\n",
        "    # Create the TensorDataset for training\n",
        "    tensor_dataset_train = TensorDataset(labelled_data, label2id)\n",
        "    tensor_dataset_train.create_tensors()\n",
        "    # Create the TensorDataset for validation\n",
        "    tensor_dataset_val = TensorDataset(labelled_data_val, label2id)\n",
        "    tensor_dataset_val.create_tensors()\n",
        "    # Create the TensorDataset for test\n",
        "    tensor_dataset_test = TensorDataset(labelled_data_test, label2id)\n",
        "    tensor_dataset_test.create_tensors()\n",
        "    # Create the dataloader for training\n",
        "    train_dataloader = tensor_dataset_train.create_dataloader()\n",
        "    # Create the dataloader for validation\n",
        "    val_dataloader = tensor_dataset_val.create_dataloader()\n",
        "    # Create the dataloader for test\n",
        "    test_dataloader = tensor_dataset_test.create_dataloader()\n",
        "    # create model from checkpoint\n",
        "    model = SlotFillingModel(\n",
        "        train_dataloader,\n",
        "        val_dataloader,\n",
        "        test_dataloader,\n",
        "        dataset.tokenizer,\n",
        "        num_labels,\n",
        "        label2id,\n",
        "        dataset.id2label(label2id),\n",
        "    )\n",
        "    model.create_model()\n",
        "    model.model.load_state_dict(torch.load(model_path))\n",
        "    model.model.eval()\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_and_query_model(query, model_path=\"checkpoint_epoch_3.pt\"):\n",
        "    # Create dialogues dataset'\n",
        "    dataset = SlotFillingDataset()\n",
        "    # Load the dataset\n",
        "    dataset.load()\n",
        "    # Filter the dataset\n",
        "    dataset.get_relevant_data({\"restaurant\", \"hotel\"})\n",
        "    # Create the labelled training data\n",
        "    labelled_data = dataset.create_labelled_data(dataset.train_dataset)\n",
        "    # Create the labelled validation data\n",
        "    labelled_data_val = dataset.create_labelled_data(dataset.val_dataset)\n",
        "    # Create the labelled test data\n",
        "    labelled_data_test = dataset.create_labelled_data(dataset.test_dataset)\n",
        "    # Create the label2id mapping\n",
        "    label2id, num_labels = dataset.create_label2id(labelled_data)\n",
        "    # Create the TensorDataset for training\n",
        "    tensor_dataset_train = TensorDataset(labelled_data, label2id)\n",
        "    tensor_dataset_train.create_tensors()\n",
        "    # Create the TensorDataset for validation\n",
        "    tensor_dataset_val = TensorDataset(labelled_data_val, label2id)\n",
        "    tensor_dataset_val.create_tensors()\n",
        "    # Create the TensorDataset for test\n",
        "    tensor_dataset_test = TensorDataset(labelled_data_test, label2id)\n",
        "    tensor_dataset_test.create_tensors()\n",
        "    # Create the dataloader for training\n",
        "    train_dataloader = tensor_dataset_train.create_dataloader()\n",
        "    # Create the dataloader for validation\n",
        "    val_dataloader = tensor_dataset_val.create_dataloader()\n",
        "    # Create the dataloader for test\n",
        "    test_dataloader = tensor_dataset_test.create_dataloader()\n",
        "    # create model from checkpoint\n",
        "    model = SlotFillingModel(\n",
        "        train_dataloader,\n",
        "        val_dataloader,\n",
        "        test_dataloader,\n",
        "        dataset.tokenizer,\n",
        "        num_labels,\n",
        "        label2id,\n",
        "        dataset.id2label(label2id),\n",
        "    )\n",
        "    model.create_model()\n",
        "    model.model.load_state_dict(torch.load(model_path))\n",
        "    model.model.eval()\n",
        "    # Query the model\n",
        "    res = model.query_slots(query)\n",
        "    print(res)\n",
        "\n",
        "\n",
        "def test_span_to_slot_value_mapping():\n",
        "    # Create the dataset\n",
        "    dataset = SlotFillingDataset()\n",
        "    # Load the dataset\n",
        "    dataset.load()\n",
        "    # Filter the dataset\n",
        "    dataset.get_relevant_data({\"restaurant\", \"hotel\"})\n",
        "    # Create the labelled training data\n",
        "    labelled_data = dataset.create_labelled_data(dataset.train_dataset)\n",
        "    # Create the labelled validation data\n",
        "    labelled_data_val = dataset.create_labelled_data(dataset.val_dataset)\n",
        "    # Create the labelled test data\n",
        "    labelled_data_test = dataset.create_labelled_data(dataset.test_dataset)\n",
        "    # Create the label2id mapping\n",
        "    label2id, num_labels = dataset.create_label2id(labelled_data)\n",
        "    # get the slot spans and values\n",
        "    parser = SlotValueParser()\n",
        "    # Get the unique slots\n",
        "    slot_spans = parser.extract_slot_spans(dataset.train_dataset)\n",
        "    # Get the mismatched slot values\n",
        "    mismatched_slot_values = parser.get_mismatched_slot_values(\n",
        "        slot_spans\n",
        "    )  # get the mismatched slot values\n",
        "\n",
        "    fix_count = 0\n",
        "    total_mismatched_slot_values = len(mismatched_slot_values)\n",
        "\n",
        "    for slot_name, slot_value, actual_span in mismatched_slot_values.values:\n",
        "        # convert the actual span to the slot value\n",
        "        fixed_span = parser.convert_span_to_slot_value(slot_name, actual_span)\n",
        "        if slot_value == fixed_span:\n",
        "            print(\n",
        "                f\"Corrected slot value for '{slot_name}': '{actual_span}' -> '{fixed_span}'\"\n",
        "            )\n",
        "            fix_count += 1\n",
        "    print(f\"Fixed {fix_count}/{total_mismatched_slot_values} slot values.\")\n",
        "\n",
        "\n",
        "def test_full_dialogue_system(\n",
        "    path_to_model_checkpoint=\"checkpoint_epoch_3.pt\", print_output=False\n",
        "):\n",
        "    # Create dialogues dataset'\n",
        "    dataset = SlotFillingDataset()\n",
        "    # Load the dataset\n",
        "    dataset.load()\n",
        "    # Filter the dataset\n",
        "    dataset.get_relevant_data({\"restaurant\", \"hotel\"})\n",
        "    # Create the labelled training data\n",
        "    labelled_data = dataset.create_labelled_data(dataset.train_dataset)\n",
        "    # Create the labelled validation data\n",
        "    labelled_data_val = dataset.create_labelled_data(dataset.val_dataset)\n",
        "    # Create the labelled test data\n",
        "    labelled_data_test = dataset.create_labelled_data(dataset.test_dataset)\n",
        "    # Create the label2id mapping\n",
        "    label2id, num_labels = dataset.create_label2id(labelled_data)\n",
        "    # Create the TensorDataset for training\n",
        "    tensor_dataset_train = TensorDataset(labelled_data, label2id)\n",
        "    tensor_dataset_train.create_tensors()\n",
        "    # Create the TensorDataset for validation\n",
        "    tensor_dataset_val = TensorDataset(labelled_data_val, label2id)\n",
        "    tensor_dataset_val.create_tensors()\n",
        "    # Create the TensorDataset for test\n",
        "    tensor_dataset_test = TensorDataset(labelled_data_test, label2id)\n",
        "    tensor_dataset_test.create_tensors()\n",
        "    # Create the dataloader for training\n",
        "    train_dataloader = tensor_dataset_train.create_dataloader()\n",
        "    # Create the dataloader for validation\n",
        "    val_dataloader = tensor_dataset_val.create_dataloader()\n",
        "    # Create the dataloader for test\n",
        "    test_dataloader = tensor_dataset_test.create_dataloader()\n",
        "    # create model from checkpoint\n",
        "    model = SlotFillingModel(\n",
        "        train_dataloader,\n",
        "        val_dataloader,\n",
        "        test_dataloader,\n",
        "        dataset.tokenizer,\n",
        "        num_labels,\n",
        "        label2id,\n",
        "        dataset.id2label(label2id),\n",
        "    )\n",
        "    model.create_model()\n",
        "    model.model.load_state_dict(torch.load(path_to_model_checkpoint))\n",
        "    model.model.eval()\n",
        "\n",
        "    # create labeled dialogue data\n",
        "    labelled_dialogue_data = dataset.create_labelled_dialogue_data(dataset.test_dataset)\n",
        "\n",
        "    # query the model for each dialogue\n",
        "\n",
        "    num_slots_correct = 0\n",
        "    num_slots_total = 0\n",
        "\n",
        "    num_labels_correct = 0\n",
        "    num_labels_total = 0\n",
        "\n",
        "    num_slots_filled_correct = 0\n",
        "    num_slots_filled_total = 0\n",
        "\n",
        "    for dialogue in labelled_dialogue_data:\n",
        "        dialogue_slot_memory = DialogSlotMemory()\n",
        "        parser = SlotValueParser()\n",
        "        for utterance, gt_slot_values in dialogue:\n",
        "            predicted_slots = model.query_slots(utterance)\n",
        "\n",
        "            for slot_name, predicted_value in predicted_slots.items():\n",
        "                # Convert the predicted slot value\n",
        "                predicted_value = parser.convert_span_to_slot_value(\n",
        "                    slot_name, predicted_value, dialogue_slot_memory\n",
        "                )\n",
        "\n",
        "                # Update the predicted slots dictionary with the converted value\n",
        "                predicted_slots[slot_name] = predicted_value\n",
        "\n",
        "                # Update dialogue slot memory\n",
        "                dialogue_slot_memory.add_slot(slot_name, predicted_value)\n",
        "\n",
        "            # Compare the dictionaries\n",
        "            num_slots_total += len(gt_slot_values)\n",
        "            num_slots_filled_total += len(predicted_slots)\n",
        "\n",
        "            for slot_name, gt_value in gt_slot_values.items():\n",
        "                if slot_name in predicted_slots:\n",
        "                    num_slots_correct += 1\n",
        "                    if predicted_slots[slot_name] == gt_value:\n",
        "                        num_slots_filled_correct += 1\n",
        "                    num_labels_total += 1\n",
        "                    if gt_value == predicted_slots[slot_name]:\n",
        "                        num_labels_correct += 1\n",
        "\n",
        "            if print_output:\n",
        "                print(f\"Utterance: '{utterance}'\")\n",
        "                print(f\"Predicted Slots: {predicted_slots}\")\n",
        "                print(f\"Ground Truth Slots: {gt_slot_values}\")\n",
        "                print(\"-\" * 50)\n",
        "\n",
        "    # Number of slots that were identified correctly\n",
        "    print(f\"Slot Accuracy: {num_slots_correct/num_slots_total}\")\n",
        "    # Fraction of values that were parsed correctly\n",
        "    print(f\"Value Accuracy: {num_labels_correct/num_labels_total}\")\n",
        "    # Fraction of slots that were filled correctly\n",
        "    print(f\"Slot/Value Accuracy: {num_slots_filled_correct/num_slots_filled_total}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation of our Agent-Move Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "def relabel_dialogue_act(dialogue_act: str):\n",
        "    if dialogue_act.split('-')[0].upper() not in ['RESTAURANT', 'HOTEL', 'BOOKING', 'GENERAL']:\n",
        "        new_dialog_act = dialogue_act.split('-')[0]\n",
        "    else:\n",
        "        new_dialog_act = dialogue_act\n",
        "\n",
        "    return new_dialog_act\n",
        "\n",
        "def add_dialogue_items_to_dialogue_history_for_agent_move(utterance: str, speaker: str, dialog_acts: typing.List[str], to_be_retrieved_gt: typing.List[str], id_dialog: int, order_in_dialog: int, previous_dialog_history_ids: typing.List[DialogItemIdentifier], dialog_history: typing.List[DialogItem]):\n",
        "    dialogue_act_relabeled = []\n",
        "    for j in range(len(dialog_acts)):\n",
        "        dialogue_act_relabeled.append(relabel_dialogue_act(dialog_acts[j]))\n",
        "\n",
        "    # Create a DialogItem object for this turn\n",
        "    dialog_item = DialogItemForAgentMove(\n",
        "        id_dialog=id_dialog,\n",
        "        order_in_dialog=order_in_dialog,\n",
        "        utterance=utterance,\n",
        "        speaker=speaker,\n",
        "        dialogue_acts=dialogue_act_relabeled,\n",
        "        to_be_retrieved=to_be_retrieved_gt,\n",
        "        gt_dialogue_acts=[],\n",
        "        previous_dialog_items=previous_dialog_history_ids\n",
        "    )\n",
        "\n",
        "    # Append the DialogItem object to the list of DialogItem objects\n",
        "    dialog_history.append(dialog_item)\n",
        "\n",
        "    # Append the DialogItemIdentifier object to the history of the user and agent\n",
        "    dialog_item_identifier = DialogItemIdentifier(\n",
        "        id_dialog=id_dialog,\n",
        "        order_in_dialog=order_in_dialog\n",
        "    )\n",
        "    previous_dialog_history_ids.append(dialog_item_identifier)\n",
        "\n",
        "    # If the history of the user and agent is longer than the past history length multiplied by two, which guarantees that this wont fail on the conversion and speeds up the process\n",
        "    # , remove the oldest turn\n",
        "    if len(previous_dialog_history_ids) > AgentToBeRetrievedModel.PAST_HISTORY_LENGTH*2:\n",
        "        previous_dialog_history_ids.pop(0)\n",
        "\n",
        "class DialogItemForAgentMove(BaseModel):\n",
        "    id_dialog: str\n",
        "    order_in_dialog: int\n",
        "    utterance: str\n",
        "    speaker: str\n",
        "    dialogue_acts: typing.List[str]\n",
        "    to_be_retrieved: typing.List[str]\n",
        "    previous_dialog_items: typing.List[DialogItemIdentifier] = []\n",
        "\n",
        "class BertBILSTMToBeRetrievedClassifier(nn.Module):\n",
        "    lstm_hidden_size = 256\n",
        "    num_lstm_layers = 2\n",
        "    bert_model_type = 'distilbert-base-uncased'\n",
        "\n",
        "    def __init__(self,num_classes: int):\n",
        "        super(BertBILSTMToBeRetrievedClassifier, self).__init__()\n",
        "        self.bert = DistilBertModel.from_pretrained(BertBILSTMToBeRetrievedClassifier.bert_model_type)\n",
        "        self.lstm = nn.LSTM(input_size=768, hidden_size=BertBILSTMToBeRetrievedClassifier.lstm_hidden_size, num_layers=BertBILSTMToBeRetrievedClassifier.num_lstm_layers, batch_first=True, bidirectional=True)\n",
        "        # Corrected linear layer to connect to LSTM output\n",
        "        self.fc = nn.Linear(BertBILSTMToBeRetrievedClassifier.lstm_hidden_size * 2, num_classes) # Multiplied by two because of the bidirectional LSTM\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        bert_output = self.bert(input_ids, attention_mask=attention_mask)\n",
        "        lstm_output, _ = self.lstm(bert_output.last_hidden_state)\n",
        "\n",
        "        # Weighted pooling\n",
        "        seq_len = lstm_output.shape[1]\n",
        "        weights = torch.linspace(1, 2, seq_len).unsqueeze(0).unsqueeze(2).to(input_ids.device)\n",
        "        weighted_lstm_output = lstm_output * weights\n",
        "        weighted_avg_pool = torch.mean(weighted_lstm_output, dim=1)\n",
        "\n",
        "        logits = torch.sigmoid(self.fc(weighted_avg_pool))\n",
        "        return logits\n",
        "\n",
        "class AgentToBeRetrievedModel:\n",
        "    PAST_HISTORY_LENGTH = 1\n",
        "    bert_model_type = 'distilbert-base-uncased'\n",
        "\n",
        "    def __init__(self, model_path=\"\", mlb_path=\"\"):\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.tokenizer = DistilBertTokenizer.from_pretrained(AgentToBeRetrievedModel.bert_model_type)\n",
        "        self.mlb: MultiLabelBinarizer = pickle.load(open(mlb_path, 'rb'))\n",
        "        # Load the best model\n",
        "        saved_model = BertBILSTMToBeRetrievedClassifier(num_classes=len(self.mlb.classes_)).to(self.device)\n",
        "        best_model = torch.load(model_path, map_location=torch.device(self.device))\n",
        "        saved_model.load_state_dict(best_model['model_state_dict'])\n",
        "\n",
        "        self.model = saved_model\n",
        "\n",
        "    def convert_dialogitem_encoded_history(self, dialog_item: DialogItemForAgentMove, dialog_item_dataset: typing.List[DialogItemForAgentMove]):\n",
        "        \"\"\"\n",
        "        Converts a DialogItem object into an Encoded History string.\n",
        "\n",
        "        Parameters:\n",
        "        - dialog_item: DialogItem object.\n",
        "        - dialog_item_dataset: List of DialogItem objects.\n",
        "        - past_history_length: Length of the past history to consider.\n",
        "\n",
        "\n",
        "        Returns:\n",
        "        - Encoded History string.\n",
        "        \"\"\"\n",
        "        encoded_history = \"\"\n",
        "\n",
        "        # Initialize the history of the user and agent as lists with empty DialogItem objects\n",
        "        agent_history: typing.List[DialogItemForAgentMove] = []\n",
        "        user_history: typing.List[DialogItemForAgentMove] = []\n",
        "        \n",
        "        # For each identifier of the previous dialog items, search in the dialog_item_dataset for the corresponding DialogItem object\n",
        "        # and append it to the agent or user history. If no DialogItem object is found, append a dialog item with empty utterance \"\" and dialogue acts []\n",
        "        for i in range(len(dialog_item.previous_dialog_items)):\n",
        "            for j in range(len(dialog_item_dataset)):\n",
        "                if dialog_item_dataset[j].id_dialog == dialog_item.previous_dialog_items[i].id_dialog and dialog_item_dataset[j].order_in_dialog == dialog_item.previous_dialog_items[i].order_in_dialog:\n",
        "                    if dialog_item_dataset[j].speaker == \"Agent\":\n",
        "                        agent_history.append(dialog_item_dataset[j])\n",
        "                        # If the agent history is longer than the past history length, remove the oldest turn\n",
        "                        if len(agent_history) > AgentToBeRetrievedModel.PAST_HISTORY_LENGTH:\n",
        "                            agent_history.pop(0)\n",
        "                    elif dialog_item_dataset[j].speaker == \"User\":\n",
        "                        user_history.append(dialog_item_dataset[j])\n",
        "                        # If the user history is longer than the past history length, remove the oldest turn\n",
        "                        if len(user_history) > AgentToBeRetrievedModel.PAST_HISTORY_LENGTH:\n",
        "                            user_history.pop(0)\n",
        "\n",
        "        for j in range(len(user_history)):\n",
        "            encoded_history += \">\".join([user_history[j].utterance, \"_\".join(user_history[j].dialogue_acts)]) + \"|\"\n",
        "\n",
        "        for j in range(len(agent_history)):\n",
        "            encoded_history += \">\".join([agent_history[j].utterance, \"_\".join(agent_history[j].dialogue_acts), \"_\".join(agent_history[j].to_be_retrieved)]) + \"|\"\n",
        "                \n",
        "        # Get the last user utterance and dialogue acts\n",
        "        last_user_utterance = dialog_item.utterance\n",
        "        last_user_dialogue_act = dialog_item.dialogue_acts\n",
        "        encoded_history = encoded_history + \"_\".join(last_user_dialogue_act) + \">\" + last_user_utterance\n",
        "        \n",
        "        return encoded_history\n",
        "    \n",
        "    def predict(self, encoded_history):\n",
        "        \"\"\"\n",
        "        Given an encoded history, predicts the dialogue acts of the last turn.\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "\n",
        "        # Separate the history from the current utterance splitting by the last \"|\" character, but don't remove it\n",
        "        history, utterance = encoded_history.rsplit(\">\", 1)\n",
        "        \n",
        "        # Merge the history and sentence into a single string adding a [SEP] token between them\n",
        "        encoded_history = \"\".join(history) + \" [SEP] \" + utterance\n",
        "        \n",
        "        encoded = self.tokenizer.encode_plus(\n",
        "            encoded_history, \n",
        "            add_special_tokens=True, \n",
        "            max_length=256, \n",
        "            padding='max_length', \n",
        "            truncation=True, \n",
        "            return_attention_mask=True)\n",
        "        \n",
        "        input_ids = torch.tensor([encoded['input_ids']], dtype=torch.long).to(self.device)\n",
        "        attention_mask = torch.tensor([encoded['attention_mask']], dtype=torch.long).to(self.device)\n",
        "        \n",
        "        # Make a prediction\n",
        "        with torch.no_grad():\n",
        "            logits = self.model(input_ids, attention_mask)\n",
        "        \n",
        "        logits_cpu = logits.to('cpu')\n",
        "        return logits_cpu.numpy()\n",
        "\n",
        "    def predict_only_last_dialog_item(self, dialog_item_dataset: typing.List[DialogItem]) -> typing.List[str]:\n",
        "        \"\"\"\n",
        "        Predicts the dialogue act of last User turn in the dialog. For the Agent, the DAs are filled using the ground truth from the pre-processing function and it arrives here filled, since\n",
        "        we know what dialog acts the agent is performing. For previous User DAs, the ground truth is also used (allowed by professors)\n",
        "\n",
        "        Parameters:\n",
        "        - dialog_item_dataset: List of DialogItem objects.\n",
        "\n",
        "\n",
        "        Returns:\n",
        "        - List of predicted dialogue acts\n",
        "        \"\"\"\n",
        "\n",
        "        dataset_copy = dialog_item_dataset.copy()\n",
        "\n",
        "\n",
        "        # If the speaker is the User, predict the dialogue act\n",
        "        encoded_history = self.convert_dialogitem_encoded_history(dataset_copy[-1], dataset_copy)\n",
        "        preds = self.predict(encoded_history)\n",
        "        threshold = 0.5\n",
        "        all_preds_binary = []\n",
        "        for all_pred in preds:\n",
        "            local_pred = []\n",
        "            for old_local_pred in all_pred:\n",
        "                binary_local_pred = (old_local_pred > threshold).astype(int)\n",
        "                local_pred.append(binary_local_pred)\n",
        "            all_preds_binary.append(local_pred)\n",
        "        labels_preds = self.mlb.inverse_transform(np.array(all_preds_binary))\n",
        "        dialogue_acts = labels_preds[0]\n",
        "\n",
        "        return dialogue_acts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 -  DA for Agent-Move"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DAAgentMoveClassifier(nn.Module):\n",
        "    bert_model_type  = 'distilbert-base-uncased'\n",
        "\n",
        "    def __init__(self, num_classes: int):\n",
        "        super(DAAgentMoveClassifier, self).__init__()\n",
        "        self.bert = DistilBertModel.from_pretrained(DAAgentMoveClassifier.bert_model_type)\n",
        "        self.fc = nn.Linear(768, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
        "        cls_output = outputs.last_hidden_state[:, 0, :] # selects the [CLS] token position.\n",
        "        logits = torch.sigmoid(self.fc(cls_output))\n",
        "        return logits\n",
        "    \n",
        "class AgentDAModel:\n",
        "    PAST_HISTORY_LENGTH = 1\n",
        "    bert_model_type = 'distilbert-base-uncased'\n",
        "\n",
        "    def __init__(self, model_path=\"\", mlb_path=\"\"):\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.tokenizer = DistilBertTokenizer.from_pretrained(AgentDAModel.bert_model_type)\n",
        "        self.mlb: MultiLabelBinarizer = pickle.load(open(mlb_path, 'rb'))\n",
        "        # Load the best model\n",
        "        saved_model = DAAgentMoveClassifier(num_classes=len(self.mlb.classes_)).to(self.device)\n",
        "        best_model = torch.load(model_path, map_location=torch.device(self.device))\n",
        "        saved_model.load_state_dict(best_model['model_state_dict'])\n",
        "\n",
        "        self.model = saved_model\n",
        "\n",
        "    def convert_dialogitem_encoded_history(self, dialog_item: DialogItemForAgentMove, dialog_item_dataset: typing.List[DialogItemForAgentMove], to_be_provided_gt: typing.List[str]):\n",
        "        \"\"\"\n",
        "        Converts a DialogItem object into an Encoded History string.\n",
        "\n",
        "        Parameters:\n",
        "        - dialog_item: DialogItem object.\n",
        "        - dialog_item_dataset: List of DialogItem objects.\n",
        "        - past_history_length: Length of the past history to consider.\n",
        "\n",
        "\n",
        "        Returns:\n",
        "        - Encoded History string.\n",
        "        \"\"\"\n",
        "        encoded_history = \"\"\n",
        "\n",
        "        # Initialize the history of the user and agent as lists with empty DialogItem objects\n",
        "        agent_history: typing.List[DialogItemForAgentMove] = []\n",
        "        user_history: typing.List[DialogItemForAgentMove] = []\n",
        "        \n",
        "        # For each identifier of the previous dialog items, search in the dialog_item_dataset for the corresponding DialogItem object\n",
        "        # and append it to the agent or user history. If no DialogItem object is found, append a dialog item with empty utterance \"\" and dialogue acts []\n",
        "        for i in range(len(dialog_item.previous_dialog_items)):\n",
        "            for j in range(len(dialog_item_dataset)):\n",
        "                if dialog_item_dataset[j].id_dialog == dialog_item.previous_dialog_items[i].id_dialog and dialog_item_dataset[j].order_in_dialog == dialog_item.previous_dialog_items[i].order_in_dialog:\n",
        "                    if dialog_item_dataset[j].speaker == \"Agent\":\n",
        "                        agent_history.append(dialog_item_dataset[j])\n",
        "                        # If the agent history is longer than the past history length, remove the oldest turn\n",
        "                        if len(agent_history) > AgentToBeRetrievedModel.PAST_HISTORY_LENGTH:\n",
        "                            agent_history.pop(0)\n",
        "                    elif dialog_item_dataset[j].speaker == \"User\":\n",
        "                        user_history.append(dialog_item_dataset[j])\n",
        "                        # If the user history is longer than the past history length, remove the oldest turn\n",
        "                        if len(user_history) > AgentToBeRetrievedModel.PAST_HISTORY_LENGTH:\n",
        "                            user_history.pop(0)\n",
        "\n",
        "        # Get the last user utterance and dialogue acts\n",
        "        last_user_utterance = dialog_item.utterance\n",
        "        last_user_dialogue_act = dialog_item.dialogue_acts\n",
        "        encoded_history = encoded_history + last_user_utterance + \">\" + \"_\".join(last_user_dialogue_act) + \"|\"\n",
        "        for j in range(len(user_history)):\n",
        "            encoded_history += \">\".join([user_history[j].utterance, \"_\".join(user_history[j].dialogue_acts)]) + \"|\"\n",
        "        for j in range(len(agent_history)):\n",
        "            encoded_history += \">\".join([agent_history[j].utterance, \"_\".join(agent_history[j].dialogue_acts)]) + \"|\"\n",
        "\n",
        "        # Add the to be provided ground truth to the encoded history for this turn\n",
        "        encoded_history = encoded_history + \"_\".join(to_be_provided_gt) \n",
        "\n",
        "        \n",
        "        return encoded_history\n",
        "    \n",
        "    def predict(self, encoded_history):\n",
        "        \"\"\"\n",
        "        Given an encoded history, predicts the dialogue acts of the last turn.\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "\n",
        "        # Separate the history from the current utterance splitting by the last \"|\" character, but don't remove it\n",
        "        history, utterance = encoded_history.rsplit(\">\", 1)\n",
        "        \n",
        "        # Merge the history and sentence into a single string adding a [SEP] token between them\n",
        "        encoded_history = \"\".join(history) + \" [SEP] \" + utterance\n",
        "        \n",
        "        encoded = self.tokenizer.encode_plus(\n",
        "            encoded_history, \n",
        "            add_special_tokens=True, \n",
        "            max_length=256, \n",
        "            padding='max_length', \n",
        "            truncation=True, \n",
        "            return_attention_mask=True)\n",
        "        \n",
        "        input_ids = torch.tensor([encoded['input_ids']], dtype=torch.long).to(self.device)\n",
        "        attention_mask = torch.tensor([encoded['attention_mask']], dtype=torch.long).to(self.device)\n",
        "        \n",
        "        # Make a prediction\n",
        "        with torch.no_grad():\n",
        "            logits = self.model(input_ids, attention_mask)\n",
        "        \n",
        "        logits_cpu = logits.to('cpu')\n",
        "        return logits_cpu.numpy()\n",
        "\n",
        "    def predict_only_last_dialog_item(self, dialog_item_dataset: typing.List[DialogItem], to_be_provided_gt: typing.List[str]) -> typing.List[str]:\n",
        "        \"\"\"\n",
        "        Predicts the dialogue act of last User turn in the dialog. For the Agent, the DAs are filled using the ground truth from the pre-processing function and it arrives here filled, since\n",
        "        we know what dialog acts the agent is performing. For previous User DAs, the ground truth is also used (allowed by professors)\n",
        "\n",
        "        Parameters:\n",
        "        - dialog_item_dataset: List of DialogItem objects.\n",
        "\n",
        "\n",
        "        Returns:\n",
        "        - List of predicted dialogue acts\n",
        "        \"\"\"\n",
        "\n",
        "        dataset_copy = dialog_item_dataset.copy()\n",
        "\n",
        "\n",
        "        # If the speaker is the User, predict the dialogue act\n",
        "        encoded_history = self.convert_dialogitem_encoded_history(dataset_copy[-1], dataset_copy, to_be_provided_gt)\n",
        "        preds = self.predict(encoded_history)\n",
        "        threshold = 0.5\n",
        "        all_preds_binary = []\n",
        "        for all_pred in preds:\n",
        "            local_pred = []\n",
        "            for old_local_pred in all_pred:\n",
        "                binary_local_pred = (old_local_pred > threshold).astype(int)\n",
        "                local_pred.append(binary_local_pred)\n",
        "            all_preds_binary.append(local_pred)\n",
        "        labels_preds = self.mlb.inverse_transform(np.array(all_preds_binary))\n",
        "        dialogue_acts = labels_preds[0]\n",
        "\n",
        "        return dialogue_acts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 - To be Provided"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ToBeProvided:\n",
        "    \n",
        "    # Load the dataset\n",
        "    def load_dataset():\n",
        "        dataset = load_dataset(\"multi_woz_v22\")\n",
        "        train_data = dataset['train']\n",
        "        val_data = dataset['validation']\n",
        "        test_data = dataset['test']\n",
        "        return train_data, val_data, test_data\n",
        "    \n",
        "    # Get the relevant data\n",
        "    def filterDomains(data):\n",
        "        return [entry for entry in data if set(entry[\"services\"]).issubset({\"restaurant\", \"hotel\", \"booking\"})]\n",
        "    \n",
        "    # Add the data to be retrieved\n",
        "    def add_data_to_be_retrieved(dataset, print_dialogue=False):\n",
        "        \"\"\"\n",
        "        Augment the dataset with the following information:\n",
        "        - Information to be retrieved (ground truth)\n",
        "        \n",
        "        Heavily inspired by the code from the evaluation script.\n",
        "        \"\"\"\n",
        "        \n",
        "        for dialogue in dataset:\n",
        "            turns = dialogue[\"turns\"]\n",
        "            turns[\"to_be_retrieved_ground_truth\"] = {turn_id: [] for turn_id in range(len(turns[\"turn_id\"]))}\n",
        "            \n",
        "            \n",
        "            for turn_id, _ in enumerate(turns[\"turn_id\"]):\n",
        "                # If it is SYSTEM turn:\n",
        "                if turns[\"speaker\"][turn_id]:\n",
        "                    slot_names_per_act = [slot['slot_name'] for slot in turns['dialogue_acts'][turn_id]['dialog_act']['act_slots']]\n",
        "                    slot_values_per_act = [slot['slot_value'] for slot in turns['dialogue_acts'][turn_id]['dialog_act']['act_slots']]\n",
        "                    dialogue_acts = turns['dialogue_acts'][turn_id]['dialog_act']['act_type']\n",
        "                    services = turns['frames'][turn_id]['service']\n",
        "                    current_booking_service = [service for service in services if service in [\"hotel\", \"restaurant\"]]\n",
        "\n",
        "                    to_be_retrieved_ground_truth = []\n",
        "                    for act_i in range(len(slot_names_per_act)):\n",
        "                        domain = dialogue_acts[act_i].split(\"-\")[0].lower()\n",
        "                        if domain == \"booking\" and len(current_booking_service) ==1:\n",
        "                            domain = current_booking_service[0]\n",
        "                        slot_names = [domain+\"-\"+slot_names_per_act[act_i][slot_i] for slot_i in range(len(slot_names_per_act[act_i]))\n",
        "                                        if slot_values_per_act[act_i][slot_i]!=\"?\" and slot_names_per_act[act_i][slot_i]!=\"none\"]\n",
        "                        if slot_names:\n",
        "                            to_be_retrieved_slot_names = [\"%s-availability\" % (domain)] + slot_names\n",
        "                            while domain+\"-choice\" in to_be_retrieved_slot_names:\n",
        "                                del to_be_retrieved_slot_names[to_be_retrieved_slot_names.index(domain+\"-choice\")]\n",
        "                            to_be_retrieved_ground_truth.extend(to_be_retrieved_slot_names)\n",
        "                    to_be_retrieved_ground_truth = sorted(list(set(to_be_retrieved_ground_truth)))\n",
        "                    \n",
        "                    # augment the dataset\n",
        "                    turns[\"to_be_retrieved_ground_truth\"][turn_id].extend(to_be_retrieved_ground_truth)\n",
        "                    \n",
        "                    if print_dialogue:\n",
        "                        print(f\"Utterance: {turns['utterance'][turn_id]}\")\n",
        "                        print(f\"To be retrieved: {to_be_retrieved_ground_truth}\")\n",
        "            if print_dialogue:        \n",
        "                print(\"-\"*50)\n",
        "    \n",
        "    # Add the data to be provided\n",
        "    def add_data_to_be_provided(dataset):\n",
        "        \"\"\"\n",
        "        Augment the dataset with the following information:\n",
        "        - Information to be provided (ground truth)\n",
        "        \n",
        "        Heavily inspired by the code from the evaluation script.\n",
        "        \"\"\"\n",
        "        for dialogue in dataset:\n",
        "            turns = dialogue[\"turns\"]\n",
        "            turns[\"to_be_provided_overall\"] = {turn_id: [] for turn_id in range(len(turns[\"turn_id\"]))}\n",
        "            \n",
        "            \n",
        "            for turn_id, _ in enumerate(turns[\"turn_id\"]):\n",
        "                # If it is SYSTEM turn:\n",
        "                if turns[\"speaker\"][turn_id]:\n",
        "                    slot_names_per_act = [slot['slot_name'] for slot in turns['dialogue_acts'][turn_id]['dialog_act']['act_slots']]\n",
        "                    slot_values_per_act = [slot['slot_value'] for slot in turns['dialogue_acts'][turn_id]['dialog_act']['act_slots']]\n",
        "                    dialogue_acts = turns['dialogue_acts'][turn_id]['dialog_act']['act_type']\n",
        "                    services = turns['frames'][turn_id]['service']\n",
        "                    current_booking_service = [service for service in services if service in [\"hotel\", \"restaurant\"]]\n",
        "                    to_be_provided_overall = []\n",
        "                    \n",
        "                    for act_i in range(len(slot_names_per_act)):\n",
        "                        domain = dialogue_acts[act_i].split(\"-\")[0].lower()\n",
        "                        if domain == \"booking\" and len(current_booking_service)==1:\n",
        "                            domain = current_booking_service[0]\n",
        "                        if domain in [\"hotel\", \"restaurant\", \"booking\", \"general\"]:\n",
        "                            slot_names_vlues = [domain+\"-\"+slot_names_per_act[act_i][slot_i]+\":\"+slot_values_per_act[act_i][slot_i] for slot_i in range(len(slot_names_per_act[act_i]))\n",
        "                                                if slot_values_per_act[act_i][slot_i]!=\"?\" and slot_names_per_act[act_i][slot_i]!=\"none\"]\n",
        "                            if slot_names_vlues and any((slot_name_value.split(\":\")[0]!=domain+\"-none\" for slot_name_value in slot_names_vlues)) and not \"-No\" in dialogue_acts[act_i]:\n",
        "                                to_be_provided = [\"%s-availability:yes\" % (domain)] + slot_names_vlues\n",
        "                                to_be_provided_overall.extend(to_be_provided)\n",
        "                            elif \"-No\" in dialogue_acts[act_i]:\n",
        "                                to_be_provided = [\"%s-availability:no\" % (domain)] + slot_names_vlues\n",
        "                                to_be_provided_overall.extend(to_be_provided)\n",
        "                    to_be_provided_overall = sorted(list(set(to_be_provided_overall)))\n",
        "                    remove_avail_no_list = [elem for elem in to_be_provided_overall if elem.endswith(\"availability:no\")]\n",
        "                    for remove_avail in remove_avail_no_list:\n",
        "                        remove_avail_yes = remove_avail[:-2]+\"yes\"\n",
        "                        while remove_avail_yes in to_be_provided_overall:\n",
        "                            del to_be_provided_overall[to_be_provided_overall.index(remove_avail_yes)]\n",
        "                    turns[\"to_be_provided_overall\"][turn_id].extend(to_be_provided_overall)\n",
        "                    \n",
        "    def extract_relevant_data(dataset):\n",
        "        \"\"\"\n",
        "        Create a dataset that can be used for using the what shall be requested model.\n",
        "        \"\"\"\n",
        "        \n",
        "        user_dialogue_acts = []\n",
        "        extracted_information = []\n",
        "        retrieved_information = []\n",
        "        information_to_be_requested = []\n",
        "        \n",
        "        for dialogue in dataset:\n",
        "            turns = dialogue[\"turns\"]\n",
        "            \n",
        "            for turn_id, _ in enumerate(turns[\"turn_id\"]):\n",
        "                \n",
        "                # if it is the USER turn:\n",
        "                if not turns[\"speaker\"][turn_id]:\n",
        "                    user_dialogue_acts.append(turns['dialogue_acts'][turn_id]['dialog_act']['act_type'])\n",
        "                    slot_names_per_act = [slot['slot_name'] for slot in turns['dialogue_acts'][turn_id]['dialog_act']['act_slots']]\n",
        "                    slot_values_per_act = [slot['slot_value'] for slot in turns['dialogue_acts'][turn_id]['dialog_act']['act_slots']]\n",
        "                    current_slots = []\n",
        "                    for act_i in range(len(slot_names_per_act)):\n",
        "                        for  slot_name, slot_value in zip(slot_names_per_act[act_i], slot_values_per_act[act_i]):\n",
        "                            if slot_name != \"none\":\n",
        "                                current_slots.append(slot_name+\":\"+slot_value)\n",
        "                    extracted_information.append(current_slots)\n",
        "                    \n",
        "                \n",
        "                # If it is SYSTEM turn:\n",
        "                if turns[\"speaker\"][turn_id]:\n",
        "                    \n",
        "                    retrieved_information.append(turns['to_be_provided_overall'][turn_id])\n",
        "                    # get the slot names with '?' as value\n",
        "                    agent_dialogue_acts = turns['dialogue_acts'][turn_id]['dialog_act']['act_type']\n",
        "                    slot_names_per_act = [slot['slot_name'] for slot in turns['dialogue_acts'][turn_id]['dialog_act']['act_slots']]\n",
        "                    slot_values_per_act = [slot['slot_value'] for slot in turns['dialogue_acts'][turn_id]['dialog_act']['act_slots']]\n",
        "                    slots_to_be_requested = []\n",
        "                    for act_i in range(len(slot_names_per_act)):\n",
        "                        for slot_name, slot_value in zip(slot_names_per_act[act_i], slot_values_per_act[act_i]):\n",
        "                            if slot_value == \"?\":\n",
        "                                prefix =  agent_dialogue_acts[act_i-1].split(\"-\")[0].lower() + \"-\"\n",
        "                                slots_to_be_requested.append(prefix+slot_name)\n",
        "                    information_to_be_requested.append(slots_to_be_requested)\n",
        "        \n",
        "        model_dataset = {\n",
        "            'user_dialogue_acts': user_dialogue_acts,\n",
        "            'extracted_information': extracted_information,\n",
        "            'retrieved_information': retrieved_information,\n",
        "            'information_to_be_requested': information_to_be_requested\n",
        "        }\n",
        "        return model_dataset\n",
        "        \n",
        "    \n",
        "    def create_x_y(dataset):\n",
        "        \"\"\"\n",
        "        Create the x and y for the model.\n",
        "        \"\"\"\n",
        "        x = []\n",
        "        y = []\n",
        "        for i in range(len(dataset['user_dialogue_acts'])):\n",
        "            x.append(dataset['user_dialogue_acts'][i] + dataset['extracted_information'][i] + dataset['retrieved_information'][i])\n",
        "            y.append(dataset['information_to_be_requested'][i]) if dataset['information_to_be_requested'][i] else y.append([\"none\"])\n",
        "        return x, y\n",
        "    \n",
        "    \n",
        "    \n",
        "    def undersample_none_label(x_data, y_data, undersample_ratio=1.0, random_state=None):\n",
        "        \"\"\"\n",
        "        Undersamples the 'none' label in a multi-label dataset.\n",
        "        \n",
        "        :param x_data: Feature set, list of lists or similar.\n",
        "        :param y_data: Label set, list of lists or lists of sets of labels.\n",
        "        :param undersample_ratio: Ratio of number of 'none' instances to other instances (default: 1.0).\n",
        "        :param random_state: Integer seed for reproducibility (default: None).\n",
        "        :return: Tuple of undersampled (x_data, y_data).\n",
        "        \"\"\"\n",
        "        if random_state is not None:\n",
        "            random.seed(random_state)\n",
        "\n",
        "        # Convert labels to a set for easier manipulation\n",
        "        y_data_sets = [set(labels) for labels in y_data]\n",
        "\n",
        "        # Separate 'none' instances and other instances\n",
        "        none_indices = [i for i, labels in enumerate(y_data_sets) if labels == {'none'}]\n",
        "        other_indices = [i for i, labels in enumerate(y_data_sets) if labels != {'none'}]\n",
        "\n",
        "        # Calculate the number of 'none' instances to keep\n",
        "        num_none_to_keep = int(len(other_indices) * undersample_ratio)\n",
        "\n",
        "        # Randomly undersample 'none' instances\n",
        "        random.shuffle(none_indices)\n",
        "        none_indices = none_indices[:num_none_to_keep]\n",
        "\n",
        "        # Combine back the indices\n",
        "        undersampled_indices = none_indices + other_indices\n",
        "\n",
        "        # Subset the original x_data and y_data\n",
        "        x_data_undersampled = [x_data[i] for i in undersampled_indices]\n",
        "        y_data_undersampled = [y_data[i] for i in undersampled_indices]\n",
        "\n",
        "        return x_data_undersampled, y_data_undersampled\n",
        "    \n",
        "    def prepare_data_for_bert(x_data, y_data, max_length=384):\n",
        "        \"\"\"\n",
        "        Prepare the data for BERT.\n",
        "        \"\"\"\n",
        "        \n",
        "        input_ids = []\n",
        "        attention_masks = []\n",
        "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        \n",
        "        for x in x_data:\n",
        "            x_joined = ' '.join(x)\n",
        "            \n",
        "            # Optional: Check the length after joining\n",
        "            if len(x_joined) > max_length:\n",
        "                print(\"Warning: Truncating input with length %d to max_length %d.\" % (len(x_joined), max_length))\n",
        "            \n",
        "            x_encoded = tokenizer.encode_plus(\n",
        "                x_joined,\n",
        "                max_length=max_length,\n",
        "                add_special_tokens=True,\n",
        "                return_token_type_ids=False,\n",
        "                padding='max_length',\n",
        "                truncation=True,\n",
        "                return_attention_mask=True,\n",
        "                return_tensors='pt',\n",
        "            )\n",
        "            input_ids.append(x_encoded['input_ids'])\n",
        "            attention_masks.append(x_encoded['attention_mask'])\n",
        "            \n",
        "        # Convert lists to tensors\n",
        "        input_ids = torch.cat(input_ids, dim=0)\n",
        "        attention_masks = torch.cat(attention_masks, dim=0)\n",
        "        y_data_tensor = torch.tensor(y_data)\n",
        "        \n",
        "        return input_ids, attention_masks, y_data_tensor\n",
        "    \n",
        "    class CustomDataset(Dataset):\n",
        "        def __init__(self, input_ids, attention_masks, labels):\n",
        "            self.input_ids = input_ids\n",
        "            self.attention_masks = attention_masks\n",
        "            self.labels = labels\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.labels)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            return self.input_ids[idx], self.attention_masks[idx], self.labels[idx]\n",
        "    \n",
        "    class BertForMultiLabelClassification(nn.Module):\n",
        "        def __init__(self, num_labels):\n",
        "            super(ToBeProvided.BertForMultiLabelClassification, self).__init__()\n",
        "            self.num_labels = num_labels\n",
        "            self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "            self.dropout = nn.Dropout(0.1)\n",
        "            self.custom_layer = nn.Linear(self.bert.config.hidden_size, self.bert.config.hidden_size)\n",
        "            self.custom_activation = nn.ReLU()\n",
        "            self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
        "\n",
        "        def forward(self, input_ids, attention_mask):\n",
        "            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            pooled_output = outputs.pooler_output\n",
        "            pooled_output = self.dropout(pooled_output)\n",
        "            custom_output = self.custom_activation(self.custom_layer(pooled_output))\n",
        "            logits = self.classifier(custom_output)\n",
        "            return logits\n",
        "    \n",
        "    \n",
        "    class EarlyStopping:\n",
        "        def __init__(self, patience=3, verbose=False, delta=0):\n",
        "            self.patience = patience\n",
        "            self.verbose = verbose\n",
        "            self.counter = 0\n",
        "            self.best_score = None\n",
        "            self.early_stop = False\n",
        "            self.val_loss_min = np.Inf\n",
        "            self.delta = delta\n",
        "\n",
        "        def __call__(self, val_loss, model):\n",
        "            score = -val_loss\n",
        "\n",
        "            if self.best_score is None:\n",
        "                self.best_score = score\n",
        "                self.save_checkpoint(val_loss, model)\n",
        "            elif score < self.best_score + self.delta:\n",
        "                self.counter += 1\n",
        "                if self.verbose:\n",
        "                    print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "                if self.counter >= self.patience:\n",
        "                    self.early_stop = True\n",
        "            else:\n",
        "                self.best_score = score\n",
        "                self.save_checkpoint(val_loss, model)\n",
        "                self.counter = 0\n",
        "\n",
        "        def save_checkpoint(self, val_loss, model):\n",
        "            if self.verbose:\n",
        "                print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model ...')\n",
        "            torch.save(model.state_dict(), 'checkpoint.pt')\n",
        "            self.val_loss_min = val_loss\n",
        "    \n",
        "    def train_model(model, train_dataloader, val_dataloader, optimizer, criterion, num_epochs, device):\n",
        "        early_stopping = ToBeProvided.EarlyStopping(patience=2, verbose=True)\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            model.train()\n",
        "            total_loss = 0\n",
        "\n",
        "            for batch in train_dataloader:\n",
        "                b_input_ids, b_attention_mask, b_labels = [t.to(device) for t in batch]\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(b_input_ids, b_attention_mask)\n",
        "                loss = criterion(outputs, b_labels.float())\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "            avg_train_loss = total_loss / len(train_dataloader)\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "            # Validation phase\n",
        "            model.eval()\n",
        "            total_val_loss = 0\n",
        "            with torch.no_grad():\n",
        "                for batch in val_dataloader:\n",
        "                    b_input_ids, b_attention_mask, b_labels = [t.to(device) for t in batch]\n",
        "\n",
        "                    outputs = model(b_input_ids, b_attention_mask)\n",
        "                    loss = criterion(outputs, b_labels.float())\n",
        "                    total_val_loss += loss.item()\n",
        "\n",
        "            avg_val_loss = total_val_loss / len(val_dataloader)\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs}, Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "            # Early Stopping\n",
        "            early_stopping(avg_val_loss, model)\n",
        "            if early_stopping.early_stop:\n",
        "                print(\"Early stopping\")\n",
        "                break\n",
        "\n",
        "    def evaluate_model(model, test_dataloader, device, threshold=0.5, path_to_model=None):\n",
        "        if path_to_model:\n",
        "            model.load_state_dict(torch.load(path_to_model))\n",
        "        model.eval()\n",
        "\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for batch in test_dataloader:\n",
        "                b_input_ids, b_attention_mask, b_labels = [t.to(device) for t in batch]\n",
        "\n",
        "                outputs = model(b_input_ids, b_attention_mask)\n",
        "                preds = torch.sigmoid(outputs).cpu().numpy()\n",
        "                labels = b_labels.cpu().numpy()\n",
        "\n",
        "                all_preds.append(preds)\n",
        "                all_labels.append(labels)\n",
        "\n",
        "        # Flatten the outputs and labels lists\n",
        "        all_preds = np.vstack(all_preds)\n",
        "        all_labels = np.vstack(all_labels)\n",
        "\n",
        "        # Apply threshold to convert probabilities to binary predictions\n",
        "        all_preds_binary = (all_preds > threshold).astype(int)\n",
        "\n",
        "        return all_preds_binary, all_labels\n",
        "\n",
        "    def process_input_for_query(user_input, tokenizer, max_length=384):\n",
        "        # Tokenize and encode the input text\n",
        "        encoded_input = tokenizer.encode_plus(\n",
        "            user_input,\n",
        "            max_length=max_length,\n",
        "            add_special_tokens=True,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        return encoded_input['input_ids'], encoded_input['attention_mask']\n",
        "\n",
        "    def load_trained_model(model_path, mlb_path):\n",
        "        # load the MultiLabelBinarizer\n",
        "        mlb = joblib.load(mlb_path)\n",
        "        \n",
        "        # Get the number of labels\n",
        "        num_labels = len(mlb.classes_)\n",
        "        \n",
        "        # Initialize the model structure\n",
        "        model = ToBeProvided.BertForMultiLabelClassification(num_labels=num_labels)\n",
        "        # Load the trained weights\n",
        "        model.load_state_dict(torch.load(model_path))\n",
        "        return model\n",
        "\n",
        "    def query_model(user_input, model, tokenizer, mlb, device, threshold=0.5):\n",
        "        # Process the input\n",
        "        input_ids, attention_mask = ToBeProvided.process_input_for_query(user_input, tokenizer)\n",
        "\n",
        "        # Load the trained model\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "\n",
        "        # Make prediction\n",
        "        with torch.no_grad():\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            logits = model(input_ids, attention_mask)\n",
        "            probabilities = torch.sigmoid(logits).cpu().numpy()\n",
        "\n",
        "        # Apply threshold to get binary predictions\n",
        "        predictions = (probabilities > threshold).astype(int)\n",
        "\n",
        "        # Transform binary predictions back to label names\n",
        "        label_names = mlb.inverse_transform(predictions)\n",
        "\n",
        "        return label_names\n",
        "      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# function used to train and evaluate the ToBeProvided model\n",
        "def train_and_evaluate_to_be_provided_model():\n",
        "    # Get the splits\n",
        "    train_data, val_data, test_data = ToBeProvided.load_dataset()\n",
        "\n",
        "    # Filter the domains\n",
        "    train_data_filtered = ToBeProvided.filterDomains(train_data)\n",
        "    val_data_filtered =   ToBeProvided.filterDomains(val_data)\n",
        "    test_data_filtered =  ToBeProvided.filterDomains(test_data)\n",
        "\n",
        "\n",
        "    # Add the data to be provided and retrieved\n",
        "    ToBeProvided.add_data_to_be_provided(train_data_filtered)\n",
        "    ToBeProvided.add_data_to_be_retrieved(train_data_filtered)\n",
        "\n",
        "    ToBeProvided.add_data_to_be_provided(val_data_filtered)\n",
        "    ToBeProvided.add_data_to_be_retrieved(val_data_filtered)\n",
        "\n",
        "    ToBeProvided.add_data_to_be_provided(test_data_filtered)\n",
        "    ToBeProvided.add_data_to_be_retrieved(test_data_filtered)\n",
        "\n",
        "\n",
        "    # Extract the relevant data\n",
        "    train_dataset = ToBeProvided.extract_relevant_data(train_data_filtered)\n",
        "    val_dataset = ToBeProvided.extract_relevant_data(val_data_filtered)\n",
        "    test_dataset = ToBeProvided.extract_relevant_data(test_data_filtered)\n",
        "\n",
        "    # Create and udernsample the dataset\n",
        "    x_train, y_train = ToBeProvided.create_x_y(train_dataset)\n",
        "    x_val, y_val = ToBeProvided.create_x_y(val_dataset)\n",
        "    x_test, y_test = ToBeProvided.create_x_y(test_dataset)\n",
        "\n",
        "    x_train, y_train = ToBeProvided.undersample_none_label(x_train, y_train, undersample_ratio= 0.5, random_state=42)\n",
        "    x_val, y_val = ToBeProvided.undersample_none_label(x_val, y_val, undersample_ratio= 0.5, random_state=42)\n",
        "    x_test, y_test = ToBeProvided.undersample_none_label(x_test, y_test, undersample_ratio= 0.5, random_state=42)\n",
        "\n",
        "    # create the binarizer\n",
        "    mlb = MultiLabelBinarizer()\n",
        "    y_train = mlb.fit_transform(y_train)\n",
        "    y_val = mlb.transform(y_val)\n",
        "    y_test = mlb.transform(y_test)\n",
        "\n",
        "\n",
        "    # Prepare the data for BERT\n",
        "    train_dataset = ToBeProvided.CustomDataset(*ToBeProvided.prepare_data_for_bert(x_train, y_train))\n",
        "    val_dataset = ToBeProvided.CustomDataset(*ToBeProvided.prepare_data_for_bert(x_val, y_val))\n",
        "    test_dataset = ToBeProvided.CustomDataset(*ToBeProvided.prepare_data_for_bert(x_test, y_test))\n",
        "\n",
        "    # set the batch size\n",
        "    batch_size = 32\n",
        "\n",
        "    # Create the dataloaders\n",
        "    train_dataloader = ToBeProvided.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_dataloader = ToBeProvided.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_dataloader = ToBeProvided.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Create the model\n",
        "    num_labels = len(mlb.classes_)\n",
        "\n",
        "    model = ToBeProvided.BertForMultiLabelClassification(num_labels)\n",
        "\n",
        "\n",
        "\n",
        "    # train and evaluate the model\n",
        "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    ToBeProvided.train_model(model, train_dataloader, val_dataloader, optimizer, criterion, num_epochs=10, device=device)\n",
        "    y_pred, y_true = evaluate_model(model, test_dataloader, device)\n",
        "\n",
        "    label_names = mlb.classes_.tolist()\n",
        "\n",
        "    print(classification_report(y_true, y_pred, target_names=label_names))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "uwPiIPnT4o_T"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Slot Filling Model\n",
        "slot_filling_model = return_model(\"slot_filling_model.pt\")\n",
        "\n",
        "# Dialogue Act Prediction Model\n",
        "dialogue_act_testing_suite = DialogActModel(\n",
        "    mlb_path=\"da_mlb.pkl\", model_path=\"dialog_act_model.pth\", bert_model_type=\"distilbert-base-uncased\"\n",
        ")\n",
        "\n",
        "# Agent to be retrieved model\n",
        "agent_to_be_retrieved_testing_suite = AgentToBeRetrievedModel(\n",
        "    mlb_path=\"mlb3.1.pkl\", model_path=\"best_model3.1.pth\"\n",
        ")\n",
        "\n",
        "# Agent DA model\n",
        "agent_da_testing_suite = AgentDAModel(\n",
        "    mlb_path=\"mlb3.2.pkl\", model_path=\"best_model3.2.pth\"\n",
        ")\n",
        "\n",
        "# To be provided model\n",
        "to_be_provided_testing_suite = ToBeProvided.load_trained_model(\"to_be_provided_model.pt\", \"mlb3-3.pkl\")\n",
        "\n",
        "\n",
        "def Dialogue_Act_Prediction(user_utterance, other_features_from_dialogue_history):\n",
        "    # Make copies so it doenst change the original, which will be changed after prediction with ground truth values\n",
        "    dialogue_history_copy = other_features_from_dialogue_history[\"dialogue_history\"].copy()\n",
        "    previous_dialog_history_ids_copy = other_features_from_dialogue_history[\"previous_dialogue_history_ids\"].copy()\n",
        "\n",
        "    # Add the user utterance to the dialogue history with the necessary features\n",
        "    DialogActModel.add_dialogue_items_to_dialogue_history(\n",
        "        utterance=user_utterance,\n",
        "        speaker=other_features_from_dialogue_history[\"speaker\"],\n",
        "        dialog_acts=[],\n",
        "        id_dialog=other_features_from_dialogue_history[\"id_dialogue\"],\n",
        "        order_in_dialog=other_features_from_dialogue_history[\"turn_id\"],\n",
        "        previous_dialog_history_ids=previous_dialog_history_ids_copy,\n",
        "        dialog_history=dialogue_history_copy,\n",
        "    )\n",
        "\n",
        "    # Predict the dialogue act\n",
        "    dialogue_acts = dialogue_act_testing_suite.predict_only_last_dialog_item(\n",
        "        dialog_item_dataset=dialogue_history_copy,\n",
        "    )\n",
        "    return dialogue_acts\n",
        "\n",
        "def Extract_and_Categorize_Spans(user_utterance, user_dialogue_acts, other_features_from_dialogue_history2={}):\n",
        "    parser = SlotValueParser()\n",
        "    predicted_slots = slot_filling_model.query_slots(user_utterance)\n",
        "    extracted_information = []\n",
        "    for slot_name, predicted_value in predicted_slots.items():\n",
        "        # Convert the predicted slot value\n",
        "        predicted_value = parser.convert_span_to_slot_value(\n",
        "            slot_name, predicted_value, other_features_from_dialogue_history2[\"slot_history\"]\n",
        "        )\n",
        "        extracted_information.append((slot_name, predicted_value))\n",
        "    #extracted_information = [('hotel-bookpeople', '2'), ('hotel-bookstay', '2'), ('hotel-bookday', 'sunday'), ('restaurant-phone', '?')]\n",
        "    return extracted_information\n",
        "\n",
        "def Information_to_be_retrieved_Prediction(user_dialogue_acts, extracted_information, other_features_from_dialogue_history3):\n",
        "    # Predict to be retrieved\n",
        "    to_be_retrieved = agent_to_be_retrieved_testing_suite.predict_only_last_dialog_item(\n",
        "        dialog_item_dataset=other_features_from_dialogue_history3[\"dialogue_history\"],\n",
        "    )\n",
        "    return to_be_retrieved\n",
        "\n",
        "def Agent_Move_Prediction(user_dialogue_acts, extracted_information, retrieved_information, other_features_from_dialogue_history4):\n",
        "\n",
        "    # Model 3.2\n",
        "    # Predict the dialogue act\n",
        "    # Filter retrieved information to only have the part before :\n",
        "    retrieved_information_filtered = []\n",
        "    for i in range(len(retrieved_information)):\n",
        "        retrieved_information_filtered.append(retrieved_information[i].split(':')[0])\n",
        "    agent_dialogue_acts = agent_da_testing_suite.predict_only_last_dialog_item(\n",
        "        dialog_item_dataset=other_features_from_dialogue_history4[\"dialogue_history\"],\n",
        "        to_be_provided_gt=retrieved_information_filtered\n",
        "    )\n",
        "    \n",
        "    \n",
        "    # convert dict to 'key:value' list\n",
        "    extracted_information = [f\"{key}:{value}\" for act_list in extracted_information.values() for key, value in act_list]\n",
        "    \n",
        "    \n",
        "    input_text = \" \".join(user_dialogue_acts + extracted_information)\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    mlb = joblib.load(\"mlb3-3.pkl\")\n",
        "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "    \n",
        "    to_be_requested = set(ToBeProvided.query_model(input_text, to_be_provided_testing_suite, tokenizer, mlb, device, threshold=0.5))\n",
        "    return {\"agent_dialogue_acts\":agent_dialogue_acts,\n",
        "            \"to_be_requested\":to_be_requested}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "tj6WtWKNg08_"
      },
      "outputs": [],
      "source": [
        "# Define dataset for the evaluation notebook\n",
        "dataset = load_dataset(\"multi_woz_v22\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "b4lMNCwkOMPS"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "\n",
        "def print_output(*output):\n",
        "    if do_print_dialogue_details:\n",
        "        if len(output) == 1:\n",
        "            print(output[0])\n",
        "        elif len(output) == 2:\n",
        "            print(output[0], output[1])\n",
        "        elif len(output) == 3:\n",
        "            print(output[0], output[1], output[2])\n",
        "        elif len(output) == 4:\n",
        "            print(output[0], output[1], output[2], output[3])\n",
        "        else:\n",
        "            print(output)\n",
        "\n",
        "def count_matches(ground_truth_list, predicted_list):\n",
        "    no_gt = len(ground_truth_list)\n",
        "    no_predicted = len(predicted_list)\n",
        "    no_correct = no_gt - sum((Counter(ground_truth_list) - Counter(predicted_list)).values())\n",
        "    return no_gt, no_predicted, no_correct\n",
        "\n",
        "def get_metrics(no_gt_global, no_predicted_global, no_correct_global):\n",
        "    precision = 1.0*no_correct_global/no_predicted_global if no_predicted_global else 0.0\n",
        "    recall = 1.0*no_correct_global/no_gt_global if no_gt_global else 0.0\n",
        "    f1_score = 2.0*precision*recall/(precision+recall) if precision and recall else 0.0\n",
        "    return precision, recall, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQUkYhsQvaVO",
        "outputId": "b34b497e-7217-4291-e8e7-1e255fba6f8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "######################################################\n",
            "To be Requested ground_truth: []\n",
            "To be Requested predicted: {('food',)}\n",
            "######################################################\n",
            "\n",
            "######################################################\n",
            "To be Requested ground_truth: []\n",
            "To be Requested predicted: {('area',)}\n",
            "######################################################\n",
            "\n",
            "######################################################\n",
            "To be Requested ground_truth: []\n",
            "To be Requested predicted: {()}\n",
            "######################################################\n",
            "\n",
            "######################################################\n",
            "To be Requested ground_truth: ['hotel-bookday']\n",
            "To be Requested predicted: {()}\n",
            "######################################################\n",
            "\n",
            "######################################################\n",
            "To be Requested ground_truth: []\n",
            "To be Requested predicted: {()}\n",
            "######################################################\n",
            "\n",
            "\n",
            "######################################################\n",
            "To be Requested ground_truth: []\n",
            "To be Requested predicted: {()}\n",
            "######################################################\n",
            "\n",
            "######################################################\n",
            "To be Requested ground_truth: ['hotel-pricerange', 'restaurant-pricerange']\n",
            "To be Requested predicted: {()}\n",
            "######################################################\n",
            "\n",
            "######################################################\n",
            "To be Requested ground_truth: []\n",
            "To be Requested predicted: {()}\n",
            "######################################################\n",
            "\n",
            "######################################################\n",
            "To be Requested ground_truth: ['hotel-bookday', 'hotel-bookstay']\n",
            "To be Requested predicted: {()}\n",
            "######################################################\n",
            "\n",
            "######################################################\n",
            "To be Requested ground_truth: []\n",
            "To be Requested predicted: {('bookday',)}\n",
            "######################################################\n",
            "\n",
            "\n",
            "######################################################\n",
            "To be Requested ground_truth: []\n",
            "To be Requested predicted: {('bookday', 'bookstay')}\n",
            "######################################################\n",
            "\n",
            "######################################################\n",
            "To be Requested ground_truth: ['hotel-bookstay']\n",
            "To be Requested predicted: {()}\n",
            "######################################################\n",
            "\n",
            "######################################################\n",
            "To be Requested ground_truth: []\n",
            "To be Requested predicted: {('bookday',)}\n",
            "######################################################\n",
            "\n",
            "######################################################\n",
            "To be Requested ground_truth: ['restaurant-food']\n",
            "To be Requested predicted: {('food',)}\n",
            "######################################################\n",
            "\n",
            "######################################################\n",
            "To be Requested ground_truth: []\n",
            "To be Requested predicted: {('bookday', 'booktime')}\n",
            "######################################################\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "######################################################\n",
            "To be Requested ground_truth: ['hotel-bookday', 'hotel-bookpeople', 'hotel-bookstay']\n",
            "To be Requested predicted: {()}\n",
            "######################################################\n",
            "\n",
            "######################################################\n",
            "To be Requested ground_truth: ['hotel-bookpeople']\n",
            "To be Requested predicted: {()}\n",
            "######################################################\n",
            "\n",
            "######################################################\n",
            "To be Requested ground_truth: []\n",
            "To be Requested predicted: {('bookday', 'bookstay')}\n",
            "######################################################\n",
            "\n",
            "\n",
            "######################################################\n",
            "To be Requested ground_truth: ['restaurant-area', 'restaurant-name']\n",
            "To be Requested predicted: {('food',)}\n",
            "######################################################\n",
            "\n",
            "######################################################\n",
            "To be Requested ground_truth: []\n",
            "To be Requested predicted: {('bookday',)}\n",
            "######################################################\n",
            "\n",
            "######################################################\n",
            "To be Requested ground_truth: []\n",
            "To be Requested predicted: {()}\n",
            "######################################################\n",
            "\n",
            "\n",
            "\n",
            "######################################################\n",
            "To be Requested ground_truth: []\n",
            "To be Requested predicted: {()}\n",
            "######################################################\n",
            "\n",
            "######################################################\n",
            "To be Requested ground_truth: []\n",
            "To be Requested predicted: {('bookday', 'booktime')}\n",
            "######################################################\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "######################################################\n",
            "To be Requested ground_truth: []\n",
            "To be Requested predicted: {('bookday',)}\n",
            "######################################################\n",
            "\n",
            "######################################################\n",
            "To be Requested ground_truth: []\n",
            "To be Requested predicted: {('bookday', 'booktime')}\n",
            "######################################################\n",
            "\n",
            "######################################################\n",
            "To be Requested ground_truth: ['restaurant-bookpeople']\n",
            "To be Requested predicted: {('booktime',)}\n",
            "######################################################\n",
            "\n",
            "######################################################\n",
            "To be Requested ground_truth: []\n",
            "To be Requested predicted: {('bookday', 'booktime')}\n",
            "######################################################\n",
            "\n",
            "\n",
            "\n",
            "######################################################\n",
            "To be Requested ground_truth: ['restaurant-food']\n",
            "To be Requested predicted: {('food',)}\n",
            "######################################################\n",
            "\n",
            "######################################################\n",
            "To be Requested ground_truth: []\n",
            "To be Requested predicted: {('area',)}\n",
            "######################################################\n",
            "\n",
            "######################################################\n",
            "To be Requested ground_truth: []\n",
            "To be Requested predicted: {('booktime',)}\n",
            "######################################################\n",
            "\n",
            "######################################################\n",
            "To be Requested ground_truth: []\n",
            "To be Requested predicted: {('booktime',)}\n",
            "######################################################\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "######################################################\n",
            "To be Requested ground_truth: ['hotel-area']\n",
            "To be Requested predicted: {()}\n",
            "######################################################\n",
            "\n",
            "\n",
            "\n",
            "######################################################\n",
            "To be Requested ground_truth: []\n",
            "To be Requested predicted: {()}\n",
            "######################################################\n",
            "\n",
            "######################################################\n",
            "To be Requested ground_truth: []\n",
            "To be Requested predicted: {()}\n",
            "######################################################\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "######################################################\n",
            "To be Requested ground_truth: []\n",
            "To be Requested predicted: {('food',)}\n",
            "######################################################\n",
            "\n",
            "######################################################\n",
            "To be Requested ground_truth: []\n",
            "To be Requested predicted: {('bookday', 'booktime')}\n",
            "######################################################\n",
            "\n",
            "######################################################\n",
            "To be Requested ground_truth: []\n",
            "To be Requested predicted: {('bookday', 'booktime')}\n",
            "######################################################\n",
            "\n",
            "\n",
            "######################################################\n",
            "To be Requested ground_truth: []\n",
            "To be Requested predicted: {('area',)}\n",
            "######################################################\n",
            "\n",
            "######################################################\n",
            "To be Requested ground_truth: []\n",
            "To be Requested predicted: {('bookday',)}\n",
            "######################################################\n",
            "\n",
            "\n",
            "\n",
            "Dialogue acts in the user's move prediction\n",
            "Precision: 0.771429, Recall: 0.729730, F1-score: 0.750000\n",
            "Extracted information from user's utterance\n",
            "Precision: 0.680000, Recall: 0.557377, F1-score: 0.612613\n",
            "Info to be retrieved by the agent\n",
            "Precision: 0.528302, Recall: 0.325581, F1-score: 0.402878\n",
            "Dialogue acts in the agent's move prediction\n",
            "Precision: 0.625000, Recall: 0.468750, F1-score: 0.535714\n",
            "Info to be requested by the agent\n",
            "Precision: 0.000000, Recall: 0.000000, F1-score: 0.000000\n"
          ]
        }
      ],
      "source": [
        "global do_print_dialogue_details\n",
        "do_print_dialogue_details = False\n",
        "print_ground_truth_structures = False\n",
        "print_predicted = False\n",
        "\n",
        "no_user_moves_gt_global = 0 # \"gt\" == \"ground truth\"\n",
        "no_user_moves_predicted_global = 0\n",
        "no_user_moves_correct_global = 0\n",
        "\n",
        "no_user_info_gt_global = 0\n",
        "no_user_info_predicted_global = 0\n",
        "no_user_info_correct_global = 0\n",
        "\n",
        "no_agent_info_to_retrieve_gt_global = 0\n",
        "no_agent_info_to_retrieve_predicted_global = 0\n",
        "no_agent_info_to_retrieve_correct_global = 0\n",
        "\n",
        "no_agent_moves_gt_global = 0\n",
        "no_agent_moves_predicted_global = 0\n",
        "no_agent_moves_correct_global = 0\n",
        "\n",
        "no_agent_info_to_request_gt_global = 0\n",
        "no_agent_info_to_request_predicted_global = 0\n",
        "no_agent_info_to_request_correct_global = 0\n",
        "\n",
        "n_dialogues_to_evaluate = 10\n",
        "n_evaluated = 0\n",
        "for d,dial in enumerate(dataset['train']):\n",
        "    if n_evaluated>=n_dialogues_to_evaluate:\n",
        "        break\n",
        "    # skip dialogues that are not in the hotel or restaurant domain\n",
        "    if not any(set(dial['turns']['frames'][turn_id]['service']).intersection(['hotel', 'restaurant']) for turn_id,utt in enumerate(dial['turns']['utterance'])):\n",
        "        continue\n",
        "    print_output(\"Dialogue ID:\", dial['dialogue_id'])\n",
        "\n",
        "    # Keep track of dialogue slot history:\n",
        "    dialogue_history = DialogSlotMemory()\n",
        "\n",
        "    # Keep track of Dialog Items for Dialogue Act Prediction:\n",
        "    dialogue_history_for_da_prediction: typing.List[DialogItem] = []\n",
        "    previous_da_dialog_history_ids: typing.List[DialogItemIdentifier] = []\n",
        "\n",
        "    # Keep Track of Dialog Items for Agent Slots to Be Retrieved Prediction:\n",
        "    dialogue_history_for_agent_move_prediction: typing.List[DialogItemForAgentMove] = []\n",
        "    previous_agent_move_dialog_history_ids: typing.List[DialogItemIdentifier] = []\n",
        "    \n",
        "\n",
        "    compulsory_slots_hotel  = set(['hotel-bookpeople', 'hotel-bookstay', 'hotel-name', 'hotel-bookday']) # as an example, to be adjusted\n",
        "    compulsory_slots_restaurant  = set(['restaurant-name']) # as an example, to be adjusted\n",
        "    filled_slots = set()\n",
        "    speaker_str = {0: 'User', 1: 'Agent'}\n",
        "    turns = dial['turns']\n",
        "    for turn_id,utt in enumerate(turns['utterance']):\n",
        "        speaker = speaker_str[turns['speaker'][turn_id]]\n",
        "        \n",
        "        if speaker == \"User\":\n",
        "            print_output(\"User's turn\")\n",
        "            print_output(\"User's utterance: \"+utt)\n",
        "\n",
        "            print_output(\"Extraction\")\n",
        "\n",
        "            indent = \" \"*4\n",
        "            dialogue_acts = turns['dialogue_acts'][turn_id]['dialog_act']['act_type']\n",
        "            user_dialogue_acts_ground_truth = dialogue_acts.copy()\n",
        "            print_output(indent, \"Dialogue acts:\", dialogue_acts)\n",
        "\n",
        "            if print_ground_truth_structures:\n",
        "                print(user_dialogue_acts_ground_truth)\n",
        "\n",
        "            if user_dialogue_acts_ground_truth:\n",
        "                dialogue_acts_predicted = Dialogue_Act_Prediction(utt, other_features_from_dialogue_history={\"dialogue_history\": dialogue_history_for_da_prediction, \"speaker\": speaker, \"id_dialogue\": dial['dialogue_id'], \"turn_id\": turn_id, \"previous_dialogue_history_ids\": previous_da_dialog_history_ids})\n",
        "                # evaluate user's dialogue acts\n",
        "                no_gt, no_predicted, no_correct = count_matches(user_dialogue_acts_ground_truth, dialogue_acts_predicted)\n",
        "                no_user_moves_gt_global += no_gt\n",
        "                no_user_moves_predicted_global += no_predicted\n",
        "                no_user_moves_correct_global += no_correct\n",
        "                if print_predicted:\n",
        "                    print(indent, \"User's dialogue acts predicted:\", dialogue_acts_predicted, \"correct = %d/%d, true covered = %d/%d\" % (no_correct, no_predicted, no_correct, no_gt))\n",
        "\n",
        "                # Add the current dialogue act to the dialogue history, AFTER PREDICTION, with the ground truth labels\n",
        "                DialogActModel.add_dialogue_items_to_dialogue_history(\n",
        "                    utterance=utt, speaker=speaker, dialog_history=dialogue_history_for_da_prediction, dialog_acts=dialogue_acts, id_dialog=dial['dialogue_id'], order_in_dialog=turn_id, previous_dialog_history_ids=previous_da_dialog_history_ids\n",
        "                )\n",
        "\n",
        "                add_dialogue_items_to_dialogue_history_for_agent_move(\n",
        "                    utterance=utt, speaker=speaker, dialog_acts=dialogue_acts, to_be_retrieved_gt=[], id_dialog=dial['dialogue_id'], order_in_dialog=turn_id, previous_dialog_history_ids=previous_agent_move_dialog_history_ids, dialog_history=dialogue_history_for_agent_move_prediction\n",
        "                )\n",
        "                \n",
        "            print_output(indent, \"Extracted information:\")\n",
        "            print_output(indent, \"Spans\")\n",
        "            extracted_information_not_mapped_ground_truth = []\n",
        "            extracted_information_ground_truth = []\n",
        "            extracted_information_per_dialogue_act_ground_truth = {}\n",
        "            span_info = turns['dialogue_acts'][turn_id]['span_info']\n",
        "            for span_i in range(len(span_info['span_start'])):\n",
        "                act_type = span_info['act_type'][span_i]\n",
        "                span_name = span_info['act_slot_name'][span_i]\n",
        "                span_value = span_info['act_slot_value'][span_i]\n",
        "                span_range = [span_info['span_start'][span_i], span_info['span_end'][span_i]]\n",
        "                span_value_as_in_utterance = utt[span_info['span_start'][span_i]: span_info['span_end'][span_i]]\n",
        "                print_output(indent*2, span_value + (\"\" if span_value_as_in_utterance==span_value else \" (\"+span_value_as_in_utterance+\")\"), span_range)\n",
        "                if not act_type in extracted_information_per_dialogue_act_ground_truth:\n",
        "                    extracted_information_per_dialogue_act_ground_truth[act_type] = []\n",
        "                act_category = act_type.split(\"-\")[0].lower()\n",
        "                extracted_information_not_mapped_ground_truth.append(tuple([act_category+\"-\"+span_name, span_value_as_in_utterance]))\n",
        "                if act_category in [\"hotel\", \"restaurant\", \"general\"]:\n",
        "                    extracted_information_ground_truth.append(tuple([act_category+\"-\"+span_name, span_value]))\n",
        "                    # update the dialogue history\n",
        "                    dialogue_history.add_slot(span_name,span_value)\n",
        "                extracted_information_per_dialogue_act_ground_truth[act_type].append(tuple([span_name, span_value]))\n",
        "\n",
        "            print_output(indent, \"Categorized information\")\n",
        "            slot_names_per_act = [slot['slot_name'] for slot in turns['dialogue_acts'][turn_id]['dialog_act']['act_slots']]\n",
        "            slot_values_per_act = [slot['slot_value'] for slot in turns['dialogue_acts'][turn_id]['dialog_act']['act_slots']]\n",
        "            for act_i in range(len(slot_names_per_act)):\n",
        "                slot_names_values_per_act = [slot_names_per_act[act_i][slot_i]+\":\"+slot_values_per_act[act_i][slot_i] for slot_i in range(len(slot_names_per_act[act_i]))]\n",
        "                print_output(indent*2, dialogue_acts[act_i], slot_names_values_per_act)\n",
        "                if dialogue_acts[act_i].startswith(\"Hotel\") or dialogue_acts[act_i].startswith(\"Restaurant\"):\n",
        "                    for slot_i in range(len(slot_names_per_act[act_i])):\n",
        "                        if slot_names_per_act[act_i][slot_i] != \"none\" and slot_values_per_act[act_i][slot_i] == \"?\":\n",
        "                            if not dialogue_acts[act_i] in extracted_information_per_dialogue_act_ground_truth:\n",
        "                                extracted_information_per_dialogue_act_ground_truth[dialogue_acts[act_i]] = []\n",
        "                            extracted_information_not_mapped_ground_truth.append(tuple([dialogue_acts[act_i].split(\"-\")[0].lower()+\"-\"+slot_names_per_act[act_i][slot_i], slot_values_per_act[act_i][slot_i]]))\n",
        "                            extracted_information_ground_truth.append(tuple([dialogue_acts[act_i].split(\"-\")[0].lower()+\"-\"+slot_names_per_act[act_i][slot_i], slot_values_per_act[act_i][slot_i]]))\n",
        "                            extracted_information_per_dialogue_act_ground_truth[dialogue_acts[act_i]].append(tuple([slot_names_per_act[act_i][slot_i], slot_values_per_act[act_i][slot_i]]))\n",
        "\n",
        "            if print_ground_truth_structures:\n",
        "                print(extracted_information_not_mapped_ground_truth)\n",
        "                print(extracted_information_ground_truth)\n",
        "                print(extracted_information_per_dialogue_act_ground_truth)\n",
        "\n",
        "            if user_dialogue_acts_ground_truth:\n",
        "                extracted_information = Extract_and_Categorize_Spans(utt, user_dialogue_acts_ground_truth, other_features_from_dialogue_history2={\"slot_history\": dialogue_history})\n",
        "                # evaluate information extraction only if there are Hotel or Restaurant or general dialogue acts\n",
        "                no_gt, no_predicted, no_correct = count_matches(extracted_information_ground_truth, extracted_information)\n",
        "                #print(\"######################################################\")\n",
        "                #print(f\"Extracted_information_ground_truth: {extracted_information_ground_truth}\")\n",
        "                ##print(f\"Extracted_information: {extracted_information}\")\n",
        "                #print(no_correct, no_predicted, no_gt)\n",
        "                #print(\"######################################################\")\n",
        "                print(\"\")\n",
        "                if any(da.startswith(\"general\") or da.startswith(\"Hotel\") or da.startswith(\"Restaurant\") for da in user_dialogue_acts_ground_truth):\n",
        "                    no_user_info_gt_global += no_gt\n",
        "                    no_user_info_predicted_global += no_predicted\n",
        "                    no_user_info_correct_global += no_correct\n",
        "                    if print_predicted:\n",
        "                        print(indent, \"Extracted and categorized information (predicted):\", extracted_information, \"correct = %d/%d, true covered = %d/%d\" % (no_correct, no_predicted, no_correct, no_gt))\n",
        "\n",
        "            print_output(\"Reasoning (dialogue state tracking)\")\n",
        "            services = turns['frames'][turn_id]['service']\n",
        "            print_output(indent, \"Services:\", services)\n",
        "            current_booking_service = [service for service in services if service in [\"hotel\", \"restaurant\"]]\n",
        "\n",
        "            not_empty_intents = [intent for intent in turns['frames'][turn_id]['state'] if intent['requested_slots'] or intent['slots_values']['slots_values_name']]\n",
        "            if not_empty_intents:\n",
        "                print_output(indent, \"Intents\")\n",
        "                for intent in not_empty_intents:\n",
        "                    print_output(indent*2, \"Active intent:\", intent['active_intent'])\n",
        "                    requested_slots = intent['requested_slots']\n",
        "                    if requested_slots:\n",
        "                        print_output(indent*2, \"Requested slots:\", requested_slots)\n",
        "                    if intent['slots_values']['slots_values_name']:\n",
        "                        slot_names = intent['slots_values']['slots_values_name']\n",
        "                        slot_values = intent['slots_values']['slots_values_list']\n",
        "                        filled_slots.update(slot_names)\n",
        "                        print_output(indent*2, \"Filled slots:\")\n",
        "                        for slot_i in range(len(slot_names)):\n",
        "                            print_output(indent*3, slot_names[slot_i]+\": \", slot_values[slot_i])\n",
        "                    print_output(indent*2, \"--------------\")\n",
        "\n",
        "            print_output(indent, \"Missing slots (Hotel):\", compulsory_slots_hotel - filled_slots)\n",
        "            print_output(indent, \"Missing slots (Restaurant):\", compulsory_slots_restaurant - filled_slots)\n",
        "        elif speaker == \"Agent\":\n",
        "            indent = \" \"*4\n",
        "            print_output(\"Agent's turn\")\n",
        "            dialogue_acts = turns['dialogue_acts'][turn_id]['dialog_act']['act_type']\n",
        "\n",
        "            DialogActModel.add_dialogue_items_to_dialogue_history(\n",
        "                utterance=utt,\n",
        "                speaker=speaker,\n",
        "                dialog_acts=dialogue_acts,\n",
        "                id_dialog=dial['dialogue_id'],\n",
        "                order_in_dialog=turn_id,\n",
        "                previous_dialog_history_ids=previous_da_dialog_history_ids,\n",
        "                dialog_history=dialogue_history_for_da_prediction,\n",
        "            )\n",
        "            \n",
        "            do_evaluate_agent_turn = True\n",
        "            if not any(da.startswith(\"Hotel\") or da.startswith(\"Restaurant\") or da.startswith(\"Booking\") for da in dialogue_acts):\n",
        "                do_evaluate_agent_turn = False\n",
        "                print_output(\"This agent's turn won't be evaluated as it is out of domain.\")\n",
        "\n",
        "            print_output(\"Retrieval\")\n",
        "\n",
        "            slot_names_per_act = [slot['slot_name'] for slot in turns['dialogue_acts'][turn_id]['dialog_act']['act_slots']]\n",
        "            slot_values_per_act = [slot['slot_value'] for slot in turns['dialogue_acts'][turn_id]['dialog_act']['act_slots']]\n",
        "\n",
        "            to_be_retrieved_ground_truth = []\n",
        "            print_output(indent, \"Information to be retrieved:\")\n",
        "            for act_i in range(len(slot_names_per_act)):\n",
        "                domain = dialogue_acts[act_i].split(\"-\")[0].lower()\n",
        "                if domain == \"booking\" and len(current_booking_service)==1:\n",
        "                    domain = current_booking_service[0]\n",
        "                slot_names = [domain+\"-\"+slot_names_per_act[act_i][slot_i] for slot_i in range(len(slot_names_per_act[act_i]))\n",
        "                              if slot_values_per_act[act_i][slot_i]!=\"?\" and slot_names_per_act[act_i][slot_i]!=\"none\"]\n",
        "                if slot_names:\n",
        "                    to_be_retrieved_slot_names = [\"%s-availability\" % (domain)] + slot_names\n",
        "                    while domain+\"-choice\" in to_be_retrieved_slot_names:\n",
        "                        del to_be_retrieved_slot_names[to_be_retrieved_slot_names.index(domain+\"-choice\")]\n",
        "                    to_be_retrieved_ground_truth.extend(to_be_retrieved_slot_names)\n",
        "            to_be_retrieved_ground_truth = sorted(list(set(to_be_retrieved_ground_truth)))\n",
        "            print_output(indent*2, \"To be retrieved:\", to_be_retrieved_ground_truth, \"<--- That's the first thing we predict in agent's move.\")\n",
        "\n",
        "            to_be_provided_overall = []\n",
        "            print_output(indent, \"Retrieved information:\")\n",
        "            for act_i in range(len(slot_names_per_act)):\n",
        "                domain = dialogue_acts[act_i].split(\"-\")[0].lower()\n",
        "                if domain == \"booking\" and len(current_booking_service)==1:\n",
        "                    domain = current_booking_service[0]\n",
        "                if domain in [\"hotel\", \"restaurant\", \"booking\", \"general\"]:\n",
        "                    slot_names_vlues = [domain+\"-\"+slot_names_per_act[act_i][slot_i]+\":\"+slot_values_per_act[act_i][slot_i] for slot_i in range(len(slot_names_per_act[act_i]))\n",
        "                                        if slot_values_per_act[act_i][slot_i]!=\"?\" and slot_names_per_act[act_i][slot_i]!=\"none\"]\n",
        "                    if slot_names_vlues and any((slot_name_value.split(\":\")[0]!=domain+\"-none\" for slot_name_value in slot_names_vlues)) and not \"-No\" in dialogue_acts[act_i]:\n",
        "                        to_be_provided = [\"%s-availability:yes\" % (domain)] + slot_names_vlues\n",
        "                        to_be_provided_overall.extend(to_be_provided)\n",
        "                    elif \"-No\" in dialogue_acts[act_i]:\n",
        "                        to_be_provided = [\"%s-availability:no\" % (domain)] + slot_names_vlues\n",
        "                        to_be_provided_overall.extend(to_be_provided)\n",
        "            to_be_provided_overall = sorted(list(set(to_be_provided_overall)))\n",
        "            remove_avail_no_list = [elem for elem in to_be_provided_overall if elem.endswith(\"availability:no\")]\n",
        "            for remove_avail in remove_avail_no_list:\n",
        "                remove_avail_yes = remove_avail[:-2]+\"yes\"\n",
        "                while remove_avail_yes in to_be_provided_overall:\n",
        "                    del to_be_provided_overall[to_be_provided_overall.index(remove_avail_yes)]\n",
        "            print_output(indent*2, \"Retrieved info to be provided:\", to_be_provided_overall)\n",
        "\n",
        "            print_output(\"Planning\")\n",
        "            agent_dialogue_acts_ground_truth = []\n",
        "            for act_i in range(len(slot_names_per_act)):\n",
        "                domain = dialogue_acts[act_i].split(\"-\")[0].lower()\n",
        "                if domain in [\"hotel\", \"restaurant\", \"booking\", \"general\"]:\n",
        "                    agent_dialogue_acts_ground_truth.append(dialogue_acts[act_i])\n",
        "            print_output(indent, \"Agent's move (dialogue acts):\", agent_dialogue_acts_ground_truth, \"<--- That's the second thing we predict in agent's move.\")\n",
        "\n",
        "            print_output(indent, \"Information to be requested:\")\n",
        "            to_be_requested_ground_truth = []\n",
        "            for act_i in range(len(slot_names_per_act)):\n",
        "                domain = dialogue_acts[act_i].split(\"-\")[0].lower()\n",
        "                if domain == \"booking\" and len(current_booking_service)==1:\n",
        "                    domain = current_booking_service[0]\n",
        "                if domain in [\"hotel\", \"restaurant\", \"booking\", \"general\"]:\n",
        "                    to_be_requested = [domain+\"-\"+slot_names_per_act[act_i][slot_i] for slot_i in range(len(slot_names_per_act[act_i])) if slot_values_per_act[act_i][slot_i]==\"?\"]\n",
        "                    to_be_requested_ground_truth.extend(to_be_requested)\n",
        "            to_be_requested_ground_truth = sorted(list(set(to_be_requested_ground_truth)))\n",
        "            print_output(indent*2, \"To be requested:\", to_be_requested_ground_truth, \"<--- That's the third thing we predict in agent's move.\")\n",
        "\n",
        "            print_output(indent, \"Planned move per dialogue act (we won't evaluate this):\")\n",
        "            for act_i in range(len(slot_names_per_act)):\n",
        "                print_output(indent*2, dialogue_acts[act_i])\n",
        "                print_output(indent*2, \"To be provided:\", [slot_names_per_act[act_i][slot_i]+\":\"+slot_values_per_act[act_i][slot_i] for slot_i in range(len(slot_names_per_act[act_i])) if slot_values_per_act[act_i][slot_i]!=\"?\"])\n",
        "                print_output(indent*2, \"To be requested:\", [slot_names_per_act[act_i][slot_i]+\":\"+slot_values_per_act[act_i][slot_i] for slot_i in range(len(slot_names_per_act[act_i])) if slot_values_per_act[act_i][slot_i]==\"?\"])\n",
        "                print_output(indent*2, \"--------------\")\n",
        "\n",
        "            agent_move_ground_truth = {\n",
        "                \"to_be_retrieved\": to_be_retrieved_ground_truth,  # set, only unique names of the slots\n",
        "                \"dialogue_acts\": agent_dialogue_acts_ground_truth,\n",
        "                \"to_be_requested\": to_be_requested_ground_truth,  # set, only unique names of the slots\n",
        "                \"retrieved_information_per_dialogue_act\": {} # non-none names of the slots and values grouped per dialogue act\n",
        "                }\n",
        "\n",
        "            for act_i in range(len(slot_names_per_act)):\n",
        "                da = dialogue_acts[act_i]\n",
        "                if da.startswith(\"Hotel\") or da.startswith(\"Restaurant\") or da.startswith(\"Booking\") or da.startswith(\"general\"):\n",
        "                    for slot_i in range(len(slot_names_per_act[act_i])):\n",
        "                        if slot_names_per_act[act_i][slot_i] != \"none\":\n",
        "                            if not dialogue_acts[act_i] in agent_move_ground_truth[\"retrieved_information_per_dialogue_act\"]:\n",
        "                                agent_move_ground_truth[\"retrieved_information_per_dialogue_act\"][dialogue_acts[act_i]] = []\n",
        "                            agent_move_ground_truth[\"retrieved_information_per_dialogue_act\"][dialogue_acts[act_i]].append(tuple([slot_names_per_act[act_i][slot_i], slot_values_per_act[act_i][slot_i]]))\n",
        "\n",
        "            if do_evaluate_agent_turn and user_dialogue_acts_ground_truth:\n",
        "                agent_to_be_retrieved_predicted = Information_to_be_retrieved_Prediction(user_dialogue_acts_ground_truth,\n",
        "                                                                                         extracted_information_per_dialogue_act_ground_truth,\n",
        "                                                                                         other_features_from_dialogue_history3={\"dialogue_history\": dialogue_history_for_agent_move_prediction, \"speaker\": speaker, \"id_dialogue\": dial['dialogue_id'], \"turn_id\": turn_id, \"previous_dialogue_history_ids\": previous_agent_move_dialog_history_ids})\n",
        "                \n",
        "                agent_move_predicted = Agent_Move_Prediction(user_dialogue_acts_ground_truth,\n",
        "                                                             extracted_information_per_dialogue_act_ground_truth,\n",
        "                                                             to_be_provided_overall,\n",
        "                                                             other_features_from_dialogue_history4={\"dialogue_history\": dialogue_history_for_agent_move_prediction, \"speaker\": speaker, \"id_dialogue\": dial['dialogue_id'], \"turn_id\": turn_id, \"previous_dialogue_history_ids\": previous_agent_move_dialog_history_ids})\n",
        "                if print_ground_truth_structures:\n",
        "                    print(agent_move_ground_truth)\n",
        "\n",
        "                if print_predicted:\n",
        "                    print(\"Planning predicted\")\n",
        "\n",
        "                # evaluate \"to_be_retrieved\" only if there are Hotel or Restaurant or Booking or general dialogue acts\n",
        "                no_gt, no_predicted, no_correct = count_matches(agent_move_ground_truth[\"to_be_retrieved\"], agent_to_be_retrieved_predicted)\n",
        "                no_agent_info_to_retrieve_gt_global += no_gt\n",
        "                no_agent_info_to_retrieve_predicted_global += no_predicted\n",
        "                no_agent_info_to_retrieve_correct_global += no_correct\n",
        "                if print_predicted:\n",
        "                    print(indent, \"Info to be retrieved predicted:\", agent_to_be_retrieved_predicted, \"correct = %d/%d, true covered = %d/%d\" % (no_correct, no_predicted, no_correct, no_gt))\n",
        "\n",
        "                # evaluate agent's dialogue acts\n",
        "                no_gt, no_predicted, no_correct = count_matches(agent_move_ground_truth[\"dialogue_acts\"], agent_move_predicted[\"agent_dialogue_acts\"])\n",
        "                no_agent_moves_gt_global += no_gt\n",
        "                no_agent_moves_predicted_global += no_predicted\n",
        "                no_agent_moves_correct_global += no_correct\n",
        "                if print_predicted:\n",
        "                    print(indent, \"Agent's dialogue acts predicted:\", agent_move_predicted[\"agent_dialogue_acts\"], \"correct = %d/%d, true covered = %d/%d\" % (no_correct, no_predicted, no_correct, no_gt))\n",
        "\n",
        "                # evaluate \"retrieved_information\" -> \"to_be_requested\" only if there are Hotel or Restaurant or Booking or general dialogue acts\n",
        "                no_gt, no_predicted, no_correct = count_matches(agent_move_ground_truth[\"to_be_requested\"], agent_move_predicted[\"to_be_requested\"])\n",
        "                tbr_gt = agent_move_ground_truth[\"to_be_requested\"]\n",
        "                tbr = agent_move_predicted[\"to_be_requested\"]\n",
        "                print(\"######################################################\")\n",
        "                print(f\"To be Requested ground_truth: {tbr_gt}\")\n",
        "                print(f\"To be Requested predicted: {tbr}\")\n",
        "                print(\"######################################################\")\n",
        "                no_agent_info_to_request_gt_global += no_gt\n",
        "                no_agent_info_to_request_predicted_global += no_predicted\n",
        "                no_agent_info_to_request_correct_global += no_correct\n",
        "                if print_predicted:\n",
        "                    print(indent, \"Info to be requested predicted:\", agent_move_predicted[\"to_be_requested\"], \"correct = %d/%d, true covered = %d/%d\" % (no_correct, no_predicted, no_correct, no_gt))\n",
        "            \n",
        "            add_dialogue_items_to_dialogue_history_for_agent_move(\n",
        "                utterance=utt, speaker=speaker, dialog_acts=dialogue_acts, to_be_retrieved_gt=agent_move_ground_truth[\"to_be_retrieved\"], id_dialog=dial['dialogue_id'], order_in_dialog=turn_id, previous_dialog_history_ids=previous_agent_move_dialog_history_ids, dialog_history=dialogue_history_for_agent_move_prediction\n",
        "            )\n",
        "            print_output(\"Agent's utterance: \"+utt)\n",
        "        print_output(\"-------------------------------------------------------------------\")\n",
        "        print_output(\"-------------------------------------------------------------------\")\n",
        "\n",
        "    n_evaluated+=1\n",
        "\n",
        "print(\"Dialogue acts in the user's move prediction\")\n",
        "precision, recall, f1_score = get_metrics(no_user_moves_gt_global, no_user_moves_predicted_global, no_user_moves_correct_global)\n",
        "print(\"Precision: %lf, Recall: %lf, F1-score: %lf\" % (precision, recall, f1_score))\n",
        "print(\"Extracted information from user's utterance\")\n",
        "precision, recall, f1_score = get_metrics(no_user_info_gt_global, no_user_info_predicted_global, no_user_info_correct_global)\n",
        "print(\"Precision: %lf, Recall: %lf, F1-score: %lf\" % (precision, recall, f1_score))\n",
        "print(\"Info to be retrieved by the agent\")\n",
        "precision, recall, f1_score = get_metrics(no_agent_info_to_retrieve_gt_global, no_agent_info_to_retrieve_predicted_global, no_agent_info_to_retrieve_correct_global)\n",
        "print(\"Precision: %lf, Recall: %lf, F1-score: %lf\" % (precision, recall, f1_score))\n",
        "print(\"Dialogue acts in the agent's move prediction\")\n",
        "precision, recall, f1_score = get_metrics(no_agent_moves_gt_global, no_agent_moves_predicted_global, no_agent_moves_correct_global)\n",
        "print(\"Precision: %lf, Recall: %lf, F1-score: %lf\" % (precision, recall, f1_score))\n",
        "print(\"Info to be requested by the agent\")\n",
        "precision, recall, f1_score = get_metrics(no_agent_info_to_request_gt_global, no_agent_info_to_request_predicted_global, no_agent_info_to_request_correct_global)\n",
        "print(\"Precision: %lf, Recall: %lf, F1-score: %lf\" % (precision, recall, f1_score))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
