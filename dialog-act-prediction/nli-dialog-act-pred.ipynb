{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-11T13:51:42.116364Z","iopub.status.busy":"2023-11-11T13:51:42.115879Z","iopub.status.idle":"2023-11-11T13:54:07.089951Z","shell.execute_reply":"2023-11-11T13:54:07.089001Z","shell.execute_reply.started":"2023-11-11T13:51:42.116332Z"},"trusted":true},"outputs":[],"source":["# To use in Google Colab or Kaggle Notebooks\n","!pip install datasets==2.14.5\n","!pip install torch==2.0.0\n","!pip install transformers==4.34.0\n","!pip install tqdm==4.66.1\n","!pip install tokenizers==0.14.1\n","!pip install pandas==2.0.3\n","!pip install scikit-learn==1.3.0\n","!pip install matplotlib==3.8.0\n","!pip install --upgrade numpy==1.22.4"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-11-11T13:54:07.092913Z","iopub.status.busy":"2023-11-11T13:54:07.092080Z","iopub.status.idle":"2023-11-11T13:54:18.108885Z","shell.execute_reply":"2023-11-11T13:54:18.107970Z","shell.execute_reply.started":"2023-11-11T13:54:07.092875Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["# Importing Libraries\n","from datasets import load_dataset\n","import pandas as pd\n","from tqdm import tqdm\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","from transformers import DistilBertTokenizer, DistilBertModel\n","from sklearn.metrics import accuracy_score\n","from enum import Enum\n","import matplotlib.pyplot as plt\n","import torch.optim.lr_scheduler as lr_scheduler\n","\n","# Use GPU if available\n","torch.cuda.empty_cache()\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-11-11T13:54:18.110569Z","iopub.status.busy":"2023-11-11T13:54:18.109965Z","iopub.status.idle":"2023-11-11T13:55:18.872877Z","shell.execute_reply":"2023-11-11T13:55:18.872077Z","shell.execute_reply.started":"2023-11-11T13:54:18.110542Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["No config specified, defaulting to: multi_woz_v22/v2.2_active_only\n","Found cached dataset multi_woz_v22 (C:/Users/berna/.cache/huggingface/datasets/multi_woz_v22/v2.2_active_only/2.2.0/6719c8b21478299411a0c6fdb7137c3ebab2e6425129af831687fb7851c69eb5)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5b4660093ae44144aed7a33bc3b3a7bf","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["dataset = load_dataset(\"multi_woz_v22\")\n","train_data = dataset['train']\n","val_data = dataset['validation']\n","test_data = dataset['test']"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-11-11T13:55:18.876076Z","iopub.status.busy":"2023-11-11T13:55:18.875478Z","iopub.status.idle":"2023-11-11T13:55:39.459807Z","shell.execute_reply":"2023-11-11T13:55:39.459025Z","shell.execute_reply.started":"2023-11-11T13:55:18.876033Z"},"trusted":true},"outputs":[],"source":["speaker_str = {0: 'User', 1: 'Agent'}\n","PAST_HISTORY_LENGTH = 2\n","\n","# Change the label of the dialogue act which has the following structure: domain-act to domain if the domain is not\n","# one of the [Restaurant, Hotel, Booking, General]\n","def relabel_dialogue_act(dialogue_act: str):\n","    if dialogue_act.split('-')[0].upper() not in ['RESTAURANT', 'HOTEL', 'BOOKING', 'GENERAL']:\n","        new_dialog_act = dialogue_act.split('-')[0]\n","    else:\n","        new_dialog_act = dialogue_act\n","\n","    return new_dialog_act\n","\n","\n","def toDataFrame(raw_data):\n","    # Initialize an empty list to store rows as dictionaries\n","    data = []\n","\n","    # Loop through each dialogue in the training data\n","    for dialogue in raw_data:\n","\n","        # Get the number of turns in this dialogue\n","        num_turns = len(dialogue['turns']['utterance'])\n","\n","        # Initialize the history of the user and agent as lists with (\"\", []) for the number of PAST_HISTORY_LENGTH elements\n","        previous_user_history = [(\"\",[]) for i in range(PAST_HISTORY_LENGTH)]\n","        previous_agent_history = [(\"\",[]) for i in range(PAST_HISTORY_LENGTH)]\n","\n","        # Loop through each turn in the dialogue\n","        for i in range(num_turns):\n","            speaker = speaker_str[dialogue['turns']['speaker'][i]]\n","\n","            # Extract the utterance and corresponding dialog act for this turn\n","            utterance = dialogue['turns']['utterance'][i]\n","            dialogue_act = dialogue['turns']['dialogue_acts'][i]['dialog_act']['act_type']\n","\n","            # Relabel the dialogue act \n","            dialogue_act_new = []\n","            for act in dialogue_act:\n","                dialogue_act_new.append(relabel_dialogue_act(act))\n","\n","            dialogue_act = dialogue_act_new\n","\n","            if speaker == 'User':\n","                # Encode the history of the user and agent as a string\n","                encoded_history = \"\"\n","                for j in range(len(previous_user_history)):\n","                    encoded_history += \">\".join([previous_user_history[j][0], \"_\".join(previous_user_history[j][1])]) + \"|\"\n","                for j in range(len(previous_agent_history)):\n","                    encoded_history += \">\".join([previous_agent_history[j][0], \"_\".join(previous_agent_history[j][1])]) + \"|\"\n","                encoded_history = encoded_history + utterance\n","\n","                # Append as a dictionary to the data lis\n","                data.append({'EncodedHistory': encoded_history, 'DialogueAct': dialogue_act})\n","\n","                \n","                previous_user_history.append((utterance, dialogue_act))\n","                if len(previous_user_history) > PAST_HISTORY_LENGTH:\n","                    previous_user_history.pop(0)\n","            elif speaker == 'Agent':\n","                previous_agent_history.append((utterance, dialogue_act))\n","\n","                if len(previous_agent_history) > PAST_HISTORY_LENGTH:\n","                    previous_agent_history.pop(0)\n","\n","    # Save data as pandas df\n","    df = pd.DataFrame(data)\n","\n","    # Separate features and labels\n","    X = df['EncodedHistory']\n","    Y = df['DialogueAct']\n","    \n","    return X, Y\n","        \n","X_train, Y_train = toDataFrame(train_data)\n","X_val, Y_val = toDataFrame(val_data)\n","X_test, Y_test = toDataFrame(test_data)\n","\n","# Final Training\n","X_test_val = pd.concat([X_test,X_val])\n","Y_test_val = pd.concat([Y_test, Y_val])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-11T13:55:39.461255Z","iopub.status.busy":"2023-11-11T13:55:39.460899Z","iopub.status.idle":"2023-11-11T13:55:40.171157Z","shell.execute_reply":"2023-11-11T13:55:40.170257Z","shell.execute_reply.started":"2023-11-11T13:55:39.461222Z"},"trusted":true},"outputs":[],"source":["# Generate a histogram of the number of Encoded History per dialogue act, knowing that there can be multiple dialogue acts per Encoded History. \n","# To deal with this, we count as a separate Encoded Historyistory each time a dialogue act is present in an Encoded History, instead of counting more than one\n","# dialogue act per Encoded History.\n","\n","# Initialize an empty dictionary to store the counts of each dialogue act\n","counts = {}\n","\n","# Loop through each dialogue act array in the training data\n","\n","for dialogue_act_array in Y_train:\n","        # Loop through each dialogue act in the dialogue act array\n","        for dialogue_act in dialogue_act_array:\n","            \n","            # If the dialogue act is already in the dictionary, increment its count by 1\n","            if dialogue_act in counts:\n","                counts[dialogue_act] += 1\n","            \n","            # If the dialogue act is not already in the dictionary, set its count to 1\n","            else:\n","                counts[dialogue_act] = 1\n","\n","# Plot the histogram\n","plt.figure(figsize=(20, 10))\n","plt.bar(counts.keys(), counts.values())\n","plt.xticks(rotation=90)\n","plt.xlabel('Dialogue Act')\n","plt.ylabel('Count')\n","plt.title('Number of Encoded Histories per Dialogue Act')\n","plt.show()\n","\n","# Bottom 10 dialogue acts\n","sorted_counts = sorted(counts.items(), key=lambda item: item[1])\n","print(sorted_counts[:10])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-11T13:55:40.172966Z","iopub.status.busy":"2023-11-11T13:55:40.172628Z","iopub.status.idle":"2023-11-11T13:55:41.394235Z","shell.execute_reply":"2023-11-11T13:55:41.393360Z","shell.execute_reply.started":"2023-11-11T13:55:40.172933Z"},"trusted":true},"outputs":[],"source":["\n","def prune_empty_dialogue_acts(X, Y):\n","    \"\"\"\n","    Prunes the data to exclude Encoded Histories with no dialogue acts.\n","\n","    Parameters:\n","    - X: list of Encoded Histories.\n","    - Y: list of lists of dialogue acts.\n","\n","    Returns:\n","    - relabelled lists of Encoded Histories and dialogue acts.\n","    \"\"\"\n","    # Initialize empty lists to store the pruned data\n","    X_pruned = []\n","    Y_pruned = []\n","\n","    # Loop through each Encoded History and corresponding dialogue act array in the data\n","    for encoded_history, dialogue_act_array in zip(X, Y):\n","\n","        # If the dialogue act array is not empty, append the Encoded History and dialogue act array to the pruned data\n","        if len(dialogue_act_array) > 0:\n","            X_pruned.append(encoded_history)\n","            Y_pruned.append(dialogue_act_array)\n","    \n","    return X_pruned, Y_pruned\n","\n","# Prune empty dialogue acts for training, validation, and test data\n","X_pruned_train, Y_pruned_train = prune_empty_dialogue_acts(X_train, Y_train)\n","X_pruned_val, Y_pruned_val = prune_empty_dialogue_acts(X_val, Y_val)\n","X_pruned_test, Y_pruned_test = prune_empty_dialogue_acts(X_test, Y_test)\n","X_pruned_test_val, Y_pruned_test_val = prune_empty_dialogue_acts(X_test_val, Y_test_val)\n","\n","# Generate a histogram of the number of Encoded Histories per dialogue act, knowing that there can be multiple dialogue acts per Encoded History.\n","# To deal with this, we count as a separate Encoded History each time a dialogue act is present in an Encoded History, instead of counting more than one\n","# dialogue act per Encoded History.\n","\n","# Initialize an empty dictionary to store the counts of each dialogue act\n","counts = {}\n","\n","# Loop through each dialogue act array in the training data\n","\n","for dialogue_act_array in Y_pruned_train:\n","    # Loop through each dialogue act in the dialogue act array\n","    for dialogue_act in dialogue_act_array:\n","        \n","        # If the dialogue act is already in the dictionary, increment its count by 1\n","        if dialogue_act in counts:\n","            counts[dialogue_act] += 1\n","        \n","        # If the dialogue act is not already in the dictionary, set its count to 1\n","        else:\n","            counts[dialogue_act] = 1\n","\n","# Plot the histogram\n","plt.figure(figsize=(20, 10))\n","plt.bar(counts.keys(), counts.values())\n","plt.xticks(rotation=90)\n","plt.xlabel('Dialogue Act')\n","plt.ylabel('Count')\n","plt.title('Number of Encoded Histories per Dialogue Act')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-11T13:55:41.395931Z","iopub.status.busy":"2023-11-11T13:55:41.395581Z","iopub.status.idle":"2023-11-11T13:55:41.669284Z","shell.execute_reply":"2023-11-11T13:55:41.668376Z","shell.execute_reply.started":"2023-11-11T13:55:41.395891Z"},"trusted":true},"outputs":[],"source":["from sklearn.preprocessing import MultiLabelBinarizer\n","\n","\n","# Initialize a MultiLabelBinarizer object\n","mlb = MultiLabelBinarizer()\n","\n","# Fit the MultiLabelBinarizer object on the training data\n","mlb.fit(Y_pruned_train)\n","\n","# Transform the training, validation, and test data\n","Y_encoded_train = mlb.transform(Y_pruned_train)\n","Y_encoded_val = mlb.transform(Y_pruned_val)\n","Y_encoded_test = mlb.transform(Y_pruned_test)\n","Y_encoded_test_val = mlb.transform(Y_pruned_test_val)\n","\n","# Print the shape of the encoded training, validation, and test data\n","print(Y_encoded_train.shape)\n","print(Y_encoded_val.shape)\n","print(Y_encoded_test.shape)\n","print(Y_encoded_test_val.shape)\n","\n","# Print the classes\n","print(mlb.classes_)\n","# Print the number of classes\n","print(len(mlb.classes_))\n","\n","# Print the first 10 encoded training labels\n","print(Y_encoded_train[:10])\n","\n","# Print the first 10 decoded training labels\n","print(mlb.inverse_transform(Y_encoded_train[:10]))\n","\n","# Dump the MultiLabelBinarizer object to a pickle file\n","import pickle\n","pickle.dump(mlb, open('mlb.pkl', 'wb'))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot the frequency of Encoded History Lengths\n","# Initialize an empty dictionary to store the counts of each Encoded History length\n","counts = {}\n","\n","# Loop through each Encoded History in the training data\n","for encoded_history in X_pruned_train:\n","    \n","    # Get the length of the utterance\n","    encoded_history_length = len(encoded_history.split())\n","    \n","    # If the Encoded History length is already in the dictionary, increment its count by 1\n","    if encoded_history_length in counts:\n","        counts[encoded_history_length] += 1\n","    \n","    # If the Encoded History length is not already in the dictionary, set its count to 1\n","    else:\n","        counts[encoded_history_length] = 1\n","\n","# Plot the histogram\n","plt.figure(figsize=(20, 10))\n","plt.bar(counts.keys(), counts.values())\n","plt.xlabel('Encoded History Length')\n","plt.ylabel('Count')\n","plt.title('Frequency of Encoded History Lengths')\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-11T13:55:41.670925Z","iopub.status.busy":"2023-11-11T13:55:41.670628Z","iopub.status.idle":"2023-11-11T13:57:57.533462Z","shell.execute_reply":"2023-11-11T13:57:57.532629Z","shell.execute_reply.started":"2023-11-11T13:55:41.670899Z"},"trusted":true},"outputs":[],"source":["# Bert Models Enumeration\n","class BertModelsEnum(str, Enum):\n","    distilbert_base_uncased = 'distilbert-base-uncased'\n","    distilbert_base_cased = 'distilbert-base-cased'\n","\n","bert_model_type = BertModelsEnum.distilbert_base_uncased.value\n","tokenizer = DistilBertTokenizer.from_pretrained(bert_model_type)\n","def tokenize_data(X, Y):\n","    input_encoded = []\n","    attention_masks = []\n","    labels = []\n","    for x,y in zip(X,Y):\n","        encoded = tokenizer.encode_plus(\n","            x, \n","            add_special_tokens=True, \n","            max_length=256, \n","            padding='max_length', \n","            truncation=True, \n","            return_attention_mask=True)\n","        input_encoded.append(encoded['input_ids'])\n","        attention_masks.append(encoded['attention_mask'])\n","        labels.append(y)\n","    return torch.tensor(input_encoded), torch.tensor(attention_masks), torch.tensor(labels)\n","\n","train_input, train_masks, train_labels = tokenize_data(X_pruned_train, Y_encoded_train)\n","val_input, val_masks, val_labels = tokenize_data(X_pruned_val, Y_encoded_val)\n","test_input, test_masks, test_labels = tokenize_data(X_pruned_test, Y_encoded_test)\n","test_val_input, test_val_masks, test_val_labels = tokenize_data(X_pruned_test_val, Y_encoded_test_val)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-11T13:57:57.534969Z","iopub.status.busy":"2023-11-11T13:57:57.534683Z","iopub.status.idle":"2023-11-11T13:57:57.541716Z","shell.execute_reply":"2023-11-11T13:57:57.540848Z","shell.execute_reply.started":"2023-11-11T13:57:57.534944Z"},"trusted":true},"outputs":[],"source":["batch_size = 64 # Adjust based on GPU memory requirements\n","num_workers = 4 # Adjust based on the number of CPU cores\n","\n","train_data = TensorDataset(train_input, train_masks, train_labels)\n","val_data = TensorDataset(val_input, val_masks, val_labels)\n","test_data = TensorDataset(test_input, test_masks, test_labels)\n","test_val_data = TensorDataset(test_val_input, test_val_masks, test_val_labels)\n","\n","train_loader = DataLoader(train_data, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n","val_loader = DataLoader(val_data, batch_size=batch_size, num_workers=num_workers)\n","test_loader = DataLoader(test_data, batch_size=batch_size, num_workers=num_workers)\n","test_val_loader = DataLoader(test_val_data, batch_size=batch_size, num_workers=num_workers)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-11T13:57:57.544816Z","iopub.status.busy":"2023-11-11T13:57:57.544532Z","iopub.status.idle":"2023-11-11T13:57:57.555906Z","shell.execute_reply":"2023-11-11T13:57:57.554899Z","shell.execute_reply.started":"2023-11-11T13:57:57.544793Z"},"trusted":true},"outputs":[],"source":["class BertClassifier(nn.Module):\n","    def __init__(self):\n","        super(BertClassifier, self).__init__()\n","        self.bert = DistilBertModel.from_pretrained(bert_model_type)\n","        self.fc = nn.Linear(768, len(mlb.classes_))\n","\n","    def forward(self, input_ids, attention_mask):\n","        outputs = self.bert(input_ids, attention_mask=attention_mask)\n","        cls_output = outputs.last_hidden_state[:, 0, :] # selects the [CLS] token position.\n","        logits = torch.sigmoid(self.fc(cls_output))\n","        return logits"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-11T13:57:57.557610Z","iopub.status.busy":"2023-11-11T13:57:57.557040Z","iopub.status.idle":"2023-11-11T13:57:57.572816Z","shell.execute_reply":"2023-11-11T13:57:57.571858Z","shell.execute_reply.started":"2023-11-11T13:57:57.557579Z"},"trusted":true},"outputs":[],"source":["class SaveModelWithBestValLoss:\n","    \"\"\"\n","    Class that saves the best model during a training process. If the current epoch's validation loss \n","    is smaller than the previous smallest validation loss saved, it saves the model state.\n","    The best valid loss is initialized to infinity.\n","    \"\"\"\n","    def __init__(\n","        self, smallest_valid_loss=float('inf')\n","    ):\n","        self.smallest_valid_loss = smallest_valid_loss\n","        \n","    def save(\n","        self, current_valid_loss, epoch, model, optimizer, criterion\n","    ):\n","        if current_valid_loss < self.smallest_valid_loss:\n","            self.smallest_valid_loss = current_valid_loss\n","            print(f\"Saving best model for epoch {epoch+1}, with new best validation loss: {self.smallest_valid_loss}\\n\")\n","            torch.save({\n","                'epoch': epoch+1,\n","                'model_state_dict': model.state_dict(),\n","                'optimizer_state_dict': optimizer.state_dict(),\n","                'loss': criterion,\n","                }, 'best_model.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-07T14:50:49.374589Z","iopub.status.busy":"2023-11-07T14:50:49.373909Z"},"trusted":true},"outputs":[],"source":["model = BertClassifier()\n","model = model.to(device)\n"," \n","optimizer = optim.Adam(model.parameters(), lr=1e-5)\n","criterion = nn.BCELoss()\n","\n","scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.3, total_iters=40)\n","\n","save_best_model_instance = SaveModelWithBestValLoss()\n","\n","# For storing training and validation loss\n","train_losses = []\n","val_losses = []\n","\n","lrs = []\n","for epoch in range(40):\n","    model.train()\n","    \n","    # Initialize tqdm progress bar\n","    #train_bar = tqdm(train_loader, desc=f'Training Epoch {epoch}')\n","    \n","    train_loss = 0.0\n","    for batch in train_loader:\n","        input_ids, attention_mask, labels = batch\n","        input_ids = input_ids.to(device)\n","        attention_mask = attention_mask.to(device)\n","        labels = labels.to(device)\n","\n","        \n","        # Zero the parameter gradients\n","        optimizer.zero_grad()\n","        \n","        # Forward pass\n","        logits = model(input_ids, attention_mask)\n","        loss = criterion(logits, labels.float())\n","        \n","        # Backward pass and optimization\n","        loss.backward()\n","        optimizer.step()\n","        \n","        lrs.append(optimizer.param_groups[0][\"lr\"])\n","        scheduler.step()\n","        \n","        # Update training loss\n","        train_loss += loss.item()\n","        #train_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n","    \n","    # Average training loss\n","    avg_train_loss = train_loss / len(train_loader)\n","    train_losses.append(avg_train_loss)\n","    \n","    # Validation\n","    val_loss = 0.0\n","    model.eval()\n","\n","    # Initialize tqdm progress bar for validation\n","    #val_bar = tqdm(val_loader, desc=f'Validation Epoch {epoch}')\n","    \n","    with torch.no_grad():\n","        for batch in val_loader:\n","            input_ids, attention_mask, labels = batch\n","            input_ids = input_ids.to(device)\n","            attention_mask = attention_mask.to(device)\n","            labels = labels.to(device)\n","\n","            \n","            # Forward pass\n","            logits = model(input_ids, attention_mask)\n","            loss = criterion(logits, labels.float())\n","            \n","            val_loss += loss.item()\n","            #val_bar.set_postfix({'validation_loss': '{:.3f}'.format(loss.item()/len(batch))})\n","            \n","    # Average validation loss\n","    avg_val_loss = val_loss / len(val_loader)\n","    val_losses.append(avg_val_loss)\n","    if len(val_losses) >= 4:\n","        diff_val_loss_1 = val_losses[-2] - val_losses[-1]\n","        diff_val_loss_2 = val_losses[-3] - val_losses[-1]\n","        diff_val_loss_3 = val_losses[-4] - val_losses[-1]\n","        if diff_val_loss_3 < 0 and diff_val_loss_2 < 0 and diff_val_loss_1 < 0:\n","            print(f'Epoch {epoch+1}, Validation Loss: {avg_val_loss} DID NOT IMPROVE')\n","            break\n","    \n","    print(f'Epoch {epoch+1}, Training Loss: {avg_train_loss}, Validation Loss: {avg_val_loss}')\n","    \n","    # Save model if validation model loss decreases\n","    save_best_model_instance.save(\n","        avg_val_loss, epoch, model, optimizer, criterion\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-31T20:25:00.841711Z","iopub.status.busy":"2023-10-31T20:25:00.840599Z","iopub.status.idle":"2023-10-31T20:25:01.180026Z","shell.execute_reply":"2023-10-31T20:25:01.178906Z","shell.execute_reply.started":"2023-10-31T20:25:00.841667Z"},"trusted":true},"outputs":[],"source":["# Plotting validation and training loss\n","import matplotlib.pyplot as plt\n","plt.plot(train_losses, label='Training loss')\n","plt.plot(val_losses, label='Validation loss')\n","plt.legend()\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-31T20:25:05.798487Z","iopub.status.busy":"2023-10-31T20:25:05.798072Z","iopub.status.idle":"2023-10-31T20:25:07.269632Z","shell.execute_reply":"2023-10-31T20:25:07.268672Z","shell.execute_reply.started":"2023-10-31T20:25:05.798456Z"},"trusted":true},"outputs":[],"source":["# build the model, without loading the pre-trained weights or fine-tune layers\n","saved_model = BertClassifier().to(device)\n","best_model = torch.load('best_model.pth')\n","saved_model.load_state_dict(best_model['model_state_dict'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-31T20:25:10.445438Z","iopub.status.busy":"2023-10-31T20:25:10.444401Z","iopub.status.idle":"2023-10-31T20:25:16.212213Z","shell.execute_reply":"2023-10-31T20:25:16.210788Z","shell.execute_reply.started":"2023-10-31T20:25:10.445398Z"},"trusted":true},"outputs":[],"source":["all_preds = []\n","all_labels = []\n","\n","# Put the Saved Model in evaluation mode\n","saved_model.eval()\n","\n","# Disable gradient computation\n","with torch.no_grad():\n","    # Initialize tqdm progress bar for Test\n","    test_bar = tqdm(test_loader, desc='Test')\n","\n","    for batch in test_bar:\n","        input_ids, attention_mask, labels = batch\n","        input_ids = input_ids.to(device)\n","        attention_mask = attention_mask.to(device)\n","        labels = labels.to(device)\n","\n","        # Forward pass\n","        logits = saved_model(input_ids, attention_mask)\n","\n","        # Get the predicted labels\n","        #_, preds = torch.max(logits, dim=1)\n","\n","        # Move preds and labels to CPU for further evaluation (if using GPU)\n","        preds = logits.cpu().numpy()\n","        labels = labels.cpu().numpy()\n","\n","        # Extend the list of predictions and labels\n","        all_preds.extend(preds)\n","        all_labels.extend(labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-31T20:33:17.688672Z","iopub.status.busy":"2023-10-31T20:33:17.687640Z","iopub.status.idle":"2023-10-31T20:33:22.277423Z","shell.execute_reply":"2023-10-31T20:33:22.276255Z","shell.execute_reply.started":"2023-10-31T20:33:17.688624Z"},"trusted":true},"outputs":[],"source":["from sklearn.metrics import multilabel_confusion_matrix as mcm, classification_report\n","import numpy as np\n","threshold = 0.5\n","all_preds_binary = []\n","for all_pred in all_preds:\n","    local_pred = []\n","    for old_local_pred in all_pred:\n","        binary_local_pred = (old_local_pred > threshold).astype(int)\n","        local_pred.append(binary_local_pred)\n","    all_preds_binary.append(local_pred)\n","\n","confusion_matrices = mcm(all_labels, all_preds_binary)\n","print(classification_report(all_labels, all_preds_binary, target_names=mlb.classes_))\n","\n","# Plotting\n","n_labels = confusion_matrices.shape[0]\n","fig, axes = plt.subplots(n_labels, 1, figsize=(5, 5 * n_labels))\n","\n","for i, (cm, ax) in enumerate(zip(confusion_matrices, axes)):\n","    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n","    ax.figure.colorbar(im, ax=ax)\n","    ax.set(title=mlb.classes_[i], \n","           ylabel='True label',\n","           xlabel='Predicted label',\n","           xticks=np.arange(cm.shape[1]),\n","           yticks=np.arange(cm.shape[0]),\n","           xticklabels=['0', '1'], yticklabels=['0', '1'])\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-11T13:57:57.574050Z","iopub.status.busy":"2023-11-11T13:57:57.573785Z"},"trusted":true},"outputs":[],"source":["# # Final Model\n","# model = BertClassifier()\n","# model = model.to(device)\n"," \n","# optimizer = optim.Adam(model.parameters(), lr=1e-5)\n","# criterion = nn.BCELoss()\n","\n","# scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.3, total_iters=40)\n","\n","# save_best_model_instance = SaveModelWithBestValLoss()\n","\n","# # For storing training and validation loss\n","# train_losses = []\n","# val_losses = []\n","\n","# lrs = []\n","# for epoch in range(45):\n","#     model.train()\n","    \n","#     # Initialize tqdm progress bar\n","#     #train_bar = tqdm(train_loader, desc=f'Training Epoch {epoch}')\n","    \n","#     train_loss = 0.0\n","#     for batch in train_loader:\n","#         input_ids, attention_mask, labels = batch\n","#         input_ids = input_ids.to(device)\n","#         attention_mask = attention_mask.to(device)\n","#         labels = labels.to(device)\n","\n","        \n","#         # Zero the parameter gradients\n","#         optimizer.zero_grad()\n","        \n","#         # Forward pass\n","#         logits = model(input_ids, attention_mask)\n","#         loss = criterion(logits, labels.float())\n","        \n","#         # Backward pass and optimization\n","#         loss.backward()\n","#         optimizer.step()\n","        \n","#         lrs.append(optimizer.param_groups[0][\"lr\"])\n","#         scheduler.step()\n","        \n","#         # Update training loss\n","#         train_loss += loss.item()\n","#         #train_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n","    \n","#     # Average training loss\n","#     avg_train_loss = train_loss / len(train_loader)\n","#     train_losses.append(avg_train_loss)\n","    \n","#     # Validation\n","#     val_loss = 0.0\n","#     model.eval()\n","\n","#     # Initialize tqdm progress bar for validation\n","#     #val_bar = tqdm(test_val_loader, desc=f'Validation Epoch {epoch}')\n","    \n","#     with torch.no_grad():\n","#         for batch in test_val_loader:\n","#             input_ids, attention_mask, labels = batch\n","#             input_ids = input_ids.to(device)\n","#             attention_mask = attention_mask.to(device)\n","#             labels = labels.to(device)\n","\n","            \n","#             # Forward pass\n","#             logits = model(input_ids, attention_mask)\n","#             loss = criterion(logits, labels.float())\n","            \n","#             val_loss += loss.item()\n","#             #val_bar.set_postfix({'validation_loss': '{:.3f}'.format(loss.item()/len(batch))})\n","            \n","#     # Average validation loss\n","#     avg_val_loss = val_loss / len(val_loader)\n","#     val_losses.append(avg_val_loss)\n","    \n","#     if len(val_losses) >= 4:\n","#         diff_val_loss_1 = val_losses[-2] - val_losses[-1]\n","#         diff_val_loss_2 = val_losses[-3] - val_losses[-1]\n","#         diff_val_loss_3 = val_losses[-4] - val_losses[-1]\n","#         if diff_val_loss_3 < 0 and diff_val_loss_2 < 0 and diff_val_loss_1 < 0:\n","#             print(f'Epoch {epoch+1}, Validation Loss: {avg_val_loss} DID NOT IMPROVE')\n","#             break\n","    \n","    \n","#     print(f'Epoch {epoch+1}, Training Loss: {avg_train_loss}, Validation Loss: {avg_val_loss}')\n","    \n","#     # Save model if validation model loss decreases\n","#     save_best_model_instance.save(\n","#         avg_val_loss, epoch, model, optimizer, criterion\n","#     )\n","\n","# import matplotlib.pyplot as plt\n","# plt.plot(train_losses, label='Training loss')\n","# plt.plot(val_losses, label='Validation loss')\n","# plt.legend()\n","# plt.xlabel('Epoch')\n","# plt.ylabel('Loss')\n","# plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# # Plotting validation and training loss\n","\n","# # build the model, without loading the pre-trained weights or fine-tune layers\n","# saved_model = BertClassifier().to(device)\n","# best_model = torch.load('best_model.pth', map_location=torch.device(device))\n","# saved_model.load_state_dict(best_model['model_state_dict'])\n","\n","# all_preds = []\n","# all_labels = []\n","\n","# # Put the Saved Model in evaluation mode\n","# saved_model.eval()\n","\n","# # Disable gradient computation\n","# with torch.no_grad():\n","#     # Initialize tqdm progress bar for Test\n","#     test_bar = tqdm(test_val_loader, desc='Test')\n","\n","#     for batch in test_bar:\n","#         input_ids, attention_mask, labels = batch\n","#         input_ids = input_ids.to(device)\n","#         attention_mask = attention_mask.to(device)\n","#         labels = labels.to(device)\n","\n","#         # Forward pass\n","#         logits = saved_model(input_ids, attention_mask)\n","\n","#         # Get the predicted labels\n","#         #_, preds = torch.max(logits, dim=1)\n","\n","#         # Move preds and labels to CPU for further evaluation (if using GPU)\n","#         preds = logits.cpu().numpy()\n","#         labels = labels.cpu().numpy()\n","\n","#         # Extend the list of predictions and labels\n","#         all_preds.extend(preds)\n","#         all_labels.extend(labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# from sklearn.metrics import multilabel_confusion_matrix as mcm, classification_report\n","# import numpy as np\n","# threshold = 0.5\n","# all_preds_binary = []\n","# for all_pred in all_preds:\n","#     local_pred = []\n","#     for old_local_pred in all_pred:\n","#         binary_local_pred = (old_local_pred > threshold).astype(int)\n","#         local_pred.append(binary_local_pred)\n","#     all_preds_binary.append(local_pred)\n","\n","# confusion_matrices = mcm(all_labels, all_preds_binary)\n","# print(classification_report(all_labels, all_preds_binary, target_names=mlb.classes_))\n","# acc_score = accuracy_score(all_labels, all_preds_binary)\n","# print(f\"Accuracy (micro): {acc_score}\")\n","\n","# # Plotting\n","# n_labels = confusion_matrices.shape[0]\n","# fig, axes = plt.subplots(n_labels, 1, figsize=(5, 5 * n_labels))\n","\n","# for i, (cm, ax) in enumerate(zip(confusion_matrices, axes)):\n","#     im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n","#     ax.figure.colorbar(im, ax=ax)\n","#     ax.set(title=mlb.classes_[i], \n","#            ylabel='True label',\n","#            xlabel='Predicted label',\n","#            xticks=np.arange(cm.shape[1]),\n","#            yticks=np.arange(cm.shape[0]),\n","#            xticklabels=['0', '1'], yticklabels=['0', '1'])\n","\n","# plt.tight_layout()\n","# plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def predict(model, sentence, tokenizer):\n","    model.eval()\n","    \n","    encoded = tokenizer.encode_plus(\n","        sentence, \n","        add_special_tokens=True, \n","        max_length=256, \n","        padding='max_length', \n","        truncation=True, \n","        return_attention_mask=True)\n","    \n","    input_ids = torch.tensor([encoded['input_ids']], dtype=torch.long).to(device)\n","    attention_mask = torch.tensor([encoded['attention_mask']], dtype=torch.long).to(device)\n","    \n","    # Make a prediction\n","    with torch.no_grad():\n","        logits = model(input_ids, attention_mask)\n","    \n","    logits_cpu = logits.to('cpu')\n","    return logits_cpu.numpy()\n","\n","\n","sentence = \"I would like to make a reservation for a table on October 12th.\"\n","preds = predict(saved_model, sentence, tokenizer)\n","threshold = 0.5\n","all_preds_binary = []\n","for all_pred in preds:\n","    local_pred = []\n","    for old_local_pred in all_pred:\n","        binary_local_pred = (old_local_pred > threshold).astype(int)\n","        local_pred.append(binary_local_pred)\n","    all_preds_binary.append(local_pred)\n","labels_preds = mlb.inverse_transform(np.array(all_preds_binary))\n","print(\"Predicted label:\", labels_preds)\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]},{"name":"stderr","output_type":"stream","text":["No config specified, defaulting to: multi_woz_v22/v2.2_active_only\n","Found cached dataset multi_woz_v22 (C:/Users/berna/.cache/huggingface/datasets/multi_woz_v22/v2.2_active_only/2.2.0/6719c8b21478299411a0c6fdb7137c3ebab2e6425129af831687fb7851c69eb5)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6194e3fb428643f1a8ae30a96da76c9b","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32md:\\Documents\\GitHub\\emai_nli_project\\dialog-act-prediction\\nli-dialog-act-pred.ipynb Cell 22\u001b[0m line \u001b[0;36m2\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/emai_nli_project/dialog-act-prediction/nli-dialog-act-pred.ipynb#X30sZmlsZQ%3D%3D?line=265'>266</a>\u001b[0m dialog_item_dataset \u001b[39m=\u001b[39m initialize_dialogitem_dataset_from_raw_data(test_data)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/emai_nli_project/dialog-act-prediction/nli-dialog-act-pred.ipynb#X30sZmlsZQ%3D%3D?line=267'>268</a>\u001b[0m \u001b[39m# Predict the dialogue acts\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/emai_nli_project/dialog-act-prediction/nli-dialog-act-pred.ipynb#X30sZmlsZQ%3D%3D?line=268'>269</a>\u001b[0m dialog_item_dataset_predicted \u001b[39m=\u001b[39m predict_da(dialog_item_dataset, saved_model, tokenizer, mlb)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/emai_nli_project/dialog-act-prediction/nli-dialog-act-pred.ipynb#X30sZmlsZQ%3D%3D?line=270'>271</a>\u001b[0m \u001b[39m# Print the first 10 predicted dialogue acts\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/emai_nli_project/dialog-act-prediction/nli-dialog-act-pred.ipynb#X30sZmlsZQ%3D%3D?line=271'>272</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFirst 10 predictions\u001b[39m\u001b[39m\"\u001b[39m)\n","\u001b[1;32md:\\Documents\\GitHub\\emai_nli_project\\dialog-act-prediction\\nli-dialog-act-pred.ipynb Cell 22\u001b[0m line \u001b[0;36m2\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/emai_nli_project/dialog-act-prediction/nli-dialog-act-pred.ipynb#X30sZmlsZQ%3D%3D?line=229'>230</a>\u001b[0m \u001b[39mif\u001b[39;00m dialog_item\u001b[39m.\u001b[39mspeaker \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mUser\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/emai_nli_project/dialog-act-prediction/nli-dialog-act-pred.ipynb#X30sZmlsZQ%3D%3D?line=230'>231</a>\u001b[0m     encoded_history \u001b[39m=\u001b[39m convert_dialogitem_encoded_history(dialog_item, dataset_copy)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/emai_nli_project/dialog-act-prediction/nli-dialog-act-pred.ipynb#X30sZmlsZQ%3D%3D?line=231'>232</a>\u001b[0m     preds \u001b[39m=\u001b[39m predict(model, encoded_history, tokenizer)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/emai_nli_project/dialog-act-prediction/nli-dialog-act-pred.ipynb#X30sZmlsZQ%3D%3D?line=232'>233</a>\u001b[0m     threshold \u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/emai_nli_project/dialog-act-prediction/nli-dialog-act-pred.ipynb#X30sZmlsZQ%3D%3D?line=233'>234</a>\u001b[0m     all_preds_binary \u001b[39m=\u001b[39m []\n","\u001b[1;32md:\\Documents\\GitHub\\emai_nli_project\\dialog-act-prediction\\nli-dialog-act-pred.ipynb Cell 22\u001b[0m line \u001b[0;36m2\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/emai_nli_project/dialog-act-prediction/nli-dialog-act-pred.ipynb#X30sZmlsZQ%3D%3D?line=201'>202</a>\u001b[0m \u001b[39m# Make a prediction\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/emai_nli_project/dialog-act-prediction/nli-dialog-act-pred.ipynb#X30sZmlsZQ%3D%3D?line=202'>203</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/emai_nli_project/dialog-act-prediction/nli-dialog-act-pred.ipynb#X30sZmlsZQ%3D%3D?line=203'>204</a>\u001b[0m     logits \u001b[39m=\u001b[39m model(input_ids, attention_mask)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/emai_nli_project/dialog-act-prediction/nli-dialog-act-pred.ipynb#X30sZmlsZQ%3D%3D?line=205'>206</a>\u001b[0m logits_cpu \u001b[39m=\u001b[39m logits\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/emai_nli_project/dialog-act-prediction/nli-dialog-act-pred.ipynb#X30sZmlsZQ%3D%3D?line=206'>207</a>\u001b[0m \u001b[39mreturn\u001b[39;00m logits_cpu\u001b[39m.\u001b[39mnumpy()\n","File \u001b[1;32mc:\\Users\\berna\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n","File \u001b[1;32mc:\\Users\\berna\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","\u001b[1;32md:\\Documents\\GitHub\\emai_nli_project\\dialog-act-prediction\\nli-dialog-act-pred.ipynb Cell 22\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/emai_nli_project/dialog-act-prediction/nli-dialog-act-pred.ipynb#X30sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input_ids, attention_mask):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/emai_nli_project/dialog-act-prediction/nli-dialog-act-pred.ipynb#X30sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbert(input_ids, attention_mask\u001b[39m=\u001b[39mattention_mask)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/emai_nli_project/dialog-act-prediction/nli-dialog-act-pred.ipynb#X30sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     cls_output \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlast_hidden_state[:, \u001b[39m0\u001b[39m, :] \u001b[39m# selects the [CLS] token position.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Documents/GitHub/emai_nli_project/dialog-act-prediction/nli-dialog-act-pred.ipynb#X30sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     logits \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msigmoid(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(cls_output))\n","File \u001b[1;32mc:\\Users\\berna\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n","File \u001b[1;32mc:\\Users\\berna\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\berna\\anaconda3\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:609\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    605\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    607\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(input_ids, inputs_embeds)  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m--> 609\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer(\n\u001b[0;32m    610\u001b[0m     x\u001b[39m=\u001b[39membeddings,\n\u001b[0;32m    611\u001b[0m     attn_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[0;32m    612\u001b[0m     head_mask\u001b[39m=\u001b[39mhead_mask,\n\u001b[0;32m    613\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m    614\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[0;32m    615\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[0;32m    616\u001b[0m )\n","File \u001b[1;32mc:\\Users\\berna\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n","File \u001b[1;32mc:\\Users\\berna\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\berna\\anaconda3\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:375\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    368\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    369\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    370\u001b[0m         hidden_state,\n\u001b[0;32m    371\u001b[0m         attn_mask,\n\u001b[0;32m    372\u001b[0m         head_mask[i],\n\u001b[0;32m    373\u001b[0m     )\n\u001b[0;32m    374\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 375\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    376\u001b[0m         hidden_state,\n\u001b[0;32m    377\u001b[0m         attn_mask,\n\u001b[0;32m    378\u001b[0m         head_mask[i],\n\u001b[0;32m    379\u001b[0m         output_attentions,\n\u001b[0;32m    380\u001b[0m     )\n\u001b[0;32m    382\u001b[0m hidden_state \u001b[39m=\u001b[39m layer_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m    384\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n","File \u001b[1;32mc:\\Users\\berna\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n","File \u001b[1;32mc:\\Users\\berna\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\berna\\anaconda3\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:295\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[1;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    286\u001b[0m \u001b[39mParameters:\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[39m    x: torch.tensor(bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[39m    torch.tensor(bs, seq_length, dim) The output of the transformer block contextualization.\u001b[39;00m\n\u001b[0;32m    293\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[39m# Self-Attention\u001b[39;00m\n\u001b[1;32m--> 295\u001b[0m sa_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention(\n\u001b[0;32m    296\u001b[0m     query\u001b[39m=\u001b[39mx,\n\u001b[0;32m    297\u001b[0m     key\u001b[39m=\u001b[39mx,\n\u001b[0;32m    298\u001b[0m     value\u001b[39m=\u001b[39mx,\n\u001b[0;32m    299\u001b[0m     mask\u001b[39m=\u001b[39mattn_mask,\n\u001b[0;32m    300\u001b[0m     head_mask\u001b[39m=\u001b[39mhead_mask,\n\u001b[0;32m    301\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m    302\u001b[0m )\n\u001b[0;32m    303\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[0;32m    304\u001b[0m     sa_output, sa_weights \u001b[39m=\u001b[39m sa_output  \u001b[39m# (bs, seq_length, dim), (bs, n_heads, seq_length, seq_length)\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\berna\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n","File \u001b[1;32mc:\\Users\\berna\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\berna\\anaconda3\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:217\u001b[0m, in \u001b[0;36mMultiHeadSelfAttention.forward\u001b[1;34m(self, query, key, value, mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    215\u001b[0m q \u001b[39m=\u001b[39m shape(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_lin(query))  \u001b[39m# (bs, n_heads, q_length, dim_per_head)\u001b[39;00m\n\u001b[0;32m    216\u001b[0m k \u001b[39m=\u001b[39m shape(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_lin(key))  \u001b[39m# (bs, n_heads, k_length, dim_per_head)\u001b[39;00m\n\u001b[1;32m--> 217\u001b[0m v \u001b[39m=\u001b[39m shape(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_lin(value))  \u001b[39m# (bs, n_heads, k_length, dim_per_head)\u001b[39;00m\n\u001b[0;32m    219\u001b[0m q \u001b[39m=\u001b[39m q \u001b[39m/\u001b[39m math\u001b[39m.\u001b[39msqrt(dim_per_head)  \u001b[39m# (bs, n_heads, q_length, dim_per_head)\u001b[39;00m\n\u001b[0;32m    220\u001b[0m scores \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(q, k\u001b[39m.\u001b[39mtranspose(\u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m))  \u001b[39m# (bs, n_heads, q_length, k_length)\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\berna\\anaconda3\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:209\u001b[0m, in \u001b[0;36mMultiHeadSelfAttention.forward.<locals>.shape\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mshape\u001b[39m(x: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m    208\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"separate heads\"\"\"\u001b[39;00m\n\u001b[1;32m--> 209\u001b[0m     \u001b[39mreturn\u001b[39;00m x\u001b[39m.\u001b[39mview(bs, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_heads, dim_per_head)\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["### Production Module\n","from typing import List\n","from pydantic import BaseModel\n","from sklearn.preprocessing import MultiLabelBinarizer\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from datasets import load_dataset\n","from transformers import DistilBertTokenizer, DistilBertModel\n","from enum import Enum\n","import torch\n","import torch.nn as nn\n","\n","\n","# Use GPU if available\n","torch.cuda.empty_cache()\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","\n","dataset = load_dataset(\"multi_woz_v22\")\n","train_data = dataset['train']\n","val_data = dataset['validation']\n","test_data = dataset['test']\n","\n","\n","speaker_str = {0: 'User', 1: 'Agent'}\n","PAST_HISTORY_LENGTH = 2\n","\n","class BertClassifier(nn.Module):\n","    def __init__(self):\n","        super(BertClassifier, self).__init__()\n","        self.bert = DistilBertModel.from_pretrained(bert_model_type)\n","        self.fc = nn.Linear(768, len(mlb.classes_))\n","\n","    def forward(self, input_ids, attention_mask):\n","        outputs = self.bert(input_ids, attention_mask=attention_mask)\n","        cls_output = outputs.last_hidden_state[:, 0, :] # selects the [CLS] token position.\n","        logits = torch.sigmoid(self.fc(cls_output))\n","        return logits\n","\n","class DialogItemIdentifier(BaseModel):\n","    id_dialog: str\n","    order_in_dialog: int\n","\n","class DialogItem(BaseModel):\n","    id_dialog: str\n","    order_in_dialog: int\n","    utterance: str\n","    speaker: str\n","    dialogue_acts: List[str]\n","    gt_dialogue_acts: List[str]\n","    previous_dialog_items: List[DialogItemIdentifier] = []\n","\n","def relabel_dialogue_act(dialogue_act: str):\n","    if dialogue_act.split('-')[0].upper() not in ['RESTAURANT', 'HOTEL', 'BOOKING', 'GENERAL']:\n","        new_dialog_act = dialogue_act.split('-')[0]\n","    else:\n","        new_dialog_act = dialogue_act\n","\n","    return new_dialog_act\n","\n","\n","def convert_dialogitem_encoded_history(dialog_item: DialogItem, dialog_item_dataset: List[DialogItem], past_history_length: int = 2):\n","    \"\"\"\n","    Converts a DialogItem object into an Encoded History string.\n","\n","    Parameters:\n","    - dialog_item: DialogItem object.\n","    - dialog_item_dataset: List of DialogItem objects.\n","    - past_history_length: Length of the past history to consider.\n","\n","\n","    Returns:\n","    - Encoded History string.\n","    \"\"\"\n","    encoded_history = \"\"\n","\n","    # Initialize the history of the user and agent as lists with empty DialogItem objects\n","    agent_history: List[DialogItem] = []\n","    user_history: List[DialogItem] = []\n","    \n","    # For each identifier of the previous dialog items, search in the dialog_item_dataset for the corresponding DialogItem object\n","    # and append it to the agent or user history. If no DialogItem object is found, append a dialog item with empty utterance \"\" and dialogue acts []\n","    for i in range(len(dialog_item.previous_dialog_items)):\n","        for j in range(len(dialog_item_dataset)):\n","            if dialog_item_dataset[j].id_dialog == dialog_item.previous_dialog_items[i].id_dialog and dialog_item_dataset[j].order_in_dialog == dialog_item.previous_dialog_items[i].order_in_dialog:\n","                if dialog_item_dataset[j].speaker == \"Agent\":\n","                    agent_history.append(dialog_item_dataset[j])\n","                    # If the agent history is longer than the past history length, remove the oldest turn\n","                    if len(agent_history) > past_history_length:\n","                        agent_history.pop(0)\n","                elif dialog_item_dataset[j].speaker == \"User\":\n","                    user_history.append(dialog_item_dataset[j])\n","                    # If the user history is longer than the past history length, remove the oldest turn\n","                    if len(user_history) > past_history_length:\n","                        user_history.pop(0)\n","\n","    # Fill up the encoded history of the user with the beggining of the array with DialogItems with empty utterance \"\" and dialogue acts [], for the amount\n","    # of turns that are missing to reach the past history length\n","    past_encoded_user_history = \"\"\n","    for i in range(past_history_length - len(user_history)):\n","        past_encoded_user_history += \">\".join([\"\", \"\"]) + \"|\"\n","    for i in range(len(user_history)):\n","        past_encoded_user_history += \">\".join([user_history[i].utterance, \"_\".join(user_history[i].dialogue_acts)]) + \"|\"\n","\n","    # Fill up the encoded history of the agent with the beggining of the array with DialogItems with empty utterance \"\" and dialogue acts [], for the amount\n","    # of turns that are missing to reach the past history length\n","    past_encoded_agent_history = \"\"\n","    for i in range(past_history_length - len(agent_history)):\n","        past_encoded_agent_history += \">\".join([\"\", \"\"]) + \"|\"\n","    \n","    for i in range(len(agent_history)):\n","        past_encoded_agent_history += \">\".join([agent_history[i].utterance, \"_\".join(agent_history[i].dialogue_acts)]) + \"|\"\n","\n","    encoded_history = past_encoded_user_history + past_encoded_agent_history + dialog_item.utterance\n","\n","    return encoded_history\n","\n","def initialize_dialogitem_dataset_from_raw_data(raw_data):\n","    \"\"\"\n","    Initializes a list of DialogItem objects from the raw data, only for User turns\n","\n","    Parameters:\n","    - raw_data: raw data from the MultiWOZ dataset.\n","\n","    Returns:\n","    - List of DialogItem objects.\n","    \"\"\"\n","    # Initialize an empty list to store the DialogItem objects\n","    dialog_items: List[DialogItem] = []\n","\n","    # Loop through each dialogue in the raw data\n","    for dialogue in raw_data:\n","\n","        # Get the number of turns in this dialogue\n","        num_turns = len(dialogue['turns']['utterance'])\n","\n","        # Initialize an empty list to store the history of the user and agent\n","        dialog_past_history: List[DialogItemIdentifier] = []\n","\n","        # Loop through each turn in the dialogue\n","        for i in range(num_turns):\n","            speaker = speaker_str[dialogue['turns']['speaker'][i]]\n","\n","            # Extract the utterance and corresponding dialog act for this turn\n","            utterance = dialogue['turns']['utterance'][i]\n","\n","            dialogue_act = dialogue['turns']['dialogue_acts'][i]['dialog_act']['act_type']\n","            dialogue_act_relabeled = []\n","            for j in range(len(dialogue_act)):\n","                dialogue_act_relabeled.append(relabel_dialogue_act(dialogue_act[j]))\n","\n","            \n","           \n","            # If the speaker is the Agent, fill up the DAs since we know the dialog acts of our answers\n","            filled_dialogue_acts = []\n","            if speaker == \"Agent\":\n","                filled_dialogue_acts = dialogue_act_relabeled\n","\n","            # Create a DialogItem object for this turn\n","            dialog_item = DialogItem(\n","                id_dialog=dialogue['dialogue_id'],\n","                order_in_dialog=i,\n","                utterance=utterance,\n","                speaker=speaker,\n","                dialogue_acts=filled_dialogue_acts,\n","                gt_dialogue_acts=dialogue_act_relabeled,\n","                previous_dialog_items=dialog_past_history\n","            )\n","\n","            # Append the DialogItem object to the list of DialogItem objects\n","            dialog_items.append(dialog_item)\n","\n","            # Append the DialogItemIdentifier object to the history of the user and agent\n","            dialog_item_identifier = DialogItemIdentifier(\n","                id_dialog=dialogue['dialogue_id'],\n","                order_in_dialog=i\n","            )\n","            dialog_past_history.append(dialog_item_identifier)\n","\n","            # If the history of the user and agent is longer than the past history length multiplied by two, which guarantees that this wont fail on the conversion and speeds up the process\n","            # , remove the oldest turn\n","            if len(dialog_past_history) > PAST_HISTORY_LENGTH*2:\n","                dialog_past_history.pop(0)\n","\n","    return dialog_items\n","\n","def predict(model, sentence, tokenizer):\n","    model.eval()\n","    \n","    encoded = tokenizer.encode_plus(\n","        sentence, \n","        add_special_tokens=True, \n","        max_length=256, \n","        padding='max_length', \n","        truncation=True, \n","        return_attention_mask=True)\n","    \n","    input_ids = torch.tensor([encoded['input_ids']], dtype=torch.long).to(device)\n","    attention_mask = torch.tensor([encoded['attention_mask']], dtype=torch.long).to(device)\n","    \n","    # Make a prediction\n","    with torch.no_grad():\n","        logits = model(input_ids, attention_mask)\n","    \n","    logits_cpu = logits.to('cpu')\n","    return logits_cpu.numpy()\n","\n","def predict_da(dialog_item_dataset: List[DialogItem], model: BertClassifier, tokenizer: DistilBertTokenizer, multilabelbinarizer: MultiLabelBinarizer):\n","    \"\"\"\n","    Predicts the dialogue act of every User turn in the dialog, filling up the dialog acts of previous dialog items in the same dialog with the previous prediction.\n","    only for the User. For the Agent, the DAs are filled using the ground truth from the pre-processing function and it arrives here filled, since\n","    we know what dialog acts the agent is performing.\n","\n","    Parameters:\n","    - dialog_item_dataset: List of DialogItem objects.\n","    - model: BertClassifier model.\n","    - tokenizer: DistilBertTokenizer object.\n","    - multilabelbinarizer: MultiLabelBinarizer object.\n","\n","    Returns:\n","    - List of predicted dialogue acts, only for User utterances.\n","    \"\"\"\n","    dataset_copy = dialog_item_dataset.copy()\n","\n","    # Loop through each DialogItem object in the dataset\n","    for dialog_item in dataset_copy:\n","\n","        # If the speaker is the User, predict the dialogue act\n","        if dialog_item.speaker == \"User\":\n","            encoded_history = convert_dialogitem_encoded_history(dialog_item, dataset_copy)\n","            preds = predict(model, encoded_history, tokenizer)\n","            threshold = 0.5\n","            all_preds_binary = []\n","            for all_pred in preds:\n","                local_pred = []\n","                for old_local_pred in all_pred:\n","                    binary_local_pred = (old_local_pred > threshold).astype(int)\n","                    local_pred.append(binary_local_pred)\n","                all_preds_binary.append(local_pred)\n","            labels_preds = multilabelbinarizer.inverse_transform(np.array(all_preds_binary))\n","            dialog_item.dialogue_acts = labels_preds[0]\n","\n","    return dataset_copy\n","\n","\n","# Load the MultiLabelBinarizer object\n","import pickle\n","mlb = pickle.load(open('mlb.pkl', 'rb'))\n","\n","class BertModelsEnum(str, Enum):\n","    distilbert_base_uncased = 'distilbert-base-uncased'\n","    distilbert_base_cased = 'distilbert-base-cased'\n","\n","bert_model_type = BertModelsEnum.distilbert_base_uncased.value\n","\n","# Load the best model\n","saved_model = BertClassifier().to(device)\n","best_model = torch.load('best_model.pth', map_location=torch.device(device))\n","saved_model.load_state_dict(best_model['model_state_dict'])\n","\n","# Initialize the tokenizer\n","tokenizer = DistilBertTokenizer.from_pretrained(bert_model_type)\n","\n","# Initialize the dataset\n","dialog_item_dataset = initialize_dialogitem_dataset_from_raw_data(test_data)\n","\n","# Predict the dialogue acts\n","dialog_item_dataset_predicted = predict_da(dialog_item_dataset, saved_model, tokenizer, mlb)\n","\n","# Print the first 10 predicted dialogue acts\n","print(\"First 10 predictions\")\n","for dialog_item in dialog_item_dataset_predicted[:10]:\n","    print(dialog_item.dialogue_acts)\n","\n","# Print the first 10 ground truth dialogue acts\n","print(\"First 10 ground truth\")\n","for dialog_item in dialog_item_dataset[:10]:\n","    print(dialog_item.gt_dialogue_acts)\n","\n","# Compute a classification report comparing to the ground truth\n","from sklearn.metrics import classification_report\n","y_true = []\n","y_pred = []\n","for dialog_item in dialog_item_dataset:\n","    if dialog_item.speaker == \"User\":\n","        y_true.append(dialog_item.gt_dialogue_acts)\n","        y_pred.append(dialog_item.dialogue_acts)\n","\n","y_true_binary = mlb.transform(y_true)\n","y_pred_binary = mlb.transform(y_pred)\n","print(classification_report(y_true_binary, y_pred_binary)) \n","\n","# Compute the accuracy\n","from sklearn.metrics import accuracy_score\n","acc_score = accuracy_score(y_true_binary, y_pred_binary)\n","print(f\"Accuracy (micro): {acc_score}\")\n","\n","# Compute the confusion matrix\n","from sklearn.metrics import multilabel_confusion_matrix as mcm\n","confusion_matrices = mcm(y_true_binary, y_pred_binary)\n","print(confusion_matrices)\n","\n","# Plot the confusion matrix\n","import matplotlib.pyplot as plt\n","n_labels = confusion_matrices.shape[0]\n","fig, axes = plt.subplots(n_labels, 1, figsize=(5, 5 * n_labels))\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}
