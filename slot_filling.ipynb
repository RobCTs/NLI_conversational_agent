{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\camil\\anaconda3\\envs\\my-torch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the Train, Validation and Test data\n",
    "\n",
    "dataset = load_dataset(\"multi_woz_v22\")\n",
    "train_data = dataset['train']\n",
    "val_data = dataset['validation']\n",
    "test_data = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterDomains(data):\n",
    "    \"\"\"\n",
    "    Filters a list of dictionaries by only including entries with services\n",
    "    either \"restaurant\" or \"hotel\" and having only one service.\n",
    "\n",
    "    Parameters:\n",
    "    - data: list of dictionaries containing a \"services\" key, which is a list of services.\n",
    "\n",
    "    Returns:\n",
    "    - List of filtered dictionaries.\n",
    "    \"\"\"\n",
    "    return [entry for entry in data if set(entry[\"services\"]).issubset({\"restaurant\", \"hotel\"})]\n",
    "\n",
    "# Only keep dialogues related to Restaurants or Hotels.\n",
    "\n",
    "train_data_filtered = filterDomains(train_data)\n",
    "val_data_filtered = filterDomains(val_data)\n",
    "test_data_filtered = filterDomains(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_utterances(dialogue):\n",
    "    labeled_data = []\n",
    "    data = dialogue['turns']\n",
    "    \n",
    "    # Loop through each turn in the dialogue\n",
    "    for i, turn_id in enumerate(data['turn_id']):\n",
    "        utterance = data['utterance'][i]\n",
    "        tokens = utterance.split()  \n",
    "        labels = ['O'] * len(tokens)  # Initialize labels as 'O' (Outside)\n",
    "\n",
    "        # Check if there are slot values for this turn\n",
    "        if i < len(data['dialogue_acts']):\n",
    "            dialogue_act = data['dialogue_acts'][i]\n",
    "            span_info = dialogue_act.get('span_info', {})\n",
    "            for act_slot_name, act_slot_value, span_start, span_end in zip(\n",
    "                    span_info.get('act_slot_name', []),\n",
    "                    span_info.get('act_slot_value', []),\n",
    "                    span_info.get('span_start', []),\n",
    "                    span_info.get('span_end', [])):\n",
    "                \n",
    "                # Find the tokens that correspond to the start and end indices\n",
    "                start_token_idx = len(utterance[:span_start].split())\n",
    "                end_token_idx = len(utterance[:span_end].split()) - 1\n",
    "\n",
    "                if start_token_idx < len(labels) and end_token_idx < len(labels):\n",
    "                    # Label tokens using IOB format\n",
    "                    labels[start_token_idx] = f\"B-{act_slot_name}:{act_slot_value}\"\n",
    "                    for j in range(start_token_idx + 1, end_token_idx + 1):\n",
    "                        labels[j] = f\"I-{act_slot_name}:{act_slot_value}\"\n",
    "                else:\n",
    "                    print(f\"Warning: Index out of range for utterance '{utterance}' with span {span_start}-{span_end}\")\n",
    "\n",
    "        # Store the tokenized utterance along with its labels\n",
    "        labeled_data.append((tokens, labels))\n",
    "        \n",
    "    return labeled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Index out of range for utterance 'Not sure of that, I am looking for a restaurant named \"ask.\"' with span 55-58\n",
      "Warning: Index out of range for utterance 'Here is the information for Parkside Police Station Telephone:01223358966 Address :Parkside, Cambridge Postcode :CB11JG' with span 113-119\n",
      "Warning: Index out of range for utterance 'Sure. The address and phone number for La Mimosa is Thompsons Lane Fen Ditton/01223362525 and for Shiraz restaurant it is 84 Regent Street City Centre/01223307581.' with span 151-162\n",
      "Warning: Index out of range for utterance 'Addenbrookes Hospital is nearby. The telephone is 01223245151 and the address is Hills Rd, Cambridge,CB20QQ' with span 101-107\n",
      "Warning: Index out of range for utterance 'Here is the contact info for your police station Telephone:01223358966 Address :Parkside, Cambridge Postcode :CB11JG' with span 110-116\n",
      "Warning: Index out of range for utterance 'Here is the information for Parkside Police Station Telephone:01223358966 Address :Parkside, Cambridge Postcode :CB11JG' with span 113-119\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def toDF(data):\n",
    "    all_labeled_data = []\n",
    "    for dialogue in data:\n",
    "        all_labeled_data.extend(label_utterances(dialogue))\n",
    "    return pd.DataFrame(all_labeled_data, columns=['Tokens', 'Labels'])\n",
    "    \n",
    "# Create DataFrames of labeled utterances\n",
    "train_df = toDF(train_data_filtered)\n",
    "test_df = toDF(test_data_filtered)\n",
    "val_df = toDF(val_data_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28928, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[i, need, a, place, to, dine, in, the, center,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, B-area:centre, O, B-p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[I, have, several, options, for, you;, do, you...</td>\n",
       "      <td>[O, O, B-choice:several, O, O, O, O, O, O, B-f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Any, sort, of, food, would, be, fine,, as, lo...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[There, is, an, Afrian, place, named, Bedouin,...</td>\n",
       "      <td>[O, O, O, B-food:Afrian, O, O, B-name:Bedouin,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Sounds, good,, could, I, get, that, phone, nu...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tokens  \\\n",
       "0  [i, need, a, place, to, dine, in, the, center,...   \n",
       "1  [I, have, several, options, for, you;, do, you...   \n",
       "2  [Any, sort, of, food, would, be, fine,, as, lo...   \n",
       "3  [There, is, an, Afrian, place, named, Bedouin,...   \n",
       "4  [Sounds, good,, could, I, get, that, phone, nu...   \n",
       "\n",
       "                                              Labels  \n",
       "0  [O, O, O, O, O, O, O, O, B-area:centre, O, B-p...  \n",
       "1  [O, O, B-choice:several, O, O, O, O, O, O, B-f...  \n",
       "2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "3  [O, O, O, B-food:Afrian, O, O, B-name:Bedouin,...  \n",
       "4  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-p...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Check if GPU is available and set the device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "524488"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "all_labels = [label for sublist in train_df['Labels'].tolist() for label in sublist]\n",
    "all_labels += [label for sublist in val_df['Labels'].tolist() for label in sublist]\n",
    "all_labels += [label for sublist in test_df['Labels'].tolist() for label in sublist]\n",
    "unique_labels = set(all_labels)\n",
    "\n",
    "unique_labels.__sizeof__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label: i for i, label in enumerate(unique_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the  data\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "def tokenize_data(tokenizer, text_sequences, label_sequences, label_map):\n",
    "    tokenized_texts, tokenized_labels = [], []\n",
    "    \n",
    "    for tokens, labels in zip(text_sequences, label_sequences):\n",
    "        tokenized_text, tokenized_label = [], []\n",
    "        \n",
    "        for word, label in zip(tokens, labels):\n",
    "            # Tokenize each word and its corresponding label\n",
    "            word_tokens = tokenizer.tokenize(word)\n",
    "            tokenized_text.extend(word_tokens)\n",
    "            tokenized_label.extend([label_map[label]] * len(word_tokens))  # Convert label to integer\n",
    "        \n",
    "        tokenized_texts.append(tokenizer.convert_tokens_to_ids(tokenized_text))\n",
    "        tokenized_labels.append(tokenized_label)\n",
    "    \n",
    "    return tokenized_texts, tokenized_labels\n",
    "\n",
    "# Tokenize the data\n",
    "train_texts, train_labels = tokenize_data(tokenizer, train_df['Tokens'].tolist(), train_df['Labels'].tolist(), label_map)\n",
    "val_texts, val_labels = tokenize_data(tokenizer, val_df['Tokens'].tolist(), val_df['Labels'].tolist(), label_map)\n",
    "test_texts, test_labels = tokenize_data(tokenizer, test_df['Tokens'].tolist(), test_df['Labels'].tolist(), label_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def createDataLoader(tokenized_texts, tokenized_labels ):\n",
    "    # Convert lists to PyTorch tensors and pad\n",
    "    tokenized_texts = pad_sequence([torch.tensor(seq, dtype=torch.long) for seq in tokenized_texts], padding_value=0, batch_first=True)\n",
    "    tokenized_labels = pad_sequence([torch.tensor(seq, dtype=torch.long) for seq in tokenized_labels], padding_value=-100, batch_first=True)\n",
    "    \n",
    "    # Create attention masks\n",
    "    attention_masks = (tokenized_texts != 0).long()\n",
    "    \n",
    "    # Create a TensorDataset\n",
    "    train_dataset = TensorDataset(tokenized_texts, attention_masks, tokenized_labels)\n",
    "    \n",
    "    # Create DataLoader    \n",
    "    return DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataloader = createDataLoader(train_texts, train_labels)\n",
    "val_dataloader = createDataLoader(val_texts, val_labels)\n",
    "test_dataloader = createDataLoader(test_texts, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForTokenClassification\n",
    "\n",
    "# Initialize the BERT-based model for token classification\n",
    "model = BertForTokenClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=len(label_map)\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 62.00 MiB. GPU 0 has a total capacty of 4.00 GiB of which 0 bytes is free. Of the allocated memory 3.04 GiB is allocated by PyTorch, and 395.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\camil\\OneDrive\\Desktop\\emai_nli_project\\slot_filling.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/camil/OneDrive/Desktop/emai_nli_project/slot_filling.ipynb#X15sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(b_input_ids, token_type_ids\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, attention_mask\u001b[39m=\u001b[39mb_input_mask, labels\u001b[39m=\u001b[39mb_labels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/camil/OneDrive/Desktop/emai_nli_project/slot_filling.ipynb#X15sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m loss \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mloss\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/camil/OneDrive/Desktop/emai_nli_project/slot_filling.ipynb#X15sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/camil/OneDrive/Desktop/emai_nli_project/slot_filling.ipynb#X15sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/camil/OneDrive/Desktop/emai_nli_project/slot_filling.ipynb#X15sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\camil\\anaconda3\\envs\\my-torch\\lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    494\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\camil\\anaconda3\\envs\\my-torch\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m     tensors,\n\u001b[0;32m    253\u001b[0m     grad_tensors_,\n\u001b[0;32m    254\u001b[0m     retain_graph,\n\u001b[0;32m    255\u001b[0m     create_graph,\n\u001b[0;32m    256\u001b[0m     inputs,\n\u001b[0;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    259\u001b[0m )\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 62.00 MiB. GPU 0 has a total capacty of 4.00 GiB of which 0 bytes is free. Of the allocated memory 3.04 GiB is allocated by PyTorch, and 395.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "epochs = 30\n",
    "patience = 2\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_dataloader)\n",
    "    print(f'Epoch {epoch + 1}/{epochs} | Train Loss: {avg_train_loss}')\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    for batch in val_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "            loss = outputs.loss\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    print(f'Epoch {epoch + 1}/{epochs} | Validation Loss: {avg_val_loss}')\n",
    "\n",
    "    # Check if the validation loss is lower than the best one seen so far\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), f'checkpoint_epoch_{epoch+1}.pt')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print('Early stopping!')\n",
    "            break\n",
    "        \n",
    "torch.save(model.state_dict(), 'final_model.pt')\n",
    "print('Training complete. Final model saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model.eval()  \n",
    "total_loss = 0\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_attention_masks, b_labels = batch\n",
    "\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_attention_masks, labels=b_labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = outputs.logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Convert logits to predicted labels\n",
    "        predictions = np.argmax(logits, axis=2)\n",
    "        \n",
    "        # Flatten the predictions and true labels to compute metrics\n",
    "        all_predictions.extend(predictions.flatten())\n",
    "        all_true_labels.extend(label_ids.flatten())\n",
    "\n",
    "# Calculate average loss over all the batches\n",
    "avg_loss = total_loss / len(test_dataloader)\n",
    "print(f\"Test loss: {avg_loss}\")\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(all_true_labels, all_predictions, labels=list(label_map.values()), target_names=list(label_map.keys())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_model(model, tokenizer, label_map, sentence, device):\n",
    "    model.eval()  \n",
    "    \n",
    "    # Tokenize the new utterance using the tokenize_data function\n",
    "    tokenized_texts, _ = tokenize_data(tokenizer, [sentence.split()], [['O']*len(sentence.split())], label_map) # Add dummy labels.\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    input_ids = torch.tensor(tokenized_texts).to(device)\n",
    "    attention_mask = (input_ids != tokenizer.pad_token_id).long().to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, token_type_ids=None, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    predictions = np.argmax(logits.detach().cpu().numpy(), axis=2)\n",
    "    predicted_labels = [list(label_map.keys())[list(label_map.values()).index(p)] for p in predictions[0] if p != -100]\n",
    "\n",
    "    return predicted_labels\n",
    "\n",
    "new_utterance = \"I would like to book a table for two at a Mexican restaurant.\"\n",
    "predicted_labels = query_model(model, tokenizer, label_map, new_utterance, device)\n",
    "print(predicted_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
