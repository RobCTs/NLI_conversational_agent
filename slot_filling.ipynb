{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the Train, Validation and Test data\n",
    "\n",
    "dataset = load_dataset(\"multi_woz_v22\")\n",
    "train_data = dataset['train']\n",
    "val_data = dataset['validation']\n",
    "test_data = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterDomains(data):\n",
    "    \"\"\"\n",
    "    Filters a list of dictionaries by only including entries with services\n",
    "    either \"restaurant\" or \"hotel\" and having only one service.\n",
    "\n",
    "    Parameters:\n",
    "    - data: list of dictionaries containing a \"services\" key, which is a list of services.\n",
    "\n",
    "    Returns:\n",
    "    - List of filtered dictionaries.\n",
    "    \"\"\"\n",
    "    return [entry for entry in data if set(entry[\"services\"]).issubset({\"restaurant\", \"hotel\"})]\n",
    "\n",
    "# Only keep dialogues related to Restaurants or Hotels.\n",
    "\n",
    "train_data_filtered = filterDomains(train_data)\n",
    "val_data_filtered = filterDomains(val_data)\n",
    "test_data_filtered = filterDomains(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_utterances(dialogue):\n",
    "    labeled_data = []\n",
    "    data = dialogue['turns']\n",
    "    \n",
    "    # Loop through each turn in the dialogue\n",
    "    for i, turn_id in enumerate(data['turn_id']):\n",
    "        utterance = data['utterance'][i]\n",
    "        tokens = utterance.split()  \n",
    "        labels = ['O'] * len(tokens)  # Initialize labels as 'O' (Outside)\n",
    "\n",
    "        # Check if there are slot values for this turn\n",
    "        if i < len(data['dialogue_acts']):\n",
    "            dialogue_act = data['dialogue_acts'][i]\n",
    "            span_info = dialogue_act.get('span_info', {})\n",
    "            for act_slot_name, act_slot_value, span_start, span_end in zip(\n",
    "                    span_info.get('act_slot_name', []),\n",
    "                    span_info.get('act_slot_value', []),\n",
    "                    span_info.get('span_start', []),\n",
    "                    span_info.get('span_end', [])):\n",
    "                \n",
    "                # Find the tokens that correspond to the start and end indices\n",
    "                start_token_idx = len(utterance[:span_start].split())\n",
    "                end_token_idx = len(utterance[:span_end].split()) - 1\n",
    "\n",
    "                if start_token_idx < len(labels) and end_token_idx < len(labels):\n",
    "                    # Label tokens using IOB format\n",
    "                    labels[start_token_idx] = f\"B-{act_slot_name}:{act_slot_value}\"\n",
    "                    for j in range(start_token_idx + 1, end_token_idx + 1):\n",
    "                        labels[j] = f\"I-{act_slot_name}:{act_slot_value}\"\n",
    "                else:\n",
    "                    print(f\"Warning: Index out of range for utterance '{utterance}' with span {span_start}-{span_end}\")\n",
    "\n",
    "        # Store the tokenized utterance along with its labels\n",
    "        labeled_data.append((tokens, labels))\n",
    "        \n",
    "    return labeled_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Index out of range for utterance 'Not sure of that, I am looking for a restaurant named \"ask.\"' with span 55-58\n",
      "Warning: Index out of range for utterance 'Here is the information for Parkside Police Station Telephone:01223358966 Address :Parkside, Cambridge Postcode :CB11JG' with span 113-119\n",
      "Warning: Index out of range for utterance 'Sure. The address and phone number for La Mimosa is Thompsons Lane Fen Ditton/01223362525 and for Shiraz restaurant it is 84 Regent Street City Centre/01223307581.' with span 151-162\n",
      "Warning: Index out of range for utterance 'Addenbrookes Hospital is nearby. The telephone is 01223245151 and the address is Hills Rd, Cambridge,CB20QQ' with span 101-107\n",
      "Warning: Index out of range for utterance 'Here is the contact info for your police station Telephone:01223358966 Address :Parkside, Cambridge Postcode :CB11JG' with span 110-116\n",
      "Warning: Index out of range for utterance 'Here is the information for Parkside Police Station Telephone:01223358966 Address :Parkside, Cambridge Postcode :CB11JG' with span 113-119\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def toDF(data):\n",
    "    all_labeled_data = []\n",
    "    for dialogue in data:\n",
    "        all_labeled_data.extend(label_utterances(dialogue))\n",
    "    return pd.DataFrame(all_labeled_data, columns=['Tokens', 'Labels'])\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# Create DataFrames of labeled utterances\n",
    "train_df = toDF(train_data_filtered)\n",
    "test_df = toDF(test_data_filtered)\n",
    "val_df = toDF(val_data_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28928, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[i, need, a, place, to, dine, in, the, center,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, B-area:centre, O, B-p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[I, have, several, options, for, you;, do, you...</td>\n",
       "      <td>[O, O, B-choice:several, O, O, O, O, O, O, B-f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Any, sort, of, food, would, be, fine,, as, lo...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[There, is, an, Afrian, place, named, Bedouin,...</td>\n",
       "      <td>[O, O, O, B-food:Afrian, O, O, B-name:Bedouin,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Sounds, good,, could, I, get, that, phone, nu...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tokens  \\\n",
       "0  [i, need, a, place, to, dine, in, the, center,...   \n",
       "1  [I, have, several, options, for, you;, do, you...   \n",
       "2  [Any, sort, of, food, would, be, fine,, as, lo...   \n",
       "3  [There, is, an, Afrian, place, named, Bedouin,...   \n",
       "4  [Sounds, good,, could, I, get, that, phone, nu...   \n",
       "\n",
       "                                              Labels  \n",
       "0  [O, O, O, O, O, O, O, O, B-area:centre, O, B-p...  \n",
       "1  [O, O, B-choice:several, O, O, O, O, O, O, B-f...  \n",
       "2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "3  [O, O, O, B-food:Afrian, O, O, B-name:Bedouin,...  \n",
       "4  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-p...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel, EncoderDecoderModel\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "524488"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_labels = [label for sublist in train_df['Labels'].tolist() for label in sublist]\n",
    "unique_labels = set(all_labels)\n",
    "\n",
    "unique_labels.__sizeof__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label: i for i, label in enumerate(unique_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the  data\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "def tokenize_data(tokenizer, text_sequences, label_sequences, label_map):\n",
    "    tokenized_texts, tokenized_labels = [], []\n",
    "    \n",
    "    for tokens, labels in zip(text_sequences, label_sequences):\n",
    "        tokenized_text, tokenized_label = [], []\n",
    "        \n",
    "        for word, label in zip(tokens, labels):\n",
    "            # Tokenize each word and its corresponding label\n",
    "            word_tokens = tokenizer.tokenize(word)\n",
    "            tokenized_text.extend(word_tokens)\n",
    "            tokenized_label.extend([label_map[label]] * len(word_tokens))  # Convert label to integer\n",
    "        \n",
    "        tokenized_texts.append(tokenizer.convert_tokens_to_ids(tokenized_text))\n",
    "        tokenized_labels.append(tokenized_label)\n",
    "    \n",
    "    return tokenized_texts, tokenized_labels\n",
    "\n",
    "# Tokenize the data\n",
    "tokenized_texts, tokenized_labels = tokenize_data(tokenizer, train_df['Tokens'].tolist(), train_df['Labels'].tolist(), label_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Convert lists to PyTorch tensors and pad\n",
    "tokenized_texts = pad_sequence([torch.tensor(seq, dtype=torch.long) for seq in tokenized_texts], padding_value=0, batch_first=True)\n",
    "tokenized_labels = pad_sequence([torch.tensor(seq, dtype=torch.long) for seq in tokenized_labels], padding_value=-100, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create attention masks\n",
    "attention_masks = (tokenized_texts != 0).long()\n",
    "\n",
    "# Create a TensorDataset\n",
    "train_dataset = TensorDataset(tokenized_texts, attention_masks, tokenized_labels)  # attention_masks should be created similarly to tokenized_texts\n",
    "\n",
    "# Create DataLoaders for training and validation\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForTokenClassification\n",
    "\n",
    "# Initialize the BERT-based model for token classification\n",
    "model = BertForTokenClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=len(label_map)  # Number of unique labels in your dataset\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\camil\\OneDrive\\Desktop\\emai_nli_project\\slot_filling.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/camil/OneDrive/Desktop/emai_nli_project/slot_filling.ipynb#X15sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m loss \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/camil/OneDrive/Desktop/emai_nli_project/slot_filling.ipynb#X15sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# Backward pass and optimization\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/camil/OneDrive/Desktop/emai_nli_project/slot_filling.ipynb#X15sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/camil/OneDrive/Desktop/emai_nli_project/slot_filling.ipynb#X15sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/camil/OneDrive/Desktop/emai_nli_project/slot_filling.ipynb#X15sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\camil\\anaconda3\\envs\\my-torch\\lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    494\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\camil\\anaconda3\\envs\\my-torch\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m     tensors,\n\u001b[0;32m    253\u001b[0m     grad_tensors_,\n\u001b[0;32m    254\u001b[0m     retain_graph,\n\u001b[0;32m    255\u001b[0m     create_graph,\n\u001b[0;32m    256\u001b[0m     inputs,\n\u001b[0;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    259\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)  # Assuming -100 is the padding value for labels\n",
    "\n",
    "# Number of training epochs\n",
    "n_epochs = 3\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        batch_texts, batch_attention_masks, batch_labels = bat\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=batch_texts, attention_mask=batch_attention_masks, labels=batch_labels)\n",
    "        \n",
    "        # Get the loss from the outputs tuple: (loss, logits)\n",
    "        loss = outputs[0]\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
